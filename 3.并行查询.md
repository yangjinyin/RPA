# 一：sql层支持

1. 基础知识

```c++
Sql_cmd_dml::execute() 
|-->Sql_cmd_dml::prepare() //prepare阶段，主要完成了Resolver和Logical Transformations的工作
|		|-->Sql_cmd_select::prepare_inner() //根据不同类型就调用不同函数
|		|		|-->//此处拿select做例子.如果表达式中没有 UNION/INTERSECT/EXCEPT 或者多层级的order，那么就认为其是简单的block
|		|		|-->1. Query_block::prepare() //解析表和列信息。对抽象语法树应用永久性转换，如半连接转换、派生表转换、常量值和冗余子句（例如 ORDER BY、GROUP BY）的消除。
|		|		|		|-->1.1 propagate_nullability() //主要设置table中nullable值
|		|		|		|-->1.2 setup_tables()					//主要设置table_ref中的next_leaf
|		|		|		|-->1.3 Query_block::resolve_placeholder_tables() //对派生表处理
|		|		|		|-->1.4 Query_block::setup_wild() //解析*
|		|		|		|-->1.5 Query_block::setup_base_ref_items() //初始化base_ref_items
|		|		|		|-->1.6 setup_fields() //主要是fix fields
|		|		|		|-->1.7 Query_block::setup_conds() //解析where 和join 条件
|		|		|		|-->1.8 Query_block::setup_group() //解析group_by
|		|		|		|-->1.9 m_having_cond->fix_fields() //解析having
|		|		|		|-->1.10 Query_block::resolve_rollup（）//解析rollup
|		|		|		|-->1.11 setup_order() //解析order by
|		|		|		|-->1.12 Query_block::resolve_subquery() //解析子查询
|		|		|		|-->1.13 Query_block::transform_scalar_subqueries_to_join_with_derived() //标量子查询转换为派生表
|		|		|		|-->1.14 Query_block::flatten_subquerie() //扁平化子查询
|		|		|		|-->1.15 Query_block::apply_local_transforms() //当前查询快转换
|		|		|		|-->1.16 Window::eliminate_unused_objects() //消除没有用到的窗口函数
|		|		|-->set_prepared()//prepare完成标记
|------------------------------------------------------------------------------------------------------------
|-->Sql_cmd_dml::execute_inner()
|		|-->Query_expression::optimize() //对每个query_block进行物理优化
|		|		|-->Query_expression::set_limit() //limit = limit+offset  
|		|		|-->Query_block::optimize() //物理优化
|		|		|		|-->JOIN::optimize()
|		|		|		|		|-->2.1 count_field_types() //统计临时表中fields信息
|		|		|		|		|-->2.2 Query_block::get_optimizable_conditions() //拷贝query_block中having where到join中
|		|		|		|		|-->2.3 //预处理rollup场景，拷贝query_block中rollup_item
|		|		|		|		|-->2.4 //如果有派生表或视图则进行优化 query_block->leaf_tables->is_view_or_derived()
|		|		|		|		|		|-->Table_ref::optimize_derived() //派生表视图优化
|		|		|		|		|		|		|-->Query_expression::optimize()//递归 表结构中Query_expression优化
|		|		|		|		|		|		|-->Table_ref::create_materialized_table() //const 表进行物化
|		|		|		|		|		|		|		|-->instantiate_tmp_table() //创建临时表  
|		|		|		|		|-->2.5 JOIN::create_access_paths_for_zero_rows() //select_limit_cnt=0场景优化
|		|		|		|		|		|-->NewZeroRowsAccessPath()
|		|		|		|		|-->2.6 optimize_cond() //where条件优化 
|		|		|		|		|		|-->2.6.1 build_equal_items()//构建等值传播
|		|		|		|		|		|-->2.6.2 propagate_cond_constants() //常数传播
|		|		|		|		|		|-->2.6.3 remove_eq_conds() //移除const and equal item
|		|		|		|		|-->2.7 optimize_cond() //having条件优化
|		|		|		|		|-->2.8 JOIN::prune_table_partitions() //分区裁剪
|		|		|		|		|		|-->prune_partitions()
|		|		|		|		|-->2.9 optimize_aggregated_query() //聚合函数COUNT()、MIN()、MAX()对应的值，替换成常量
|		|		|		|		|-->2.10 substitute_gc() //将生成列代替having or where中条件
|		|		|		|		|-------------------------------------------------------------------------------------------
|		|		|		|		|-->2.11 JOIN::make_join_plan() //生成最佳访问路径
|		|		|		|		|		|-->2.11.1 JOIN::init_planner_arrays() //初始化相关变量
|		|		|		|		|		|-->2.11.2 JOIN::propagate_dependencies() //依赖性传播
|		|		|		|		|		|-->2.11.3 update_ref_and_keys() //更新join_table中m_keyuse
|		|		|		|		|		|-->2.11.4 pull_out_semijoin_tables() //半连接一定条件下转换成inner join
|		|		|		|		|		|-->2.11.5 JOIN::extract_const_tables() //const表，如果是，join_read_const_table()进行读取
|		|		|		|		|		|-->2.11.6 JOIN::extract_func_dependent_tables()//会判断一个表是否函数性依赖一个常量，是则读取
|		|		|		|		|		|-->2.11.7 JOIN::update_sargable_from_const() //const表读取后，更新sargable中内容
|		|		|		|		|		|-->2.11.8 JOIN::estimate_rowcount() //每张表估算行数
|		|		|		|		|		|-->2.11.9 JOIN::optimize_keyuse() //计算keyuse中ref_table_rows值
|		|		|		|		|		|-->2.11.10 optimize_semijoin_nests_for_materialization()//semijoin采取物化策略时的order
|		|		|		|		|		|-->2.11.11 Optimize_table_order::choose_table_order() //选择table 顺序
|		|		|		|		|		|		|-->Optimize_table_order::optimize_straight_join() //如果有hint指定了连接顺序，则直接生成
|		|		|		|		|		|		|-->Optimize_table_order::greedy_search() //贪心搜索来寻得最小的连接代价
|		|		|		|		|		|		|		|-->Optimize_table_order::best_extension_by_limited_search() ////穷举搜索
|		|		|		|		|		|-->2.11.12 JOIN::decide_subquery_strategy()//决定子查询的策略
|		|		|		|		|		|-->2.11.13 JOIN::get_best_combination()//根据join order设置join_tables,best_ref,best_pos
|		|		|		|		|		|-->2.11.14 JOIN::finalize_derived_keys()//删除派生表不需要的key
|		|		|		|		|--//该函数结束后，确定所有表的AccessPath以及他们的Join order,还缺GROUP/ORDER/DISTINCT/WINDOWS信息
|		|		|		|		|------------------------------------------------------------------------------------------
|		|		|		|		|-->2.12 substitute_for_best_equal_field()//利用2.6.1中的等值类来应用，消除所有多重相等谓词
|		|		|		|		|-->2.13 JOIN::init_ref_access() //当表的方式方式是ref时 则初始化table access信息
|		|		|		|		|		|-->create_ref_for_key() //ref table创建Index_lookup结构体(m_ref)，记录了用于ref的完整信息
|		|		|		|		|-->2.14 make_join_query_block()
|		|		|		|		|		|-->make_cond_for_table() //每个表添加条件，这个条件是当从这个表读完数据后能立刻check的
|		|		|		|		|		|-->test_if_cheaper_ordering()//recheck 考虑索引，特定条件下需要调整索引
|		|		|		|		|-->2.15 JOIN::optimize_distinct_group_order()//对group by，order by，distinct优化
|		|		|		|		|		|-->test_if_skip_sort_order() //使用索引跳过order by
|		|		|		|		|		|-->create_order_from_distinct() //使用group by(order by) 来替代distinct
|		|		|		|		|-->2.16 need_tmp_before_win = true//判断窗函数前是否需要创建临时表
|		|		|		|		|-->2.17 JOIN::alloc_qep() //为best_ref中的每个JOIN_TAB都创建一个QEP_TAB对象
|		|		|		|		|-->2.18 JOIN::test_skip_sort() //使用索引跳过order by
|		|		|		|		|-->2.19 JOIN::finalize_table_conditions()//最终确定每个表的condition，并且做去除冗余和常量缓存
|		|		|		|		|-->2.20 make_join_readinfo() //执行前设置一些必要信息
|		|		|		|		|		|-->2.20.1 setup_semijoin_dups_elimination() //设置semi-join的必要信息
|		|		|		|		|		|-->2.20.2 table->prepare_for_position() //告诉handler 接下来将会通过主键来设置访问位置
|		|		|		|		|		|-->2.20.3 QEP_TAB::init_join_cache() //设置join cache(bnl/bka)
|		|		|		|		|		|-->2.20.4 QEP_TAB::push_index_cond() //谓词下推
|		|		|		|		|		|		|-->make_cond_for_index()//满足一系列条件后 创建下推cond item
|		|		|		|		|		|		|-->ha_innobase::idx_cond_push()//innodb层ICP接口调用
|		|		|		|		|-->2.21 JOIN::make_tmp_tables_info() //设置临时表信息，并且将其加入到QEP_TAB数组的合适位置
|		|		|		|		|-->2.22 JOIN::create_access_paths() //生成accessPath
|		|		|-->set_optimized()//设置optimize标记
|		|		|-->2.23 CreateIteratorFromAccessPath()//创建迭代器 
|---------------------------------------------------------------------------------------------------------
|		|-->Query_expression::execute() //对每个quer_block进行执行
|		|		|-->Query_expression::ExecuteIteratorQuery()
|		|		|		|-->3.1 THD_STAGE_INFO()           //设置线程的状态为executing
|		|		|		|-->3.2 ClearForExecution()        //清理之前的迭代器数据
|		|		|		|-->3.3 query_result->start_execution(thd) //设置为执行状态
|		|		|		|-->3.4 send_result_set_metadata()  //先把元数据发送给客户端(m_protocol...)
|		|		|		|-->3.5 m_root_iterator->Init()         //调用不同的init迭代器。此处我们拿hashjoin迭代器举例
|		|		|		|		|-->HashJoinIterator::Init()....
|		|		|		|-->//enter loop 读一行并发送
|		|		|		|-->3.6 m_root_iterator->Read() //调用不同read的迭代器
|		|		|		|-->3.7 query_result->send_data() //发送数据
```





1.1 语法支持

1.2 优化器阶段

prepare阶段：主要添加调用`fix_prepare_information_for_order()`对group_list 和order_list 进行备份

optimize 阶段主要创建临时表，已供后面创建woker的join中tmp表信息备份使用

```c++
if (thd->m_suite_for_pq == PqConditionStatus::ENABLED) {
    // save temp table param for later PQ scan
    saved_tmp_table_param = new (thd->mem_root) Temp_table_param();
    if (!saved_tmp_table_param) return true;

    saved_tmp_table_param->pq_copy(tmp_table_param);

    // saved optimized variables to saved_optimized_vars.
    save_optimized_vars();
    saved_optimized_vars.pq_no_jbuf_after = no_jbuf_after;
}
```



leader线程：

1. 在做完optimize 后，会调用`make_pq_leader_plan()`来创建leader的执行计划
2. 调用`check_pq_conditions()`先检查是否适合并查场景
3. 找出并查表，构建新thd，构建新的join，此join将赋给worker线程
4. leader 的join信息重新构建，并重新构建新的accesspath 以及 ParallelScanIterator迭代器
5. 在执行迭代器时，会先初始化mq, 创建worker线程, 对B+树进行拆分
6. 从mq中读取数据

```c++
|-->Sql_cmd_dml::execute()
|		|-->//optimize 完成后
|		|-->1. make_pq_leader_plan() //构建物理执行计划
|		|		|-->1.1 check_pq_conditions() //检查是否适合并查场景
|		|		|		|-->1.1.1 get_pq_memory_total() //检查当前使用内存是否超过使用内存限制
|		|		|		|-->1.1.2 check_pq_running_threads() //检查并查线程数量是否超过最大线程数，会等待一段时间，看是否有线程释放
|		|		|		|-->1.1.3 suite_for_parallel_query(thd)//1.存储过程，触发器不支持；2.可附加事务不支持；3.隔离级别是ISO_SERIALIZABLE不支持
|		|		|		|-->1.1.4 suite_for_parallel_query(thd->lex)//预处理语句不支持
|		|		|		|-->1.1.5 suite_for_parallel_query(thd->lex->unit)//union/intersect/except 不支持
|		|		|		|-->1.1.6 suite_for_parallel_query(select)//1.distinct不支持 2.窗口函数不支持
|		|		|		|-->1.1.7 suite_for_parallel_query(select->join)//1.rollup不支持 2.zero_result_cause不支持 3.只支持全表扫描，index scan，range scan
|		|		|		|-->1.1.8 choose_parallel_scan_table() //只支持first no-const primary表，subquery不支持，inner join内表不支持
|		|		|-->1.2 //找出要pq表 qep_tab[tab_idx]
|		|		|-->pq_check_stable_sort() //判断mq里的数据是否需要排序,一般情况都是需要排序输出的
|		|		|-->1.3 make_pq_gather_operator() //
|		|		|		|-->1.3.1 pq_new_thd() //构建新thd，之后会给worker线程
|		|		|		|-->1.3.2 pq_make_join() //为worker线程构建JOIN结构体
|		|		|		|		|-->pq_dup_select()//把leader线程中的join->query_block深拷贝出来，之后制作worker线程的JOIN
|		|		|		|		|-->pq_select_prepare() //prepare化
|		|		|		|		|-->set_prepared() //prepare 完成标记
|		|		|		|		|-->pq_join = new (thd->pq_mem_root) JOIN(thd, select)//初始化出worker线程的join
|		|		|		|		|-->pq_join->pq_copy_from(join); //把leader的join中相关信息赋值给pq_join
|		|		|		|		|-->pq_join->query_expression()->select_limit_cnt//limit的条件有聚合/groupby时不能下推给worker
|		|		|		|-->1.3.3 pq_dup_tabs(template_join, join, true)//把leader中join中alloc出来的信息深拷贝给worker join
|		|		|		|		|-->//join中qep_tab深拷贝
|		|		|		|		|-->//query_block->leaf_tables内容深拷贝
|		|		|		|		|-->//condition深拷贝 pq_cond深拷贝(后面将用于索引下推)
|		|		|		|		|-->//如果是range_scan,深拷贝range scan 的accesspath
|		|		|		|		|-->//having condition 深拷贝
|		|		|		|-->1.3.3 template_join->setup_tmp_table_info(join) //深拷贝leader join中的临时表信息
|		|		|		|-->1.3.4 PQ_worker_manager(n)//创建n个worker线程管理器，并把workder join信息赋值给管理器
|		|		|//worker的join已经构建完成，接下来leader的join就不能scan原来的表了，得用tmp表替代
|		|		|-->1.4 need_tmp_pq_leader=true && restore_optimized_vars() //各种参数还原
|		|		|-->1.5 JOIN::make_leader_tables_info()//创建tmp表用来scan，函数参考了optimze阶段make_tmp_tables_info()
|		|		|		|-->pq_replace_avg_func()//添加了聚合函数处理 
|		|		|-->1.6 tab->set_type(JT_ALL) //设置为全表扫描
|		|		|-->1.7 clear_root_access_path() //清理leader线程的conditon，position，以及accesspath
|		|		|-->1.8 JOIN::create_access_paths() //重新构建leader的accesspath
|		|		|		|--> NewParallelScanAccessPath() //构建出leader的path，扫描方式为AccessPath::PARALLEL_SCAN
|		|		|-->1.9 Query_expression::create_access_paths() //查看是否有其他query_block需要构建path
|		|		|-->1.10 join->query_expression()->force_create_iterators(thd)//创建不同的iterator
|		|		|		|-->CreateIteratorFromAccessPath()//新增了两个迭代器，1.PARALLEL_SCAN:ParallelScanIterator迭代器 
|		|		|-->1.11 //如果是explain的话，则需要构建worker的path(make_pq_worker_plan()),而不需要启动worker线程去创建了
|---------------------------------------------------------------------------------------------------------
|		|		|-->Query_expression::ExecuteIteratorQuery()
|		|		|		|-->2. ParallelScanIterator::Init() //leader thread init
|		|		|		|		|->2.1 Gather_operator::init() //设置worker线程中m_table的访问方式，目前只支持all,range,ref,index
|		|		|		|		|		|->handler::ha_pq_init()
|		|		|		|		|		|		|->ha_innobase::pq_leader_scan_init()	//innodb层对B+数进行拆分 
|		|		|		|		|->2.2 ParallelScanIterator::pq_init_record_gather()
|		|		|		|		|		|->2.2.1 ParallelScanIterator::pq_make_filesort()//如果需要sort的话
|		|		|		|		|		|->2.2.2 MQ_record_gather::mq_scan_init()
|		|		|		|		|		|		|->Exchange_sort::init() or Exchange_nosort::init()//根据是否需要排序创建缓冲区
|		|		|		|		|		|->2.2.3 m_record_gather->m_exchange->get_mq_handle(i) //设置worker线程的MQ_handle(n个)
|		|		|		|		|->2.3 ParallelScanIterator::pq_launch_worker() //启动worker线程
|		|		|		|		|		|->mysql_thread_create()//创建线程
|		|		|		|-->3. ParallelScanIterator::Read()
|		|		|		|		|-->MQ_record_gather::mq_scan_next()
|		|		|		|		|		|-->Exchange_nosort::read_mq_record() //从mq中读取记录填充到table->record[0]
|		|		|		|		|		|		|-->Exchange_nosort::read_next()
|		|		|		|		|		|		|-->Exchange::convert_mq_data_to_record() //mq数据转record
  
  
tips：const表是指可以当常量处理的表。比如 查询条件使用了唯一索引或主键时，且查询只会返回一行数据。则视为const表
```



Worker线程

1. 根据leader传进来的join 创建各自线程的join
2. 补充做一些optimize的工作
3. 创建新增的迭代器PQblockScanIterator
4. 设置新的query_result，更改输出管道输出到mq中
5. 读取数据并发送到mq中

```c++
|-->pq_worker_exec()
|		|-->1. PQ_worker_manager *mngr = static_cast<PQ_worker_manager *>(arg)//PQ_worker_manager是leader准备好的
|		|-->2. make_pq_worker_plan()//创建worker的执行器
|		|		|-->2.1 pq_new_thd() // 构建新的thd,并加入到mngr->thd_leader->pq_workers进行管理
|		|		|-->2.2 pq_make_join() //深拷贝出一个新的join，这样不影响父线程给其他子线程传参。同leader线程
|		|		|-->2.3 pq_dup_tabs() //同leader
|		|		|-->2.4 join->setup_tmp_table_info(template_join)//同leader
|		|		|-->2.5 JOIN::make_tmp_tables_info()//原optimize过程
|		|		|-->2.6 pq_make_join_readinfo()//leader已经做过optimize了。不过由于新的thd与innodb层交互的信息丢了
|		|		|		|-->2.6.1 table->prepare_for_position()
|		|		|		|-->2.6.2 tbl->file->idx_cond_push(keyno, idx_cond) //icp下推
|		|		|		|-->2.6.3 join->set_optimized()		//设置optimize完成标记
|		|		|		|-->2.6.4 JOIN::create_access_paths()
|		|		|		|		|-->NewPQBlockScanAccessPath() //创建pqblock accesspath，设置type AccessPath::PQBLOCK_SCAN
|		|		|		|-->2.6.5 Query_expression::force_create_iterators
|		|		|		|		|-->CreateIteratorFromAccessPath()	//创建新增的PQblockScanIterator 迭代器
|		|		|-->2.7 set_query_result(mq_result) //创建并设置新的query_result，更改输出管道输出到mq中
|		|-->3 Join->query_expression()->ExecuteIteratorQuery(thd) //执行器调用
|		|		|-->3.1 PQblockScanIterator::Init()
|		|		|		|-->ha_innobase::pq_worker_scan_init() //调用innodb层pq模块
|		|		|-->3.2 PQblockScanIterator::Read()
|		|		|		|-->handler::ha_pq_next()
|		|		|		|		|-->ha_innobase::pq_worker_scan_next()//innodb pq scan模块
|		|		|-->3.3 query_result->send_data() 
|		|		|		|-->Query_result_mq::send_data() //往mq中发送数据
|		|		|		|		|-->MQueue_handle::send()
|		|		|-->3.4 send_exception_msg()//worker 异常处理；发送异常信息给mq
```



# 二  MQ模块

主要文件：msq_queue.h,msq_queue.cpp: 主要管理三个类`MQ_event`, `MQueue`,`MQueue_handle`

​                   exchange.h, exchage.cpp : 管理基类 `Exchange`: 主要管理MQueue_handle，MQ_event用来与innodb层以及sql层交互数据

​                   exchange_sort.h,  exchange_sort.cpp：管理类`Exchange_sort`在mq的基础上添加排序功能

​                  exchage_nosort.h, exchange_nosrot.h：管理类`Exchange_nosort`无排序功能的mq

​		  binary_heap.h ：服务于Exchange_sort，在heap上比较两个worker线程中的value值 

## 初始化

1. leader线程会根据是否需要排序输出来创建不同的m_exchange对象

   ```c++
   bool MQ_record_gather::mq_scan_init(Filesort *sort, int workers,
                                       uint ref_length, bool stab_output) {
     if (sort) {
       m_exchange = new (m_thd->pq_mem_root)
           Exchange_sort(m_thd, m_tab->table(), sort, m_tab->old_table()->file,
                         workers, ref_length, stab_output);
     } else {
       m_exchange = new (m_thd->pq_mem_root) Exchange_nosort(
           m_thd, m_tab->table(), workers, ref_length, stab_output);
     }
   
     if (!m_exchange || m_exchange->init()) return true;
   
     return false;
   }
   ```

2. 初始化会调用到`Exchange::init()` 新建出n个queue buffer(环形阵列)，以及N个queue handler，用来管理buffer的。这样每个worker线程会循环往其中写入。

## 读数据：

1. 无排序场景

   循环依次从mq1,mq2....mqN中读取数据，如果一个mq空了则会跳过。直到所有队列都MQ_DETACHED了，也就说所有worker线程都不再往mq中写入数据了。

2. 有排序场景

   ```c++
   |-->Exchange_sort::read_mq_record()
   |		|-->Exchange_sort::get_min_record() //输出下一条最小的record， 一直读到最小堆为空为止
   |		|		|-->//第一次进入建heap (!m_init_heap)
   |		|		|-->Exchange_sort::build_heap()//建堆来排序
   |		|		|		|-->1. m_record_groups[N] //先建立N个record group队列，分别对应N个mq中ring buffer
   |		|		|		|-->2. Exchange_sort::read_group()
   |		|		|		|		|-->Exchange_sort::load_group_record()//从mq[1~n]中读取数据到record group队列
   |		|		|		|		|		|-->2.1 get_mq_handle() //先获得操作mq的对象
   |		|		|		|		|		|-->2.2 MQueue_handle::receive() //获取数据
   |		|		|		|		|		|		|-->a. receive_bytes() //a.获取要读取的record msg的长度
   |		|		|		|		|		|		|-->b. receive_bytes() //b.再读取record的内容
   |		|		|		|		|		|-->2.3 store_mq_record(rec_group->records[i], data, msg_len)//拷贝data到record group
   |		|		|		|		|-->m_min_records[id] = rec_group->records[rec_group->n_read++]//同时赋值给min_records[i]
   |		|		|		|-->3.m_heap->build()//建立最小堆
   |		|		|		|		|-->...heap_compare_records()//比较两个record的大小
   |		|		|-->Exchange_sort::read_group() //读数据
   |		|		|-->m_heap->replace_first(i); //替换最top上的，同时调整堆
   |		|		|-->m_min_records[m_heap->first()]//找到heap中最top的数据,存放的是最小的id编号
   |		|-->Exchange::convert_mq_data_to_record()//mq_data 转 table[0]
   ```

   heap_compare_records()：比较两个队列中record的大小
   
   1. 对于table scan 或者index scan场景，没有order by时，此时比较的是两个record的row_id。`file->cmp_ref(row_id_0, row_id_1)`
   2. 对于有order by key时，此时比较的是key值。调用`cmp_varlen_keys()`比较，  如果key相等，则比较rowid
   
   
   
   ## 写数据
   
   写数据只有worker线程会写入，各自写入到自己的ring buffer中去。
   
   1. 先写msg的长度
   2. 再写msg的data
   
   ```c++
   MQ_RESULT MQueue_handle::send(const void *data, uint32 len, bool nowait) {
     MQ_RESULT res;
     uint32 nbytes = len;
     uint32 written;
   
     /** (1) write the message length into MQ */
     res = send_bytes(WORD_LENGTH, (char *)&nbytes, &written, nowait);
     if (res != MQ_SUCCESS) {
       assert(res == MQ_DETACHED);
       return res;
     }
     assert((!written && m_queue->detached == MQ_TMP_DETACHED) ||
                 written == WORD_LENGTH);
   
     /** (2) write the message data into MQ */
     res = send_bytes(nbytes, data, &written, nowait);
     if (res != MQ_SUCCESS) {
       assert(res == MQ_DETACHED);
       return res;
     }
     assert((!written && m_queue->detached == MQ_TMP_DETACHED) ||
                 written == nbytes);
     return MQ_SUCCESS;
   }
   
   ```
   
   

# 三：innodb 层

## 3.0 基础

重要类：

﻿`Parallel_reader`:  负责并行扫描的总类

﻿`Scan_ctx`: 负责一个索引的相关扫描工作，将扫描工作进行划分，划分为一个个具体的Ctx。

﻿`Ctx`: 负责一个具体的扫描工作，比如从扫描[start,end）的数据记录。

﻿`Scan_range` : 要扫描的范围

﻿`Iter`: 指定一个`Ctx`要扫描的范围，由`Scan_range`经过计算得出的

﻿`Config`：指定一个`Scan_ctx`的配置，主要包括`Scan_range`和具体要扫描的index

```cpp
##Parallel_reader类
class Parallel_reader {
  class Ctx;
  class Scan_ctx;
  struct Thread_ctx;
  
  constexpr static size_t MAX_THREADS{256};//最大线程数
  
  Ctxs m_ctxs{}; //ctx集合
  Scan_ctxs m_scan_ctxs{};//scan_ctx集合
  os_event_t m_event{}; //worker threads的事件
}

#Scan_range 类 split时把这个范围进行分割
class Parallel_reader::Scan_range {
  const dtuple_t *m_start{};			//索引扫描开始位置
  const dtuple_t *m_end{};        //索引扫描结束位置
}

##config类
struct Config {
  Scan_range m_scan_range;  //索引扫描范围
  dict_index_t *m_index{}; // /** (Cluster) Index in table to scan. */
   /** Btree level from which records need to be read. */
  size_t m_read_level{0}; 
 }

#scan_ctx类
class Parallel_reader::Scan_ctx {
 
  /** worker线程具体扫描的dutuple*/
  struct Iter {
    /** Heap used to allocate m_rec, m_tuple and m_pcur. */
    mem_heap_t *m_heap{}; //申请的堆空间
    /** Start scanning from this key. Raw data of the row. */
    const rec_t *m_rec{};//扫描的第一个rec内容

    /** Tuple representation inside m_rec, for two Iter instances in a range
    m_tuple will be [first->m_tuple, second->m_tuple). */
    const dtuple_t *m_tuple{};//m_rec的逻辑记录

    /** Persistent cursor.*/
    btr_pcur_t *m_pcur{};//游标
  };
  //对于一个scan 线程，游标要从range.first->m_tuple 一直扫描到 range.second->m_tuple。
  using Range = std::pair<std::shared_ptr<Iter>, std::shared_ptr<Iter>>; 
  //range的集合，即分割成n块区域的集合
  using Ranges = std::vector<Range, ut::allocator<Range>>;
};
```



```c++
|-->row_mysql_parallel_select_count_star()
|		|-->1. Parallel_reader reader(n_threads); //创建并行scan 线程对象
|		|-->2. Parallel_reader::Config config(FULL_SCAN, index);//创建config，参数1：扫描范围；参数2.具体索引。
|		|-->3. reader.add_scan(trx, config, [&](const Parallel_reader::Ctx *ctx) {Counter::inc(n_recs, ctx->thread_id())
|		|		|-->err = scan_ctx->partition(config.m_scan_range, ranges, 0)//从root阶段开始分区 第0层开始
|		|		|		|-->Parallel_reader::Scan_ctx::create_ranges() //创建分区集合
|		|		|		|		|-->page_cur_get_rec(&page_cursor) //从root结点开始找到一个rec
|		|		|		|		|-->btr_node_ptr_get_child_page_no()  //找到这个rec儿子结点的page no
|		|		|		|		|-->Parallel_reader::Scan_ctx::start_range()//在叶子结点找到记录，作为本次range的end以及下次start
|		|		|		|		|-->Parallel_reader::Scan_ctx::create_range() //把Iter构建出来，并加入到ranges中去
|		|		|		|		|-->page_cur_move_to_next() //移动下一个record记录，再循环
|		|		|-->Parallel_reader::Scan_ctx::create_context() //创建的range放到ctx中
|		|		|		|-->m_reader->enqueue(ctx); //压进队列中去
|		|-->4. Parallel_reader::run()
|		|		|-->Parallel_reader::parallel_read()
|		|		|		|-->os_thread_create()//创建worker线程，执行worker函数
  
//worker线程
|-->Parallel_reader::worker()
|		|-->ctx = dequeue() //取出ctx 进行scan
|		|-->Parallel_reader::Ctx::split() //判断是否需要二次分区，第一次没发均分则需二次分区
|		|		|--> m_scan_ctx->partition(scan_range, ranges, 1);//对第一层再次分区
|		|-->Parallel_reader::Ctx::traverse() //遍历分区树
|		|		|-->mtr.start(); //开启mtr 防止B+树出现smo
|		|		|-->Parallel_reader::Ctx::traverse_recs()
|		|		|		|-->1. page_cur_get_rec()//根据pcursor找到start位置
|		|		|		|-->2. end_tuple->compare(rec, index, offsets)//跟end比较，如果已经等于或者超过边界则跳出循环停止scan
|		|		|		|-->3. check_visibility() //如果已经到叶子结点，则判断可见性
|		|		|		|-->4. err = m_scan_ctx->m_f(this);//调用回掉函数  count++
|		|		|		|-->4. page_cur_move_to_next() //移到下一个record，继续循环
```





核心函数 Parallel_reader::add_scan()

```cpp
dberr_t Parallel_reader::add_scan(trx_t *trx,
                                  const Parallel_reader::Config &config,
                                  Parallel_reader::F &&f) {
  
   //初始化scan_ctx
  auto scan_ctx = std::shared_ptr<Scan_ctx>(
      ut::new_withkey<Scan_ctx>(UT_NEW_THIS_FILE_PSI_KEY, this, m_scan_ctx_id, trx, config, std::move(f)),
      [](Scan_ctx *scan_ctx) { ut::delete_(scan_ctx); });


  if (scan_ctx.get() == nullptr) {
    ib::error(ER_IB_ERR_PARALLEL_READ_OOM) << "Out of memory";
    return (DB_OUT_OF_MEMORY);
  }

  m_scan_ctxs.push_back(scan_ctx);

  ++m_scan_ctx_id;

  scan_ctx->index_s_lock();

  Parallel_reader::Scan_ctx::Ranges ranges{};
  dberr_t err{DB_SUCCESS};

 /*第一次从root结点开始分区；全表扫描，所以 m_scan_range=[null,null]。partition后ranges记录的就是各个分区的start，end范围*/
  err = scan_ctx->partition(config.m_scan_range, ranges, 0);

  if (ranges.empty() || err != DB_SUCCESS) {
    /* Table is empty. */
    scan_ctx->index_s_unlock();
    return (err);
  }
  
  //把这个ranges加入 任务队列中。
  err = scan_ctx->create_contexts(ranges);

  scan_ctx->index_s_unlock();

  return (err);
}
```

create_ranges(): 分区函数

```cpp
dberr_t Parallel_reader::Scan_ctx::create_ranges(const Scan_range &scan_range,
                                                 page_no_t page_no,
                                                 size_t depth,
                                                 const size_t split_level,
                                                 Ranges &ranges, mtr_t *mtr) {
  
  auto start = scan_range.m_start;

  if (start != nullptr) {  //二次分区，start 才不为空
    //先定位到具体的block
    page_cur_search(block, index, start, PAGE_CUR_LE, &page_cursor);
    
    if (page_cur_is_after_last(&page_cursor)) { //游标已经位于最后一个rec，那么就无需分区扫描。
      return (DB_SUCCESS);
    } else if (page_cur_is_before_first((&page_cursor))) { //游标位于first record前面，那么把游标移到下一个page开始扫
      page_cur_move_to_next(&page_cursor);
    }
  } else { //start 为空，首次分区
    page_cur_set_before_first(block, &page_cursor);//将游标移到first record前
    /* Skip the infimum record. */
    page_cur_move_to_next(&page_cursor);
  }

  mem_heap_t *heap{};

  const auto at_leaf = page_is_leaf(buf_block_get_frame(block));
  const auto at_level = btr_page_get_level(buf_block_get_frame(block));

  Savepoints savepoints{};

  //从start record开始一直扫到最后一条记录
  while (!page_cur_is_after_last(&page_cursor)) {
    const auto rec = page_cur_get_rec(&page_cursor);

    ut_a(at_leaf || rec_get_node_ptr_flag(rec) ||
         !dict_table_is_comp(index->table));

    if (heap == nullptr) {
      heap = mem_heap_create(srv_page_size / 4, UT_LOCATION_HERE);
    }

    offsets = rec_get_offsets(rec, index, offsets, ULINT_UNDEFINED,
                              UT_LOCATION_HERE, &heap);

    const auto end = scan_range.m_end;
     
    //如果end 不为空，那么就比较一下当前rec是否已经到end了，如果是则跳出循环，二次分区才会触发。
    if (end != nullptr && end->compare(rec, index, offsets) <= 0) {
      break;
    }

    page_cur_t level_page_cursor;

    /* 找到要分区的层数 m_config.m_read_level=0*/
    if (at_level > m_config.m_read_level) {
       //1. 找到这条rec的儿子page no
      auto page_no = btr_node_ptr_get_child_page_no(rec, offsets);

      if (depth < split_level) {
        /* Need to create a range starting at a lower level in the tree. */
        create_ranges(scan_range, page_no, depth + 1, split_level, ranges, mtr);

        page_cur_move_to_next(&page_cursor);
        continue;
      }

      /* 2. 在叶子结点上找到range start的位置 */
      level_page_cursor = start_range(page_no, mtr, start, savepoints);
    } else {
      /* 根结点即叶子结点的处理逻辑 */

      if (start != nullptr) {
        page_cur_search(block, index, start, PAGE_CUR_GE, &page_cursor);
        ut_a(!page_rec_is_infimum(page_cur_get_rec(&page_cursor)));
      } else {
        page_cur_set_before_first(block, &page_cursor);

        /* Skip the infimum record. */
        page_cur_move_to_next(&page_cursor);
        ut_a(!page_cur_is_after_last(&page_cursor));
      }

      /* Since we are already at the requested level use the current page
       cursor. */
      memcpy(&level_page_cursor, &page_cursor, sizeof(level_page_cursor));
    }

    if (!page_rec_is_supremum(page_cur_get_rec(&level_page_cursor))) {
      //3. 根据那条记录的游标，构建出Iter结构体，然后把iter加入到range.second。最后加入到ranges集合中
      create_range(ranges, level_page_cursor, mtr);
    }

    /* We've created the persistent cursor, safe to release S latches on
    the blocks that are in this range (sub-tree). */
    for (auto &savepoint : savepoints) {
      mtr->release_block_at_savepoint(savepoint.first, savepoint.second);
    }

    if (m_depth == 0 && depth == 0) {
      m_depth = savepoints.size();
    }

    savepoints.clear();

    if (at_level == m_config.m_read_level) {
      break;
    }
     
    //4. 读下一跳记录
    page_cur_move_to_next(&page_cursor);
  }

  savepoints.push_back(savepoint);

  for (auto &savepoint : savepoints) {
    mtr->release_block_at_savepoint(savepoint.first, savepoint.second);
  }

  if (heap != nullptr) {
    mem_heap_free(heap);
  }

  return (DB_SUCCESS);
}
```

Parallel_reader::Scan_ctx::start_range：在叶子结点找到rec，然后记录下来。作为本次分区的end，以及下一次分区的start。循环往复

```cpp
page_cur_t Parallel_reader::Scan_ctx::start_range(
    page_no_t page_no, mtr_t *mtr, const dtuple_t *key,
    Savepoints &savepoints) const {
  ut_ad(index_s_own());

  auto index = m_config.m_index;
  page_id_t page_id(index->space, page_no);
  ulint height{};

  /* Follow the left most pointer down on each page. */
  for (;;) {
    auto savepoint = mtr->get_savepoint();

    auto block = block_get_s_latched(page_id, mtr, __LINE__);

    height = btr_page_get_level(buf_block_get_frame(block));

    savepoints.push_back({savepoint, block});

    if (height != 0 && height != m_config.m_read_level) {
      page_id.set_page_no(search(block, key));
      continue;
    }

    page_cur_t page_cursor;

    if (key != nullptr) {
      page_cur_search(block, index, key, PAGE_CUR_GE, &page_cursor);
    } else {
      page_cur_set_before_first(block, &page_cursor);
    }

    if (page_rec_is_infimum(page_cur_get_rec(&page_cursor))) {
      page_cur_move_to_next(&page_cursor);
    }

    return (page_cursor);
  }
}
```

![img](https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=a73bf85734e846c392616d7c4c456995&docGuid=J3U_PPKrXEwmE7)

总结：mysql在历史版本中早已为并行扫描做了一些工作，可惜过去这么多年，仍然没有再一步推进。在并行这块落后pg甚远。

## 3.1 leader 对B+树分区

​        当前pq只支持ref，index，range，以及all 这四种方式。其中index scan 和all两种对全表进行partition即可，range scan 和ref scan则    需要计算出对应的range_start，和range_end；剩下过程则一样

```c++
//ref scan
ha_innobase::pq_leader_ref_init(uint keyno, void *&pq_ctx,
                                    uint &n_threads) {
  pq_ctx = nullptr;
  update_thd();
  auto trx = m_prebuilt->trx;
  innobase_register_trx(ht, ha_thd(), trx);
  trx_start_if_not_started_xa(trx, false, UT_LOCATION_HERE);
  trx_assign_read_view(trx);

  dtuple_t *range_start{nullptr};
  dtuple_t *range_end{nullptr};
  dict_index_t *index{nullptr};
  uint range_errno{0};
  mem_heap_t *heap{nullptr};
  btr_pcur_t *pcur{nullptr};
  
  //1.创建n_threads 来并行scan，
  auto pq_reader = ut::new_withkey<Parallel_reader>(
      UT_NEW_THIS_FILE_PSI_KEY, Parallel_reader::available_threads(n_threads));

  if (pq_reader == nullptr || !pq_reader->pq_have_event()) {
    if (pq_reader) ut::delete_(pq_reader);
    return (HA_ERR_OUT_OF_MEM);
  }

  pq_reader->key = keyno;
  index = innobase_get_index(keyno);
  m_prebuilt->index = index;

//找出search 的边界
  // create search tuple on this index
  if (index != nullptr) {
    ulint search_tuple_n_fields;
    search_tuple_n_fields = 2 * (index->table->get_n_cols() +
                                 dict_table_get_n_v_cols(index->table));
    if (!heap)
      heap = mem_heap_create(DTUPLE_EST_ALLOC(search_tuple_n_fields),
                  UT_LOCATION_HERE);
  }

  range_errno = 0;
  const uchar *key = pq_ref_key.key;
  auto keypart_map = pq_ref_key.keypart_map;
  uint key_len = calculate_key_len(table, keyno, keypart_map);

  // populate search range boudary from ref record value
  int ret = index_read(table->record[0], key, key_len, HA_READ_KEY_EXACT);
  if (ret) {
    // record errorno when can't find ref key. which will lead process finish
    // early
    range_errno = ret;
  } else {
    // record range boudary for searching
    auto start_flag =
        pq_reverse_scan ? HA_READ_BEFORE_KEY : HA_READ_KEY_OR_NEXT;

    m_prebuilt->pq_index_read = true;
    //假如是正序扫描，HA_READ_KEY_OR_NEXT一直扫到下一个rec 是目标rec，即当然dtuple则record的初始位置
    int err = index_read(table->record[0], key, key_len, start_flag);
    m_prebuilt->pq_index_read = false;

    if (!err) {
      range_start = dtuple_copy(m_prebuilt->pq_tuple, heap); //构建range_start
      range_start->n_fields_cmp = m_prebuilt->pq_tuple->n_fields_cmp;//用来比较的索引字段数
    } else {
      if (err == HA_ERR_KEY_NOT_FOUND) {
        index_first(table->record[0]);
        pcur = m_prebuilt->pcur;
        range_errno = 0;
      } else {
        range_errno = err;
      }
    }

    if (!range_errno) {
      //假如是正序扫描，HA_READ_AFTER_KEY 找到目标rec的下一个rec，即当然dtuple则record的结束位置
      auto end_flag = pq_reverse_scan ? HA_READ_KEY_OR_PREV : HA_READ_AFTER_KEY;
      m_prebuilt->pq_index_read = true;
      int err = index_read(table->record[0], key, key_len, end_flag);
      m_prebuilt->pq_index_read = false;
      if (!err) {
        range_end = dtuple_copy(m_prebuilt->pq_tuple, heap);//构建range_end
        range_end->n_fields_cmp = m_prebuilt->pq_tuple->n_fields_cmp;
        pcur = m_prebuilt->pcur;
      } else {
        if (err == HA_ERR_KEY_NOT_FOUND) {
          index_last(table->record[0]);
          pcur = m_prebuilt->pcur;
          range_errno = 0;
        } else {
          range_errno = err;
        }
      }
    }
  }

  //如果是index scan/full scan 则range_start, range_end 为null即可
  Parallel_reader::Scan_range range_scan{range_start, range_end};
  Parallel_reader::Config config(range_scan, index);
  config.m_range_errno = range_errno;
  config.m_pcur = pcur;
  config.m_pq_reverse_scan = pq_reverse_scan;
  
  //2.构建扫描范围，加入到add_scan中，add_scan函数会进行partition
  auto success = pq_reader->add_scan(trx, config, nullptr, false);
  pq_reader->snapshot = trx->read_view;//把leader的read_view 拷贝给pq_reader->snapshot。后面会赋值给worker线程

  if (heap != nullptr) {
    mem_heap_free(heap);
  }
  if (success != DB_SUCCESS) {
    ut::delete_(pq_reader);
    return (HA_ERR_GENERIC);
  }

  if (pq_reverse_scan) pq_reader->pq_set_reverse_scan();
  pq_ctx = pq_reader;
  build_template(false);

  if (pq_reader->max_splits() < n_threads) {
    n_threads = pq_reader->max_splits() > 1 ? pq_reader->max_splits() : 1;
  }

  return (0);
}

```

  range的处理过程基本跟其一致，实现`ha_innobase::pq_leader_range_select_scan_init()`; 



## 3.2 Worker线程

 worker线程在init阶段主要获取read_view

```c++
int ha_innobase::pq_worker_scan_init(uint keyno, void *scan_ctx) {
  active_index = keyno;

  Parallel_reader *pq_reader = static_cast<Parallel_reader *>(scan_ctx);
  update_thd();
  m_prebuilt->index = innobase_get_index(pq_reader->key);
  /**
   * here, we must init m_prebuilt->is_attach_ctx because this value may
   * not be reset in the last execution.
   */
  m_prebuilt->is_attach_ctx = false;
  auto trx = m_prebuilt->trx;
  innobase_register_trx(ht, ha_thd(), trx);
  trx_start_if_not_started_xa(trx, false, UT_LOCATION_HERE);
  if (trx->read_view == nullptr && pq_reader->snapshot) {
    trx_clone_read_view(trx, pq_reader->snapshot);
  }
  build_template(false);
  inited = handler::PQ_WORKER;

  return (0);
}
```

`ha_innobase::pq_worker_scan_next()`: 1. 从queue队列中取出ctx （pq_reader->dispatch_ctx(m_prebuilt);）2. read_record

```c++
|-->Parallel_reader::Ctx::read_record()
|		|-->1. page_cur_get_rec() //1. 获取rec
|		|-->2. Parallel_reader::Scan_ctx::find_visible_record() //2. 获取可见版本的record。参考row_search_mvcc
|		|-->3. row_sel_store_mysql_rec() //3. rec 转mysql format
```

```c++
dberr_t Parallel_reader::Scan_ctx::find_visible_record(byte *buf,
                                                 const rec_t *&rec,
                                                 const rec_t *&clust_rec,
                                                 ulint *&offsets,
                                                 ulint *&clust_offsets,
                                                 mem_heap_t *&heap,
                                                 mtr_t *mtr,
                                                 row_prebuilt_t *prebuilt) {

  const auto table_name = m_config.m_index->table->name;
  ut_ad(m_trx->read_view == nullptr || MVCC::is_view_active(m_trx->read_view));

  if (m_trx->read_view != nullptr) {
    auto view = m_trx->read_view; //先获取read_view

    if (m_config.m_index->is_clustered()) {
      //聚簇索引 or 主键索引
      trx_id_t rec_trx_id;

      if (m_config.m_index->trx_id_offset > 0) {
        rec_trx_id = trx_read_trx_id(rec + m_config.m_index->trx_id_offset);
      } else {
        rec_trx_id = row_get_rec_trx_id(rec, m_config.m_index, offsets);
      }

      if (m_trx->isolation_level > TRX_ISO_READ_UNCOMMITTED &&
          !view->changes_visible(rec_trx_id, table_name)) {
        rec_t *old_vers = nullptr;
				
        //获取可见版本的rec
        row_vers_build_for_consistent_read(rec, mtr, m_config.m_index, &offsets,
                                           view, &heap, heap, &old_vers,
                                           nullptr, nullptr);

        rec = old_vers;
        if (rec == nullptr) {
          return DB_NOT_FOUND;
        }
      }
    } else {
      //二级索引
      auto max_trx_id = page_get_max_trx_id(page_align(rec));
      ut_ad(max_trx_id > 0);
			
      //如果page最大的trx_id都可见，那么就不需要回表去判断可见性了
      if (!view->sees(max_trx_id) ||
          (prebuilt && prebuilt->need_to_access_clustered)) {
        if (prebuilt)
        {
          if (prebuilt->idx_cond) {
            //索引下推，判断是否满足条件
            switch (row_search_idx_cond_check(buf, prebuilt, rec, offsets))
            {
              case ICP_NO_MATCH:
                return DB_NOT_FOUND;
              case ICP_OUT_OF_RANGE:
                return DB_END_OF_RANGE;
              case ICP_MATCH:
                 break;
            }
          }
          if (prebuilt->sel_graph == nullptr)
            row_prebuild_sel_graph(prebuilt);

          Row_sel_get_clust_rec_for_mysql row_sel_get_clust_rec_for_mysql;
          que_thr_t *thr = que_fork_get_first_thr(prebuilt->sel_graph);
					
          //需要回表获取完整的rec
          prebuilt->pq_requires_clust_rec = true;
          //通过cluster索引获取完整的rec
          int err = row_sel_get_clust_rec_for_mysql(prebuilt, m_config.m_index, rec, thr, &clust_rec,
                                          &clust_offsets, &heap, NULL, mtr, nullptr);

          if (err != DB_SUCCESS)
            return DB_NOT_FOUND;
          else {
            if (clust_rec == NULL) {
              /* The record did not exist in the read view */
              ut_ad(prebuilt->select_lock_type == LOCK_NONE);
           
              return DB_NOT_FOUND;
            }
            else if(rec_get_deleted_flag(clust_rec, m_config.m_is_compact)) {
            /* The record is delete marked: we can skip it */
              return DB_NOT_FOUND;
            }
            else {
              return DB_SUCCESS;
            }
          }
        } else 
          return DB_NOT_FOUND;
      }
    }
  } else if (srv_read_only_mode &&                               /** innodb_read_only */
             (prebuilt && prebuilt->need_to_access_clustered &&   /** secondary index and non-covered index */
              !m_config.m_index->is_clustered())) {
    //read only 场景
    if (prebuilt->idx_cond) {
      switch (row_search_idx_cond_check(buf, prebuilt, rec, offsets))
      {
        case ICP_NO_MATCH:
          return DB_NOT_FOUND;
        case ICP_OUT_OF_RANGE:
          return DB_END_OF_RANGE;
        case ICP_MATCH:
          break;
      }
    }

    if (prebuilt->sel_graph == nullptr)
      row_prebuild_sel_graph(prebuilt);

    que_thr_t *thr = que_fork_get_first_thr(prebuilt->sel_graph);
    prebuilt->pq_requires_clust_rec = true;
    Row_sel_get_clust_rec_for_mysql row_sel_get_clust_rec_for_mysql;
    int err = row_sel_get_clust_rec_for_mysql(prebuilt, m_config.m_index, rec, thr, &clust_rec,
                                              &clust_offsets, &heap, NULL, mtr, nullptr);

    if (err != DB_SUCCESS)
      return DB_NOT_FOUND;
    else {
      if (clust_rec == NULL) {
        /* The record did not exist in the read view */
        ut_ad(prebuilt->select_lock_type == LOCK_NONE);

        return DB_NOT_FOUND;
      } else if(rec_get_deleted_flag(clust_rec, m_config.m_is_compact)) {
        /* The record is delete marked: we can skip it */
        return DB_NOT_FOUND;
      } else {
        return DB_SUCCESS;
      }
    }
  }
  
  if (rec_get_deleted_flag(rec, m_config.m_is_compact)) {
    /* This record was deleted in the latest committed version, or it was
    deleted and then reinserted-by-update before purge kicked in. Skip it. */
    return DB_NOT_FOUND;
  }
 
  return DB_SUCCESS;
}
```



Tips:

```sql
//t表PRIMARY KEY (`a`),KEY `idc` (`c`)
1. const: 主键或者唯一键且等值比较
explain select * from t where a = 10;
+----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+----------+-------+
| id | select_type | table | partitions | type  | possible_keys | key     | key_len | ref   | rows | filtered | Extra |
+----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+----------+-------+
|  1 | SIMPLE      | t     | NULL       | const | PRIMARY       | PRIMARY | 4       | const |    1 |   100.00 | NULL  |

  
2. ref: 非唯一键等值比较
  explain select * from t where c = 1;
+----+-------------+-------+------------+------+---------------+------+---------+-------+------+----------+-------+
| id | select_type | table | partitions | type | possible_keys | key  | key_len | ref   | rows | filtered | Extra |
+----+-------------+-------+------------+------+---------------+------+---------+-------+------+----------+-------+
|  1 | SIMPLE      | t     | NULL       | ref  | idc           | idc  | 5       | const |    6 |   100.00 | NULL  |
  
3. index: 全表扫描，走索引且无需回表
 mysql> explain select c from t;
+----+-------------+-------+------------+-------+---------------+------+---------+------+------+----------+-------------+
| id | select_type | table | partitions | type  | possible_keys | key  | key_len | ref  | rows | filtered | Extra       |
+----+-------------+-------+------------+-------+---------------+------+---------+------+------+----------+-------------+
|  1 | SIMPLE      | t     | NULL       | index | NULL          | idc  | 5       | NULL |   10 |   100.00 | Using index |
+----+-------------+-------+------------+-------+---------------+------+---------+------+------+----------+-------------+
  
 4. all：全表扫描
  mysql> explain select * from t;
+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+
| id | select_type | table | partitions | type | possible_keys | key  | key_len | ref  | rows | filtered | Extra |
+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+
|  1 | SIMPLE      | t     | NULL       | ALL  | NULL          | NULL | NULL    | NULL |   10 |   100.00 | NULL  |
  
5. eq_ref: 比较的非等值
explain select * from t,t0 where t.a = t0.a;
+----+-------------+-------+------------+--------+---------------+---------+---------+-----------+------+----------+-------------+
| id | select_type | table | partitions | type   | possible_keys | key     | key_len | ref       | rows | filtered | Extra       |
+----+-------------+-------+------------+--------+---------------+---------+---------+-----------+------+----------+-------------+
|  1 | SIMPLE      | t0    | NULL       | ALL    | NULL          | NULL    | NULL    | NULL      |    3 |   100.00 | Using where |
|  1 | SIMPLE      | t     | NULL       | eq_ref | PRIMARY       | PRIMARY | 4       | test.t0.a |    1 |   100.00 | NULL        |

6. range
 mysql> explain format = tree select c from t where c < 10;
+-------------------------------------------------------------------------------------------------------------------------------------+
| EXPLAIN                                                                                                                             |
+-------------------------------------------------------------------------------------------------------------------------------------+
| -> Filter: (t.c < 10)  (cost=1.46 rows=6)
    -> Covering index range scan on t using idc over (NULL < c < 10)  (cost=1.46 rows=6)
 |
+-------------------------------------------------------------------------------------------------------------------------------------+
1 row in set (0.00 sec)

mysql> explain format = tree select c from t where a < 10;
+-------------------------------------------------------------------------------------------------------------------------+
| EXPLAIN                                                                                                                 |
+-------------------------------------------------------------------------------------------------------------------------+
| -> Filter: (t.a < 10)  (cost=2.06 rows=9)
    -> Index range scan on t using PRIMARY over (a < 10)  (cost=2.06 rows=9)
 |
+-------------------------------------------------------------------------------------------------------------------------+
```



# 使用限制：

-  无锁查询
-  聚簇索引（哪怕有二级索引也会退化成聚簇索引来并行）
-  当前只支持select count(*) 和 check table

```c++
|-->row_mysql_parallel_select_count_star()
|		|-->1. Parallel_reader reader(n_threads); //创建并行scan 线程对象
|		|-->2. Parallel_reader::Config config(FULL_SCAN, index);//创建config，参数1：扫描范围；参数2.具体索引。
|		|-->3. reader.add_scan(trx, config, [&](const Parallel_reader::Ctx *ctx) {Counter::inc(n_recs, ctx->thread_id())
|		|		|-->err = scan_ctx->partition(config.m_scan_range, ranges, 0)//从root阶段开始分区 第0层开始
|		|		|		|-->Parallel_reader::Scan_ctx::create_ranges() //创建分区集合
|		|		|		|		|-->page_cur_get_rec(&page_cursor) //从root结点开始找到一个rec
|		|		|		|		|-->btr_node_ptr_get_child_page_no()  //找到这个rec儿子结点的page no
|		|		|		|		|-->Parallel_reader::Scan_ctx::start_range()//在叶子结点找到记录，作为本次range的end以及下次start
|		|		|		|		|-->Parallel_reader::Scan_ctx::create_range() //把Iter构建出来，并加入到ranges中去
|		|		|		|		|-->page_cur_move_to_next() //移动下一个record记录，再循环
|		|		|-->Parallel_reader::Scan_ctx::create_context() //创建的range放到ctx中
|		|		|		|-->m_reader->enqueue(ctx); //压进队列中去
|		|-->4. Parallel_reader::run()
|		|		|-->Parallel_reader::parallel_read()
|		|		|		|-->os_thread_create()//创建worker线程，执行worker函数
  
//worker线程
|-->Parallel_reader::worker()
|		|-->ctx = dequeue() //取出ctx 进行scan
|		|-->Parallel_reader::Ctx::split() //判断是否需要二次分区，第一次没发均分则需二次分区
|		|		|--> m_scan_ctx->partition(scan_range, ranges, 1);//对第一层再次分区
|		|-->Parallel_reader::Ctx::traverse() //遍历分区树
|		|		|-->mtr.start(); //开启mtr 防止B+树出现smo
|		|		|-->Parallel_reader::Ctx::traverse_recs()
|		|		|		|-->1. page_cur_get_rec()//根据pcursor找到start位置
|		|		|		|-->2. end_tuple->compare(rec, index, offsets)//跟end比较，如果已经等于或者超过边界则跳出循环停止scan
|		|		|		|-->3. check_visibility() //如果已经到叶子结点，则判断可见性
|		|		|		|-->4. err = m_scan_ctx->m_f(this);//调用回掉函数  count++
|		|		|		|-->4. page_cur_move_to_next() //移到下一个record，继续循环
```





# 总结

MySQL8.0引入了并行查询虽然还比较初级，mysql已经在为未来做准备了（不过这个时间有点长，都过去5年了还没什么动静）。比如其封装了一个适配器类Parallel_reader_adapter来供上层使用，通过统一的接口process_rows()返回给上层使用。

process_rows

{

 1.将引擎记录转换成MySQL记录

 2.获取本线程的buffer信息(转换了多少mysql记录，发送了多少给上层)

 3.将MySQL记录填充进buffer，自增统计m_n_read

 4.调用回调函数处理(比如统计，聚合，排序等)，自增统计m_n_send

} 对于调用者来说，需要设置表的元信息，以及注入处理记录回调函数，比如处理聚集，排序，分组的工作。回调函数通过设置m_init_fn，m_load_fn和m_end_fn来控制。



```c++
//手动打开表并插入
pthread_rwlock_wrlock(&LOCK_insert_audit_log);
  previous_lock = thd->lock;
  thd->lock = nullptr;
  
  /* need to open before acquiring LOCK_plugin or it will deadlock */
  if (!(table = open_ltable(thd, &tables, TL_WRITE, MYSQL_LOCK_IGNORE_TIMEOUT))){
        thd->lock = previous_lock;
        pthread_rwlock_unlock(&LOCK_insert_audit_log);
        my_error(ER_OPEN_AUDITOR_LOG, MYF(0));
        LogErr(ERROR_LEVEL, ER_PCIE_ERR_INFO, "open audit log failed.");
        my_free(sql_create_user_command);
        my_free(sql_alter_user_command);
        return true;
  }
table->use_all_columns();

//读取数据
{
    if(table->file->ha_index_or_rnd_end() == 0) {
      if(0 != (error = table->file->ha_index_init(0, false))) {
        table->file->print_error(error, MYF(0));
        my_error(ER_WRITE_TO_AUDITOR_LOG, MYF(0));
        LogErr(ERROR_LEVEL, ER_PCIE_ERR_INFO, "audit_log ha_index_init failed.");
        goto err;
      }
      is_index_init = true;

      if(0 != (error = table->file->ha_index_last(table->record[0]))) {
        if(error == HA_ERR_END_OF_FILE)
          goto encrypt;                                  //empty table
        else {
          table->file->print_error(error, MYF(0));      //error 
          my_error(ER_WRITE_TO_AUDITOR_LOG, MYF(0));
          LogErr(ERROR_LEVEL, ER_PCIE_ERR_INFO, "audit_log ha_index_last.");
          goto err;
        } 
      }

      table_id = table->field[FIELD_AUDITOR_LOG_ID]->val_int() + 1;
      table->field[FIELD_AUDITOR_LOG_HMAC]->val_str(&str);
      
      if(strlen(last_auditor_log) == 0)
        memcpy(last_auditor_log,str.ptr(), str.length());
    }
  }

//填充field，并插入
  restore_record(table, s->default_values);
  table->field[FIELD_AUDITOR_LOG_ID]->store(table_id);                                                     
  table->field[FIELD_AUDITOR_LOG_HMAC]->store(hmac, ENCRYPT_LENGTH, system_charset_info); 
  
  if (0 != (error = table->file->ha_write_row(table->record[0]))) //插入
  
err:
  if (is_index_init) {
    table->file->ha_index_end();
  }
  my_free(source_data);
  my_free(sql_create_user_command);
  my_free(sql_alter_user_command);
  close_thread_tables(thd);



```



find_visible_record() //等价于row_search_mvcc()

move_to_next_block //类似于move_to_next_page()

