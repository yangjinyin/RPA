diff --git a/.gitignore b/.gitignore
index 5e696f6..ce4e9f9 100644
--- a/.gitignore
+++ b/.gitignore
@@ -18,7 +18,7 @@ source_downloads
 
 # Configuration files for Visual Studio Code
 .vscode/
-
+build/
 build/output
 build/release
 build/compute_node
diff --git a/include/mysql/plugin.h b/include/mysql/plugin.h
index 1b9cb8e..60c76a0 100644
--- a/include/mysql/plugin.h
+++ b/include/mysql/plugin.h
@@ -855,6 +855,11 @@ unsigned long thd_get_thread_id(const MYSQL_THD thd_g);
 void thd_get_xid(const MYSQL_THD thd_g, MYSQL_XID *xid);
 
 /**
+  Invalidate the query cache for a given table.
+*/
+void mysql_fast_query_cache_invalidate(char *fullname);
+
+/**
   Provide a handler data getter to simplify coding
 */
 void *thd_get_ha_data(const MYSQL_THD thd_g, const struct handlerton *hton);
diff --git a/include/mysql/plugin_audit.h.pp b/include/mysql/plugin_audit.h.pp
index a9db99d..9a70758 100644
--- a/include/mysql/plugin_audit.h.pp
+++ b/include/mysql/plugin_audit.h.pp
@@ -125,6 +125,7 @@ void thd_binlog_pos(const void * thd_g, const char **file_var,
                     unsigned long long *pos_var);
 unsigned long thd_get_thread_id(const void * thd_g);
 void thd_get_xid(const void * thd_g, MYSQL_XID *xid);
+void mysql_fast_query_cache_invalidate(char *fullname);
 void *thd_get_ha_data(const void * thd_g, const struct handlerton *hton);
 void thd_set_ha_data(void * thd_g, const struct handlerton *hton,
                      const void *ha_data);
diff --git a/include/mysql/plugin_auth.h.pp b/include/mysql/plugin_auth.h.pp
index 081fa0a..e4a2354 100644
--- a/include/mysql/plugin_auth.h.pp
+++ b/include/mysql/plugin_auth.h.pp
@@ -116,6 +116,7 @@ void thd_binlog_pos(const void * thd_g, const char **file_var,
                     unsigned long long *pos_var);
 unsigned long thd_get_thread_id(const void * thd_g);
 void thd_get_xid(const void * thd_g, MYSQL_XID *xid);
+void mysql_fast_query_cache_invalidate(char *fullname);
 void *thd_get_ha_data(const void * thd_g, const struct handlerton *hton);
 void thd_set_ha_data(void * thd_g, const struct handlerton *hton,
                      const void *ha_data);
diff --git a/include/mysql/plugin_ftparser.h.pp b/include/mysql/plugin_ftparser.h.pp
index 7d40d7c..1190788 100644
--- a/include/mysql/plugin_ftparser.h.pp
+++ b/include/mysql/plugin_ftparser.h.pp
@@ -116,6 +116,7 @@ void thd_binlog_pos(const void * thd_g, const char **file_var,
                     unsigned long long *pos_var);
 unsigned long thd_get_thread_id(const void * thd_g);
 void thd_get_xid(const void * thd_g, MYSQL_XID *xid);
+void mysql_fast_query_cache_invalidate(char *fullname);
 void *thd_get_ha_data(const void * thd_g, const struct handlerton *hton);
 void thd_set_ha_data(void * thd_g, const struct handlerton *hton,
                      const void *ha_data);
diff --git a/include/mysql/plugin_keyring.h.pp b/include/mysql/plugin_keyring.h.pp
index 2c2868c..f91ede1 100644
--- a/include/mysql/plugin_keyring.h.pp
+++ b/include/mysql/plugin_keyring.h.pp
@@ -116,6 +116,7 @@ void thd_binlog_pos(const void * thd_g, const char **file_var,
                     unsigned long long *pos_var);
 unsigned long thd_get_thread_id(const void * thd_g);
 void thd_get_xid(const void * thd_g, MYSQL_XID *xid);
+void mysql_fast_query_cache_invalidate(char *fullname);
 void *thd_get_ha_data(const void * thd_g, const struct handlerton *hton);
 void thd_set_ha_data(void * thd_g, const struct handlerton *hton,
                      const void *ha_data);
diff --git a/include/mysql_com.h b/include/mysql_com.h
index ab6cf49..65b9c9e 100644
--- a/include/mysql_com.h
+++ b/include/mysql_com.h
@@ -248,6 +248,7 @@
 #define REFRESH_FOR_EXPORT 0x100000L      /** FLUSH TABLES ... FOR EXPORT */
 #define REFRESH_OPTIMIZER_COSTS 0x200000L /** FLUSH OPTIMIZER_COSTS */
 #define REFRESH_PERSIST 0x400000L         /** RESET PERSIST */
+#define REFRESH_QUERY_CACHE 0x800000L    /** Flush QUERY CACHE */
 
 /** @}*/
 
diff --git a/share/messages_to_clients.txt b/share/messages_to_clients.txt
index 0b6962f..92aeaa7 100644
--- a/share/messages_to_clients.txt
+++ b/share/messages_to_clients.txt
@@ -9687,6 +9687,9 @@ ER_WARN_C_DISABLE_GTID_ONLY_WITH_SOURCE_AUTO_POS_INVALID_POS
 ER_DA_SSL_FIPS_MODE_ERROR
   eng "SSL fips mode error: %s"
 
+ER_MEMORY_POOL_FAILED
+  eng "Memory pool operation failed: %s"
+
 ER_VALUE_OUT_OF_RANGE
   eng "%s=%llu is outside the valid range [%llu,%llu]. %llu will be used."
 
diff --git a/sql-common/net_serv.cc b/sql-common/net_serv.cc
index 8b40475..ea019b3 100644
--- a/sql-common/net_serv.cc
+++ b/sql-common/net_serv.cc
@@ -59,6 +59,7 @@
 #include "mysql_com.h"
 #include "mysqld_error.h"
 #include "violite.h"
+#include "sql/mysqld.h"
 
 using std::max;
 using std::min;
@@ -86,6 +87,9 @@ using std::min;
   extern, but as it's hard to include sql_class.h here, we have to
   live with this for a while.
 */
+extern void fast_query_cache_insert(const uchar_t *packet, ulong length,
+                               unsigned pkt_nr);
+
 extern void thd_increment_bytes_sent(size_t length);
 extern void thd_increment_bytes_received(size_t length);
 
@@ -1289,6 +1293,11 @@ bool net_write_packet(NET *net, const uchar_t *packet, size_t length) {
   bool res;
   DBUG_TRACE;
 
+#if defined(MYSQL_SERVER)
+  if(fast_query_cache_enable && current_thd->fast_query_cache_store_query)
+    fast_query_cache_insert(packet, length, net->pkt_nr);
+#endif
+
   /* Socket can't be used */
   if (net->error == NET_ERROR_SOCKET_UNUSABLE ||
       net->error == NET_ERROR_SOCKET_NOT_WRITABLE)
diff --git a/sql/CMakeLists.txt b/sql/CMakeLists.txt
index 42666a5..5fc8e8e 100644
--- a/sql/CMakeLists.txt
+++ b/sql/CMakeLists.txt
@@ -444,6 +444,7 @@ SET(SQL_SHARED_SOURCES
   locking_service.cc
   locks/shared_spin_lock.cc
   log.cc
+  memory/mp.cc
   mdl.cc
   mdl_context_backup.cc
   migrate_keyring.cc
@@ -552,6 +553,7 @@ SET(SQL_SHARED_SOURCES
   sql_error.cc
   sql_exception_handler.cc
   sql_executor.cc
+  sql_fast_query_cache.cc
   sql_filter.cc
   sql_get_diagnostics.cc
   sql_gipk.cc
diff --git a/sql/handler.cc b/sql/handler.cc
index cc3827e..da0f37a 100644
--- a/sql/handler.cc
+++ b/sql/handler.cc
@@ -5371,6 +5371,23 @@ static bool check_if_system_table(const char *db, const char *table_name,
 }
 
 /**
+  Check if a table specified by name is a system table.
+
+  @param       db                         Database name for the table.
+  @param       table_name                 Table name to be checked.
+
+  @return Operation status
+    @retval    true              If the table name is a system table.
+    @retval    false             If the table name is a user-level table.
+*/
+bool check_if_system_table_for_query_cache(const char *db, const char *table_name) {
+  // Check if we have the system database name in the command.
+  if (!dd::get_dictionary()->is_dd_schema_name(db)) return false;
+
+  // Check if this is SQL layer system tables.
+  return (dd::get_dictionary()->is_system_table_name(db, table_name));
+}
+/**
   @brief Check if a given table is a system table.
 
   @details The primary purpose of introducing this function is to stop system
diff --git a/sql/handler.h b/sql/handler.h
index e48c177..2e298a4 100644
--- a/sql/handler.h
+++ b/sql/handler.h
@@ -139,7 +139,7 @@ struct sdi_vector_t {
   sdi_container m_vec;
 };
 
-typedef bool (*qc_engine_callback)(THD *thd_g, const char *table_key,
+typedef bool (*qc_engine_callback)(THD *thd_g, char *table_key,
                                    uint key_length, ulonglong_typ *engine_data);
 
 typedef bool(stat_print_fn)(THD *thd_g, const char *type, size_t type_len,
@@ -208,6 +208,12 @@ enum enum_alter_inplace_result {
   HA_ALTER_INPLACE_INSTANT
 };
 
+/* Table caching type */
+#define HA_CACHE_TBL_NONTRANSACT 0
+#define HA_CACHE_TBL_NOCACHE     1
+#define HA_CACHE_TBL_ASKTRANSACT 2
+#define HA_CACHE_TBL_TRANSACT    4
+
 enum rollback_type {
   ROLL_TABLESPACE,
   ROLL_DATABASE
@@ -1678,6 +1684,8 @@ typedef int (*find_files_t)(handlerton *hton, THD *thd_g, const char *db,
 typedef int (*table_exists_in_engine_t)(handlerton *hton, THD *thd_g,
                                         const char *db, const char *name);
 
+typedef void (*qc_invalidate_table_t)(THD *thd_g, const char *table_name);
+
 /**
   Let storage engine inspect the query Accesspath and pick whatever
   it like for being pushed down to the engine. (Join, conditions, ..)
@@ -2722,6 +2730,8 @@ struct handlerton {
   get_binlog_redo_write_lsn_t get_binlog_redo_write_lsn;
   innobase_gaia_slow_log_innodb_extension_t innobase_gaia_slow_log_innodb_extension;
 
+  qc_invalidate_table_t qc_invalidate_table; 
+
   get_binlog_in_redo_info_t get_binlog_in_redo_info;
   update_binlog_in_redo_info_t update_binlog_in_redo_info;
   secondary_recover_binlog_in_redo_t secondary_recover_binlog_in_redo;
@@ -5930,6 +5940,19 @@ class handler {
   virtual THR_LOCK_DATA **store_lock(THD *thd_g, THR_LOCK_DATA **to,
                                      enum thr_lock_type lock_type) = 0;
 
+  /** Type of table for caching query */
+  virtual uint8 table_cache_type() { return HA_CACHE_TBL_NONTRANSACT; }
+
+  virtual bool register_query_cache_table(THD *thd_g [[maybe_unused]], char *table_key [[maybe_unused]],
+                                             size_t key_length [[maybe_unused]],
+                                             qc_engine_callback *engine_callback,
+                                             ulonglong_typ *engine_data [[maybe_unused]])
+  {
+
+    *engine_callback= 0;
+    return true;
+  }
+
   /**
     Check if the primary key is clustered or not.
 
@@ -7348,6 +7371,7 @@ int ha_init_key_cache(std::string_view name, KEY_CACHE *key_cache);
 int ha_resize_key_cache(KEY_CACHE *key_cache);
 int ha_change_key_cache(KEY_CACHE *old_key_cache, KEY_CACHE *new_key_cache);
 
+bool check_if_system_table_for_query_cache(const char *db, const char *table_name);
 /* transactions: interface to handlerton functions */
 int ha_start_consistent_snapshot(THD *thd_g);
 int ha_commit_trans(THD *thd_g, bool all, bool ignore_global_read_lock = false);
diff --git a/sql/lex.h b/sql/lex.h
index d7387c4..dfef647 100644
--- a/sql/lex.h
+++ b/sql/lex.h
@@ -545,6 +545,7 @@ static const SYMBOL symbols[] = {
     {SYM("PROFILES", PROFILES_SYM)},
     {SYM("PROXY", PROXY_SYM)},
     {SYM("PURGE", PURGE)},
+    {SYM("QC", QC_SYM)},
     {SYM("QUARTER", QUARTER_SYM)},
     {SYM("QUERY", QUERY_SYM)},
     {SYM("QUICK", QUICK)},
diff --git a/sql/memory/mp.cc b/sql/memory/mp.cc
new file mode 100644
index 0000000..0c84953
--- /dev/null
+++ b/sql/memory/mp.cc
@@ -0,0 +1,1536 @@
+#include "mp.h"
+
+#include <cstdarg>
+#include <cstdio>
+
+#include "memory_debugging.h"
+#include "mysql/components/services/log_builtins.h"
+#include "mysqld_error.h"
+#include "sql/mysqld.h"
+#include "sql/memory/palloc.h"
+
+static void elog(eloglevel log_level, const char *format, ...)
+    __attribute__((format(gnu_printf, 2, 3)));
+
+static void elog(eloglevel log_level, const char *format, ...) {
+  char buffer[LOG_BUFF_MAX];
+
+  va_list args;
+  va_start(args, format);
+
+  vsnprintf(buffer, LOG_BUFF_MAX, format, args);
+
+  va_end(args);
+
+  LogErr(static_cast<std::underlying_type_t<eloglevel>>(log_level),
+         ER_MEMORY_POOL_FAILED, buffer);
+}
+
+/*
+ * AllocSetIsValid
+ *		True if set is valid allocation set.
+ */
+#define AllocSetIsValid(set) (set != nullptr)
+
+/*
+ * AllocBlockIsValid
+ *		True if block is valid block of allocation set.
+ */
+#define AllocBlockIsValid(block) (block != nullptr)
+
+#ifdef CLOBBER_FREED_MEMORY
+
+/* Wipe freed memory for debugging purposes */
+static inline void wipe_mem(void *ptr, size_t size) {
+  MEM_UNDEFINED(ptr, size);
+  memset(ptr, 0x7F, size);
+  MEM_NOACCESS(ptr, size);
+}
+
+#endif /* CLOBBER_FREED_MEMORY */
+
+#ifdef MEMORY_CONTEXT_CHECKING
+
+static inline void set_sentinel(void *base, Size offset) {
+  char *ptr = (char *)base + offset;
+
+  MEM_UNDEFINED(ptr, 1);
+  *ptr = 0x7E;
+  MEM_NOACCESS(ptr, 1);
+}
+
+static inline bool sentinel_ok(const void *base, Size offset) {
+  const char *ptr = (const char *)base + offset;
+  bool ret;
+
+  MEM_DEFINED_IF_ADDRESSABLE(ptr, 1);
+  ret = *ptr == 0x7E;
+  MEM_NOACCESS(ptr, 1);
+
+  return ret;
+}
+#endif /* MEMORY_CONTEXT_CHECKING */
+
+/*--------------------
+ * The first block allocated for an allocset has size initBlockSize.
+ * Each time we have to allocate another block, we double the block size
+ * (if possible, and without exceeding maxBlockSize), so as to reduce
+ * the bookkeeping load on malloc().
+ *
+ * Blocks allocated to hold oversize chunks do not follow this rule, however;
+ * they are just however big they need to be to hold that single chunk.
+ *
+ * Also, if a minContextSize is specified, the first block has that size,
+ * and then initBlockSize is used for the next one.
+ *--------------------
+ */
+
+#define ALLOC_BLOCKHDRSZ MAXALIGN(sizeof(AllocBlockData))
+#define ALLOC_CHUNKHDRSZ sizeof(MemoryChunk)
+
+/*
+ * We always store external chunks on a dedicated block.  This makes fetching
+ * the block from an external chunk easy since it's always the first and only
+ * chunk on the block.
+ */
+#define ExternalChunkGetBlock(chunk) \
+  (AllocBlockData *)((char *)chunk - ALLOC_BLOCKHDRSZ)
+
+/* Obtain the keeper block for an allocation set */
+#define KeeperBlock(set) \
+  ((AllocBlockData *)(((char *)set) + MAXALIGN(sizeof(AllocSetContext))))
+
+/* Check if the block is the keeper block of the given allocation set */
+#define IsKeeperBlock(set, block) ((block) == (KeeperBlock(set)))
+
+/*
+ * Obtain a AllocFreeListLink for the given chunk.  Allocation sizes are
+ * always at least sizeof(AllocFreeListLink), so we reuse the pointer's memory
+ * itself to store the freelist link.
+ */
+#define GetFreeListLink(chkptr) \
+  (AllocFreeListLink *)((char *)(chkptr) + ALLOC_CHUNKHDRSZ)
+
+/* Validate a freelist index retrieved from a chunk header */
+#define FreeListIdxIsValid(fidx) \
+  ((fidx) >= 0 && (fidx) < ALLOCSET_NUM_FREELISTS)
+
+/* Determine the size of the chunk based on the freelist index */
+#define GetChunkSizeFromFreeListIdx(fidx) \
+  ((((Size)1) << ALLOC_MINBITS) << (fidx))
+
+/* Get the MemoryChunk from the pointer */
+#define PointerGetMemoryChunk(p) \
+  ((MemoryChunk *)((char *)(p) - sizeof(MemoryChunk)))
+/* Get the pointer from the MemoryChunk */
+#define MemoryChunkGetPointer(c) ((void *)((char *)(c) + sizeof(MemoryChunk)))
+
+/* ----------
+ * AllocSetFreeIndex -
+ *
+ *		Depending on the size of an allocation compute which freechunk
+ *		list of the alloc set it belongs to.  Caller must have verified
+ *		that size <= ALLOC_CHUNK_LIMIT.
+ * ----------
+ */
+static inline int AllocSetFreeIndex(Size size) {
+  int idx;
+
+  if (size > (1 << ALLOC_MINBITS)) {
+    /*----------
+     * At this point we must compute ceil(log2(size >> ALLOC_MINBITS)).
+     * This is the same as
+     *		pg_leftmost_one_pos32((size - 1) >> ALLOC_MINBITS) + 1
+     * or equivalently
+     *		pg_leftmost_one_pos32(size - 1) - ALLOC_MINBITS + 1
+     *
+     * However, for platforms without intrinsic support, we duplicate the
+     * logic here, allowing an additional optimization.  It's reasonable
+     * to assume that ALLOC_CHUNK_LIMIT fits in 16 bits, so we can unroll
+     * the byte-at-a-time loop in pg_leftmost_one_pos32 and just handle
+     * the last two bytes.
+     *
+     * Yes, this function is enough of a hot-spot to make it worth this
+     * much trouble.
+     *----------
+     */
+#ifdef HAVE_BITSCAN_REVERSE
+    idx = pg_leftmost_one_pos32((uint32)size - 1) - ALLOC_MINBITS + 1;
+#else
+    uint32 t, tsize;
+
+    /* Statically assert that we only have a 16-bit input value. */
+    StaticAssertDecl(ALLOC_CHUNK_LIMIT < (1 << 16),
+                     "ALLOC_CHUNK_LIMIT must be less than 64kB");
+
+    tsize = size - 1;
+    t = tsize >> 8;
+    idx = t ? pg_leftmost_one_pos[t] + 8 : pg_leftmost_one_pos[tsize];
+    idx -= ALLOC_MINBITS - 1;
+#endif
+
+    assert(idx < ALLOCSET_NUM_FREELISTS);
+  } else
+    idx = 0;
+
+  return idx;
+}
+
+/*
+ * MemoryChunkSetHdrMask
+ *		Store the given 'block', 'chunk_size' and 'methodid' in the
+ *given MemoryChunk.
+ *
+ * The number of bytes between 'block' and 'chunk' must be <=
+ * MEMORYCHUNK_MAX_BLOCKOFFSET.
+ * 'value' must be <= MEMORYCHUNK_MAX_VALUE.
+ * Both 'chunk' and 'block' must be MAXALIGNed pointers.
+ */
+static inline void MemoryChunkSetHdrMask(MemoryChunk *chunk, void *block,
+                                         Size value,
+                                         MemoryContextMethodID methodid) {
+  Size blockoffset = (char *)chunk - (char *)block;
+
+  assert((char *)chunk >= (char *)block);
+  assert((blockoffset & MEMORYCHUNK_BLOCKOFFSET_MASK) == blockoffset);
+  assert(value <= MEMORYCHUNK_MAX_VALUE);
+  assert((int)methodid <= MEMORY_CONTEXT_METHODID_MASK);
+
+  chunk->hdrmask = (((uint64)blockoffset) << MEMORYCHUNK_BLOCKOFFSET_BASEBIT) |
+                   (((uint64)value) << MEMORYCHUNK_VALUE_BASEBIT) | methodid;
+}
+
+/*
+ * MemoryChunkSetHdrMaskExternal
+ *		Set 'chunk' as an externally managed chunk.  Here we only record
+ *the MemoryContextMethodID and set the external chunk bit.
+ */
+static inline void MemoryChunkSetHdrMaskExternal(
+    MemoryChunk *chunk, MemoryContextMethodID methodid) {
+  assert((int)methodid <= MEMORY_CONTEXT_METHODID_MASK);
+
+  chunk->hdrmask = MEMORYCHUNK_MAGIC |
+                   (((uint64)1) << MEMORYCHUNK_EXTERNAL_BASEBIT) | methodid;
+}
+
+/*
+ * MemoryChunkIsExternal
+ *		Return true if 'chunk' is marked as external.
+ */
+static inline bool MemoryChunkIsExternal(MemoryChunk *chunk) {
+  /*
+   * External chunks should always store MEMORYCHUNK_MAGIC in the upper
+   * portion of the hdrmask, check that nothing has stomped on that.
+   */
+  assert(!HdrMaskIsExternal(chunk->hdrmask) ||
+         HdrMaskCheckMagic(chunk->hdrmask));
+
+  return HdrMaskIsExternal(chunk->hdrmask);
+}
+
+/*
+ * MemoryChunkGetValue
+ *		For non-external chunks, returns the value field as it was set
+ *in MemoryChunkSetHdrMask.
+ */
+static inline Size MemoryChunkGetValue(MemoryChunk *chunk) {
+  assert(!HdrMaskIsExternal(chunk->hdrmask));
+
+  return HdrMaskGetValue(chunk->hdrmask);
+}
+
+/*
+ * MemoryChunkGetBlock
+ *		For non-external chunks, returns the pointer to the block as was
+ *set in MemoryChunkSetHdrMask.
+ */
+static inline void *MemoryChunkGetBlock(MemoryChunk *chunk) {
+  assert(!HdrMaskIsExternal(chunk->hdrmask));
+
+  return (void *)((char *)chunk - HdrMaskBlockOffset(chunk->hdrmask));
+}
+
+static inline void RelinkGroup(MemoryChunk *prev, MemoryChunk *next,
+                               MemoryChunk *chunk) {
+  if (prev != nullptr) {
+    prev->next = chunk;
+    chunk->prev = prev;
+  }
+
+  if (next != nullptr) {
+    next->prev = chunk;
+    chunk->next = next;
+  }
+}
+
+void MemoryPool::init() {
+  mysql_mutex_init(key_Memory_pool_mutex, &m_lock, MY_MUTEX_INIT_SLOW);
+
+  assert(TopMemoryContext == nullptr);
+
+  /*
+   * First, initialize TopMemoryContext, which is the parent of all others.
+   */
+  TopMemoryContext =
+      AllocSetContextCreate("TopMemoryContext", ALLOCSET_DEFAULT_SIZES);
+}
+
+void MemoryPool::close() {
+  if (TopMemoryContext == nullptr) return;
+
+  pdelete();
+  mysql_mutex_destroy(&m_lock);
+}
+
+/*
+ * AllocSetContextCreateInternal
+ *		Create a new AllocSet context.
+ *
+ * parent: parent context, or nullptr if top-level context
+ * name: name of context (must be statically allocated)
+ * minContextSize: minimum context size
+ * initBlockSize: initial allocation block size
+ * maxBlockSize: maximum allocation block size
+ *
+ * Most callers should abstract the context size parameters using a macro
+ * such as ALLOCSET_DEFAULT_SIZES.
+ *
+ * Note: don't call this directly; go through the wrapper macro
+ * AllocSetContextCreate.
+ */
+AllocSetContext *MemoryPool::AllocSetContextCreate(const char *name,
+                                                   Size minContextSize,
+                                                   Size initBlockSize,
+                                                   Size maxBlockSize) {
+  int freeListIndex;
+  Size firstBlockSize;
+  AllocSetContext *set;
+  AllocBlockData *block;
+
+  /* ensure MemoryChunk's size is properly maxaligned */
+  StaticAssertDecl(ALLOC_CHUNKHDRSZ == MAXALIGN(ALLOC_CHUNKHDRSZ),
+                   "sizeof(MemoryChunk) is not maxaligned");
+  /* check we have enough space to store the freelist link */
+  StaticAssertDecl(
+      sizeof(AllocFreeListLink) <= (1 << ALLOC_MINBITS),
+      "sizeof(AllocFreeListLink) larger than minimum allocation size");
+
+  /*
+   * First, validate allocation parameters.  Once these were regular runtime
+   * tests and elog's, but in practice Asserts seem sufficient because
+   * nobody varies their parameters at runtime.  We somewhat arbitrarily
+   * enforce a minimum 1K block size.  We restrict the maximum block size to
+   * MEMORYCHUNK_MAX_BLOCKOFFSET as MemoryChunks are limited to this in
+   * regards to addressing the offset between the chunk and the block that
+   * the chunk is stored on.  We would be unable to store the offset between
+   * the chunk and block for any chunks that were beyond
+   * MEMORYCHUNK_MAX_BLOCKOFFSET bytes into the block if the block was to be
+   * larger than this.
+   */
+  assert(initBlockSize == MAXALIGN(initBlockSize) && initBlockSize >= 1024);
+  assert(maxBlockSize == MAXALIGN(maxBlockSize) &&
+         maxBlockSize >= initBlockSize &&
+         AllocHugeSizeIsValid(maxBlockSize)); /* must be safe to double */
+  assert(minContextSize == 0 ||
+         (minContextSize == MAXALIGN(minContextSize) &&
+          minContextSize >= 1024 && minContextSize <= maxBlockSize));
+  assert(maxBlockSize <= MEMORYCHUNK_MAX_BLOCKOFFSET);
+
+  /*
+   * Check whether the parameters match either available freelist.  We do
+   * not need to demand a match of maxBlockSize.
+   */
+  if (minContextSize == ALLOCSET_DEFAULT_MINSIZE &&
+      initBlockSize == ALLOCSET_DEFAULT_INITSIZE)
+    freeListIndex = 0;
+  else if (minContextSize == ALLOCSET_SMALL_MINSIZE &&
+           initBlockSize == ALLOCSET_SMALL_INITSIZE)
+    freeListIndex = 1;
+  else
+    freeListIndex = -1;
+
+  /* Determine size of initial block */
+  firstBlockSize =
+      MAXALIGN(sizeof(AllocSetContext)) + ALLOC_BLOCKHDRSZ + ALLOC_CHUNKHDRSZ;
+  if (minContextSize != 0)
+    firstBlockSize = std::max(firstBlockSize, minContextSize);
+  else
+    firstBlockSize = std::max(firstBlockSize, initBlockSize);
+
+  /*
+   * Allocate the initial block.  Unlike other aset.c blocks, it starts with
+   * the context header and its block header follows that.
+   */
+  set = (AllocSetContext *)malloc(firstBlockSize);
+  if (set == nullptr) {
+    // error todo
+    return nullptr;
+  }
+
+  /*
+   * Avoid writing code that can fail between here and MemoryContextCreate;
+   * we'd leak the header/initial block if we ereport in this stretch.
+   */
+
+  /* Fill in the initial block's block header */
+  block = KeeperBlock(set);
+  block->freeptr = ((char *)block) + ALLOC_BLOCKHDRSZ;
+  block->endptr = ((char *)set) + firstBlockSize;
+  block->prev = nullptr;
+  block->next = nullptr;
+
+  /* Remember block as part of block list */
+  set->blocks = block;
+
+  /* Finish filling in aset-specific parts of the context header */
+  MemSetAligned(set->freelist, 0, sizeof(set->freelist));
+
+  set->initBlockSize = (uint32)initBlockSize;
+  set->maxBlockSize = (uint32)maxBlockSize;
+  set->nextBlockSize = (uint32)initBlockSize;
+  set->freeListIndex = freeListIndex;
+
+  /*
+   * Compute the allocation chunk size limit for this context.  It can't be
+   * more than ALLOC_CHUNK_LIMIT because of the fixed number of freelists.
+   * If maxBlockSize is small then requests exceeding the maxBlockSize, or
+   * even a significant fraction of it, should be treated as large chunks
+   * too.  For the typical case of maxBlockSize a power of 2, the chunk size
+   * limit will be at most 1/8th maxBlockSize, so that given a stream of
+   * requests that are all the maximum chunk size we will waste at most
+   * 1/8th of the allocated space.
+   *
+   * Also, allocChunkLimit must not exceed ALLOCSET_SEPARATE_THRESHOLD.
+   */
+  assert(ALLOC_CHUNK_LIMIT == ALLOCSET_SEPARATE_THRESHOLD);
+
+  /*
+   * Determine the maximum size that a chunk can be before we allocate an
+   * entire AllocBlockData * dedicated for that chunk.  We set the absolute
+   * limit of that size as ALLOC_CHUNK_LIMIT but we reduce it further so that we
+   * can fit about ALLOC_CHUNK_FRACTION chunks this size on a maximally
+   * sized block.  (We opt to keep allocChunkLimit a power-of-2 value
+   * primarily for legacy reasons rather than calculating it so that exactly
+   * ALLOC_CHUNK_FRACTION chunks fit on a maximally sized block.)
+   */
+  set->allocChunkLimit = ALLOC_CHUNK_LIMIT;
+  while ((Size)(set->allocChunkLimit + ALLOC_CHUNKHDRSZ) >
+         (Size)((maxBlockSize - ALLOC_BLOCKHDRSZ) / ALLOC_CHUNK_FRACTION))
+    set->allocChunkLimit >>= 1;
+
+  set->isReset = true;
+  set->name = name;
+  set->reset_cbs = nullptr;
+  set->allowInCritSection = false;
+  set->mem_allocated = firstBlockSize;
+
+  return (AllocSetContext *)set;
+}
+
+/*
+ * AllocSetAlloc
+ *		Returns a pointer to allocated memory of given size or raises an
+ *ERROR on allocation failure, or returns nullptr when flags contains
+ *		MCXT_ALLOC_NO_OOM.
+ *
+ * No request may exceed:
+ *		MAXALIGN_DOWN(SIZE_MAX) - ALLOC_BLOCKHDRSZ - ALLOC_CHUNKHDRSZ
+ * All callers use a much-lower limit.
+ *
+ * Note: when using valgrind, it doesn't matter how the returned allocation
+ * is marked, as mcxt.c will set it to UNDEFINED.  In some paths we will
+ * return space that is marked NOACCESS - AllocSetRealloc has to beware!
+ *
+ * This function should only contain the most common code paths.  Everything
+ * else should be in pg_noinline helper functions, thus avoiding the overhead
+ * of creating a stack frame for the common cases.  Allocating memory is often
+ * a bottleneck in many workloads, so avoiding stack frame setup is
+ * worthwhile.  Helper functions should always directly return the newly
+ * allocated memory so that we can just return that address directly as a tail
+ * call.
+ */
+void *MemoryPool::AllocSetAlloc(Size size, int64 group_id) {
+  mysql_mutex_assert_owner(&m_lock);
+
+  AllocSetContext *set = TopMemoryContext;
+  AllocBlockData *block;
+  MemoryChunk *chunk;
+  Size fidx;
+  Size chunk_size;
+  Size availspace;
+
+  /* due to the keeper block set->blocks should never be nullptr */
+  assert(set->blocks != nullptr);
+
+  /*
+   * If requested size exceeds maximum for chunks we hand the request off to
+   * AllocSetAllocLarge().
+   */
+  if (size > set->allocChunkLimit) return AllocSetAllocLarge(size, group_id);
+
+  /*
+   * Request is small enough to be treated as a chunk.  Look in the
+   * corresponding free list to see if there is a free chunk we could reuse.
+   * If one is found, remove it from the free list, make it again a member
+   * of the alloc set and return its data address.
+   *
+   * Note that we don't attempt to ensure there's space for the sentinel
+   * byte here.  We expect a large proportion of allocations to be for sizes
+   * which are already a power of 2.  If we were to always make space for a
+   * sentinel byte in MEMORY_CONTEXT_CHECKING builds, then we'd end up
+   * doubling the memory requirements for such allocations.
+   */
+  fidx = AllocSetFreeIndex(size);
+  chunk = set->freelist[fidx];
+  if (chunk != nullptr) {
+    AllocFreeListLink *link = GetFreeListLink(chunk);
+
+    assert(fidx == MemoryChunkGetValue(chunk));
+
+    /* pop this chunk off the freelist */
+    set->freelist[fidx] = link->next;
+
+#ifdef MEMORY_CONTEXT_CHECKING
+    chunk->requested_size = size;
+    /* set mark to catch clobber of "unused" space */
+    if (size < GetChunkSizeFromFreeListIdx(fidx))
+      set_sentinel(MemoryChunkGetPointer(chunk), size);
+#endif
+
+    MemoryChunkAddGroup(chunk, group_id);
+
+    return MemoryChunkGetPointer(chunk);
+  }
+
+  /*
+   * Choose the actual chunk size to allocate.
+   */
+  chunk_size = GetChunkSizeFromFreeListIdx(fidx);
+  assert(chunk_size >= size);
+
+  block = set->blocks;
+  availspace = block->endptr - block->freeptr;
+
+  /*
+   * If there is enough room in the active allocation block, we will put the
+   * chunk into that block.  Else must start a new one.
+   */
+  if (unlikely(availspace < (chunk_size + ALLOC_CHUNKHDRSZ)))
+    return AllocSetAllocFromNewBlock(size, fidx, group_id);
+
+  /* There's enough space on the current block, so allocate from that */
+  return AllocSetAllocChunkFromBlock(block, size, chunk_size, fidx, group_id);
+}
+
+/*
+ * Helper for AllocSetAlloc() that allocates an entire block for the chunk.
+ *
+ * AllocSetAlloc()'s comment explains why this is separate.
+ */
+void *MemoryPool::AllocSetAllocLarge(Size size, int64 group_id) {
+  mysql_mutex_assert_owner(&m_lock);
+
+  AllocSetContext *set = TopMemoryContext;
+  AllocBlockData *block;
+  MemoryChunk *chunk;
+  Size chunk_size;
+  Size blksize;
+
+#ifdef MEMORY_CONTEXT_CHECKING
+  /* ensure there's always space for the sentinel byte */
+  chunk_size = MAXALIGN(size + 1);
+#else
+  chunk_size = MAXALIGN(size);
+#endif
+
+  blksize = chunk_size + ALLOC_BLOCKHDRSZ + ALLOC_CHUNKHDRSZ;
+  block = (AllocBlockData *)malloc(blksize);
+  if (block == nullptr) return nullptr;
+
+  set->mem_allocated += blksize;
+
+  block->freeptr = block->endptr = ((char *)block) + blksize;
+
+  chunk = (MemoryChunk *)(((char *)block) + ALLOC_BLOCKHDRSZ);
+
+  MemoryChunkAddGroup(chunk, group_id);
+
+  /* mark the MemoryChunk as externally managed */
+  MemoryChunkSetHdrMaskExternal(chunk, MCTX_ASET_ID);
+
+#ifdef MEMORY_CONTEXT_CHECKING
+  chunk->requested_size = size;
+  /* set mark to catch clobber of "unused" space */
+  assert(size < chunk_size);
+  set_sentinel(MemoryChunkGetPointer(chunk), size);
+#endif
+
+  /*
+   * Stick the new block underneath the active allocation block, if any, so
+   * that we don't lose the use of the space remaining therein.
+   */
+  if (set->blocks != nullptr) {
+    block->prev = set->blocks;
+    block->next = set->blocks->next;
+    if (block->next) block->next->prev = block;
+    set->blocks->next = block;
+  } else {
+    block->prev = nullptr;
+    block->next = nullptr;
+    set->blocks = block;
+  }
+
+  /* Ensure any padding bytes are marked NOACCESS. */
+  MEM_NOACCESS((char *)MemoryChunkGetPointer(chunk) + size, chunk_size - size);
+
+  /* Disallow access to the chunk header. */
+  MEM_NOACCESS(chunk, ALLOC_CHUNKHDRSZ);
+
+  return MemoryChunkGetPointer(chunk);
+}
+
+/*
+ * Helper for AllocSetAlloc() that allocates a new block and returns a chunk
+ * allocated from it.
+ *
+ * AllocSetAlloc()'s comment explains why this is separate.
+ */
+void *MemoryPool::AllocSetAllocFromNewBlock(Size size, int fidx,
+                                            int64 group_id) {
+  mysql_mutex_assert_owner(&m_lock);
+
+  AllocSetContext *set = TopMemoryContext;
+  AllocBlockData *block;
+  Size availspace;
+  Size blksize;
+  Size required_size;
+  Size chunk_size;
+
+  /* due to the keeper block set->blocks should always be valid */
+  assert(set->blocks != nullptr);
+  block = set->blocks;
+  availspace = block->endptr - block->freeptr;
+
+  /*
+   * The existing active (top) block does not have enough room for the
+   * requested allocation, but it might still have a useful amount of space
+   * in it.  Once we push it down in the block list, we'll never try to
+   * allocate more space from it. So, before we do that, carve up its free
+   * space into chunks that we can put on the set's freelists.
+   *
+   * Because we can only get here when there's less than ALLOC_CHUNK_LIMIT
+   * left in the block, this loop cannot iterate more than
+   * ALLOCSET_NUM_FREELISTS-1 times.
+   */
+  while (availspace >= ((1 << ALLOC_MINBITS) + ALLOC_CHUNKHDRSZ)) {
+    AllocFreeListLink *link;
+    MemoryChunk *chunk;
+    Size availchunk = availspace - ALLOC_CHUNKHDRSZ;
+    int a_fidx = AllocSetFreeIndex(availchunk);
+
+    /*
+     * In most cases, we'll get back the index of the next larger freelist
+     * than the one we need to put this chunk on.  The exception is when
+     * availchunk is exactly a power of 2.
+     */
+    if (availchunk != GetChunkSizeFromFreeListIdx(a_fidx)) {
+      a_fidx--;
+      assert(a_fidx >= 0);
+      availchunk = GetChunkSizeFromFreeListIdx(a_fidx);
+    }
+
+    chunk = (MemoryChunk *)(block->freeptr);
+
+    /* Prepare to initialize the chunk header. */
+    MEM_UNDEFINED(chunk, ALLOC_CHUNKHDRSZ);
+    block->freeptr += (availchunk + ALLOC_CHUNKHDRSZ);
+    availspace -= (availchunk + ALLOC_CHUNKHDRSZ);
+
+    /* store the freelist index in the value field */
+    MemoryChunkSetHdrMask(chunk, block, a_fidx, MCTX_ASET_ID);
+#ifdef MEMORY_CONTEXT_CHECKING
+    chunk->requested_size = InvalidAllocSize; /* mark it free */
+#endif
+    /* push this chunk onto the free list */
+    link = GetFreeListLink(chunk);
+
+    MEM_DEFINED_IF_ADDRESSABLE(link, sizeof(AllocFreeListLink));
+    link->next = set->freelist[a_fidx];
+    MEM_NOACCESS(link, sizeof(AllocFreeListLink));
+
+    set->freelist[a_fidx] = chunk;
+  }
+
+  /*
+   * The first such block has size initBlockSize, and we double the space in
+   * each succeeding block, but not more than maxBlockSize.
+   */
+  blksize = set->nextBlockSize;
+  set->nextBlockSize <<= 1;
+  if (set->nextBlockSize > set->maxBlockSize)
+    set->nextBlockSize = set->maxBlockSize;
+
+  /* Choose the actual chunk size to allocate */
+  chunk_size = GetChunkSizeFromFreeListIdx(fidx);
+  assert(chunk_size >= size);
+
+  /*
+   * If initBlockSize is less than ALLOC_CHUNK_LIMIT, we could need more
+   * space... but try to keep it a power of 2.
+   */
+  required_size = chunk_size + ALLOC_BLOCKHDRSZ + ALLOC_CHUNKHDRSZ;
+  while (blksize < required_size) blksize <<= 1;
+
+  /* Try to allocate it */
+  block = (AllocBlockData *)malloc(blksize);
+
+  /*
+   * We could be asking for pretty big blocks here, so cope if malloc fails.
+   * But give up if there's less than 1 MB or so available...
+   */
+  while (block == nullptr && blksize > 1024 * 1024) {
+    blksize >>= 1;
+    if (blksize < required_size) break;
+    block = (AllocBlockData *)malloc(blksize);
+  }
+
+  if (block == nullptr) return nullptr;
+
+  set->mem_allocated += blksize;
+
+  block->freeptr = ((char *)block) + ALLOC_BLOCKHDRSZ;
+  block->endptr = ((char *)block) + blksize;
+
+  /* Mark unallocated space NOACCESS. */
+  MEM_NOACCESS(block->freeptr, blksize - ALLOC_BLOCKHDRSZ);
+
+  block->prev = nullptr;
+  block->next = set->blocks;
+  if (block->next) block->next->prev = block;
+  set->blocks = block;
+
+  return AllocSetAllocChunkFromBlock(block, size, chunk_size, fidx, group_id);
+}
+
+/*
+ * Small helper for allocating a new chunk from a chunk, to avoid duplicating
+ * the code between AllocSetAlloc() and AllocSetAllocFromNewBlock().
+ */
+void *MemoryPool::AllocSetAllocChunkFromBlock(AllocBlockData *block,
+                                              Size size [[maybe_unused]],
+                                              Size chunk_size, int fidx,
+                                              int64 group_id) {
+  mysql_mutex_assert_owner(&m_lock);
+
+  MemoryChunk *chunk;
+
+  chunk = (MemoryChunk *)(block->freeptr);
+
+  /* Prepare to initialize the chunk header. */
+  MEM_UNDEFINED(chunk, ALLOC_CHUNKHDRSZ);
+
+  block->freeptr += (chunk_size + ALLOC_CHUNKHDRSZ);
+  assert(block->freeptr <= block->endptr);
+
+  MemoryChunkAddGroup(chunk, group_id);
+
+  /* store the free list index in the value field */
+  MemoryChunkSetHdrMask(chunk, block, fidx, MCTX_ASET_ID);
+
+#ifdef MEMORY_CONTEXT_CHECKING
+  chunk->requested_size = size;
+  /* set mark to catch clobber of "unused" space */
+  if (size < chunk_size) set_sentinel(MemoryChunkGetPointer(chunk), size);
+#endif
+
+  /* Ensure any padding bytes are marked NOACCESS. */
+  MEM_NOACCESS((char *)MemoryChunkGetPointer(chunk) + size, chunk_size - size);
+
+  /* Disallow access to the chunk header. */
+  MEM_NOACCESS(chunk, ALLOC_CHUNKHDRSZ);
+
+  return MemoryChunkGetPointer(chunk);
+}
+
+/*
+ * AllocSetFree
+ *		Frees allocated memory; memory is removed from the set.
+ */
+void MemoryPool::AllocSetFree(void *pointer) {
+  mysql_mutex_assert_owner(&m_lock);
+
+  AllocSetContext *set = TopMemoryContext;
+  MemoryChunk *chunk = PointerGetMemoryChunk(pointer);
+
+  /* Allow access to the chunk header. */
+  MEM_DEFINED_IF_ADDRESSABLE(chunk, ALLOC_CHUNKHDRSZ);
+
+  if (MemoryChunkIsExternal(chunk)) {
+    /* Release single-chunk block. */
+    AllocBlockData *block = ExternalChunkGetBlock(chunk);
+
+    /*
+     * Try to verify that we have a sane block pointer: the block header
+     * should reference an aset and the freeptr should match the endptr.
+     */
+    if (block->freeptr != block->endptr)
+      elog(eloglevel::ERROR, "could not find block containing chunk %p", chunk);
+
+#ifdef MEMORY_CONTEXT_CHECKING
+    {
+      /* Test for someone scribbling on unused space in chunk */
+      assert(chunk->requested_size < (Size)(block->endptr - (char *)pointer));
+      if (!sentinel_ok(pointer, chunk->requested_size))
+        elog(eloglevel::WARNING, "detected write past chunk end in %s %p",
+             set->name, chunk);
+    }
+#endif
+
+    /* OK, remove block from aset's list and free it */
+    if (block->prev)
+      block->prev->next = block->next;
+    else
+      set->blocks = block->next;
+    if (block->next) block->next->prev = block->prev;
+
+    set->mem_allocated -= block->endptr - ((char *)block);
+
+#ifdef CLOBBER_FREED_MEMORY
+    wipe_mem(block, block->freeptr - ((char *)block));
+#endif
+    free(block);
+  } else {
+    int fidx;
+    AllocFreeListLink *link;
+
+    fidx = MemoryChunkGetValue(chunk);
+    assert(FreeListIdxIsValid(fidx));
+    link = GetFreeListLink(chunk);
+
+#ifdef MEMORY_CONTEXT_CHECKING
+    /* Test for someone scribbling on unused space in chunk */
+    if (chunk->requested_size < GetChunkSizeFromFreeListIdx(fidx))
+      if (!sentinel_ok(pointer, chunk->requested_size))
+        elog(eloglevel::WARNING, "detected write past chunk end in %s %p",
+             set->name, chunk);
+#endif
+
+#ifdef CLOBBER_FREED_MEMORY
+    wipe_mem(pointer, GetChunkSizeFromFreeListIdx(fidx));
+#endif
+    /* push this chunk onto the top of the free list */
+    MEM_DEFINED_IF_ADDRESSABLE(link, sizeof(AllocFreeListLink));
+    link->next = set->freelist[fidx];
+    MEM_NOACCESS(link, sizeof(AllocFreeListLink));
+    set->freelist[fidx] = chunk;
+
+#ifdef MEMORY_CONTEXT_CHECKING
+
+    /*
+     * Reset requested_size to InvalidAllocSize in chunks that are on free
+     * list.
+     */
+    chunk->requested_size = InvalidAllocSize;
+#endif
+  }
+}
+
+/*
+ * AllocSetFree
+ *		Release all memory on this memory group and clear this group
+ */
+void MemoryPool::AllocSetFree(void *pointer, int64 group_id) {
+  const auto it = m_group_map.find(group_id);
+
+  if (it == m_group_map.end()) {
+    if (pointer != nullptr) AllocSetFree(pointer);
+
+    return;
+  }
+
+  MemoryChunk *chunk = it->second;
+
+  while (chunk != nullptr) {
+    void *ptr = MemoryChunkGetPointer(chunk);
+    chunk = chunk->next;
+    // chunk->next = nullptr;
+
+    if (ptr != nullptr) AllocSetFree(ptr);
+  }
+
+  m_group_map.erase(it);
+}
+
+/*
+ * AllocSetRealloc
+ *		Returns new pointer to allocated memory of given size or nullptr
+ *if request could not be completed; this memory is added to the set. Memory
+ *associated with given pointer is copied into the new memory, and the old
+ *memory is freed.
+ *
+ * Without MEMORY_CONTEXT_CHECKING, we don't know the old request size.  This
+ * makes our Valgrind client requests less-precise, hazarding false negatives.
+ * (In principle, we could use VALGRIND_GET_VBITS() to rediscover the old
+ * request size.)
+ */
+void *MemoryPool::AllocSetRealloc(void *pointer, Size size) {
+  mysql_mutex_assert_owner(&m_lock);
+
+  AllocBlockData *block;
+  AllocSetContext *set = TopMemoryContext;
+  MemoryChunk *chunk = PointerGetMemoryChunk(pointer);
+  MemoryChunk *cprev = chunk->prev;
+  MemoryChunk *cnext = chunk->next;
+  Size oldchksize;
+  int fidx;
+
+  /* Allow access to the chunk header. */
+  MEM_DEFINED_IF_ADDRESSABLE(chunk, ALLOC_CHUNKHDRSZ);
+
+  if (MemoryChunkIsExternal(chunk)) {
+    /*
+     * The chunk must have been allocated as a single-chunk block.  Use
+     * realloc() to make the containing block bigger, or smaller, with
+     * minimum space wastage.
+     */
+    Size chksize;
+    Size blksize;
+    Size oldblksize;
+
+    block = ExternalChunkGetBlock(chunk);
+
+    /*
+     * Try to verify that we have a sane block pointer: the block header
+     * should reference an aset and the freeptr should match the endptr.
+     */
+    if (block->freeptr != block->endptr)
+      elog(eloglevel::ERROR, "could not find block containing chunk %p", chunk);
+
+    oldchksize = block->endptr - (char *)pointer;
+
+#ifdef MEMORY_CONTEXT_CHECKING
+    /* Test for someone scribbling on unused space in chunk */
+    assert(chunk->requested_size < oldchksize);
+    if (!sentinel_ok(pointer, chunk->requested_size))
+      elog(eloglevel::WARNING, "detected write past chunk end in %s %p",
+           set->name, chunk);
+#endif
+
+#ifdef MEMORY_CONTEXT_CHECKING
+    /* ensure there's always space for the sentinel byte */
+    chksize = MAXALIGN(size + 1);
+#else
+    chksize = MAXALIGN(size);
+#endif
+
+    /* Do the realloc */
+    blksize = chksize + ALLOC_BLOCKHDRSZ + ALLOC_CHUNKHDRSZ;
+    oldblksize = block->endptr - ((char *)block);
+
+    AllocBlockData *temp = (AllocBlockData *)realloc(block, blksize);
+    if (temp == nullptr) {
+      /* Disallow access to the chunk header. */
+      MEM_NOACCESS(chunk, ALLOC_CHUNKHDRSZ);
+      free(block);
+      return nullptr;
+    }
+    
+    block = temp;
+
+    /* updated separately, not to underflow when (oldblksize > blksize) */
+    set->mem_allocated -= oldblksize;
+    set->mem_allocated += blksize;
+
+    block->freeptr = block->endptr = ((char *)block) + blksize;
+
+    /* Update pointers since block has likely been moved */
+    chunk = (MemoryChunk *)(((char *)block) + ALLOC_BLOCKHDRSZ);
+
+    RelinkGroup(cprev, cnext, chunk);
+
+    pointer = MemoryChunkGetPointer(chunk);
+    if (block->prev)
+      block->prev->next = block;
+    else
+      set->blocks = block;
+    if (block->next) block->next->prev = block;
+
+#ifdef MEMORY_CONTEXT_CHECKING
+#ifdef RANDOMIZE_ALLOCATED_MEMORY
+
+    /*
+     * We can only randomize the extra space if we know the prior request.
+     * When using Valgrind, randomize_mem() also marks memory UNDEFINED.
+     */
+    if (size > chunk->requested_size)
+      randomize_mem((char *)pointer + chunk->requested_size,
+                    size - chunk->requested_size);
+#else
+
+      /*
+       * If this is an increase, realloc() will have marked any
+       * newly-allocated part (from oldchksize to chksize) UNDEFINED, but we
+       * also need to adjust trailing bytes from the old allocation (from
+       * chunk->requested_size to oldchksize) as they are marked NOACCESS.
+       * Make sure not to mark too many bytes in case chunk->requested_size
+       * < size < oldchksize.
+       */
+#ifdef USE_VALGRIND
+    if (Min(size, oldchksize) > chunk->requested_size)
+      MEM_UNDEFINED((char *)pointer + chunk->requested_size,
+                    Min(size, oldchksize) - chunk->requested_size);
+#endif
+#endif
+
+    chunk->requested_size = size;
+    /* set mark to catch clobber of "unused" space */
+    assert(size < chksize);
+    set_sentinel(pointer, size);
+#else /* !MEMORY_CONTEXT_CHECKING */
+
+    /*
+     * We may need to adjust marking of bytes from the old allocation as
+     * some of them may be marked NOACCESS.  We don't know how much of the
+     * old chunk size was the requested size; it could have been as small
+     * as one byte.  We have to be conservative and just mark the entire
+     * old portion DEFINED.  Make sure not to mark memory beyond the new
+     * allocation in case it's smaller than the old one.
+     */
+    MEM_DEFINED_IF_ADDRESSABLE(pointer, Min(size, oldchksize));
+#endif
+
+    /* Ensure any padding bytes are marked NOACCESS. */
+    MEM_NOACCESS((char *)pointer + size, chksize - size);
+
+    /* Disallow access to the chunk header . */
+    MEM_NOACCESS(chunk, ALLOC_CHUNKHDRSZ);
+
+    return pointer;
+  }
+
+  fidx = MemoryChunkGetValue(chunk);
+  assert(FreeListIdxIsValid(fidx));
+  oldchksize = GetChunkSizeFromFreeListIdx(fidx);
+
+#ifdef MEMORY_CONTEXT_CHECKING
+  /* Test for someone scribbling on unused space in chunk */
+  if (chunk->requested_size < oldchksize)
+    if (!sentinel_ok(pointer, chunk->requested_size))
+      elog(eloglevel::WARNING, "detected write past chunk end in %s %p",
+           set->name, chunk);
+#endif
+
+  /*
+   * Chunk sizes are aligned to power of 2 in AllocSetAlloc().  Maybe the
+   * allocated area already is >= the new size.  (In particular, we will
+   * fall out here if the requested size is a decrease.)
+   */
+  if (oldchksize >= size) {
+#ifdef MEMORY_CONTEXT_CHECKING
+    Size oldrequest = chunk->requested_size;
+
+#ifdef RANDOMIZE_ALLOCATED_MEMORY
+    /* We can only fill the extra space if we know the prior request */
+    if (size > oldrequest)
+      randomize_mem((char *)pointer + oldrequest, size - oldrequest);
+#endif
+
+    chunk->requested_size = size;
+
+    /*
+     * If this is an increase, mark any newly-available part UNDEFINED.
+     * Otherwise, mark the obsolete part NOACCESS.
+     */
+    if (size > oldrequest)
+      MEM_UNDEFINED((char *)pointer + oldrequest, size - oldrequest);
+    else
+      MEM_NOACCESS((char *)pointer + size, oldchksize - size);
+
+    /* set mark to catch clobber of "unused" space */
+    if (size < oldchksize) set_sentinel(pointer, size);
+#else /* !MEMORY_CONTEXT_CHECKING */
+
+    /*
+     * We don't have the information to determine whether we're growing
+     * the old request or shrinking it, so we conservatively mark the
+     * entire new allocation DEFINED.
+     */
+    MEM_NOACCESS(pointer, oldchksize);
+    MEM_DEFINED_IF_ADDRESSABLE(pointer, size);
+#endif
+
+    /* Disallow access to the chunk header. */
+    MEM_NOACCESS(chunk, ALLOC_CHUNKHDRSZ);
+
+    return pointer;
+  } else {
+    /*
+     * Enlarge-a-small-chunk case.  We just do this by brute force, ie,
+     * allocate a new chunk and copy the data.  Since we know the existing
+     * data isn't huge, this won't involve any great memcpy expense, so
+     * it's not worth being smarter.  (At one time we tried to avoid
+     * memcpy when it was possible to enlarge the chunk in-place, but that
+     * turns out to misbehave unpleasantly for repeated cycles of
+     * palloc/repalloc/pfree: the eventually freed chunks go into the
+     * wrong freelist for the next initial palloc request, and so we leak
+     * memory indefinitely.  See pgsql-hackers archives for 2007-08-11.)
+     */
+    void *newPointer;
+    Size oldsize;
+
+    /* allocate new chunk (this also checks size is valid) */
+    newPointer = AllocSetAlloc(size, INT64_MAX);
+
+    /* leave immediately if request was not completed */
+    if (newPointer == nullptr) {
+      /* Disallow access to the chunk header. */
+      MEM_NOACCESS(chunk, ALLOC_CHUNKHDRSZ);
+      return nullptr;
+    }
+
+    RelinkGroup(cprev, cnext, PointerGetMemoryChunk(newPointer));
+
+    /*
+     * AllocSetAlloc() may have returned a region that is still NOACCESS.
+     * Change it to UNDEFINED for the moment; memcpy() will then transfer
+     * definedness from the old allocation to the new.  If we know the old
+     * allocation, copy just that much.  Otherwise, make the entire old
+     * chunk defined to avoid errors as we copy the currently-NOACCESS
+     * trailing bytes.
+     */
+    MEM_UNDEFINED(newPointer, size);
+#ifdef MEMORY_CONTEXT_CHECKING
+    oldsize = chunk->requested_size;
+#else
+    oldsize = oldchksize;
+    MEM_DEFINED_IF_ADDRESSABLE(pointer, oldsize);
+#endif
+
+    /* transfer existing data (certain to fit) */
+    memcpy(newPointer, pointer, oldsize);
+
+    /* free old chunk */
+    AllocSetFree(pointer);
+
+    return newPointer;
+  }
+}
+
+/*
+ * AllocSetReset
+ *		Frees all memory which is allocated in the given set.
+ *
+ * Actually, this routine has some discretion about what to do.
+ * It should mark all allocated chunks freed, but it need not necessarily
+ * give back all the resources the set owns.  Our actual implementation is
+ * that we give back all but the "keeper" block (which we must keep, since
+ * it shares a malloc chunk with the context header).  In this way, we don't
+ * thrash malloc() when a context is repeatedly reset after small allocations,
+ * which is typical behavior for per-tuple contexts.
+ */
+void MemoryPool::AllocSetReset() {
+  mysql_mutex_assert_owner(&m_lock);
+
+  AllocSetContext *set = TopMemoryContext;
+  AllocBlockData *block;
+  Size keepersize [[maybe_unused]];
+
+#ifdef MEMORY_CONTEXT_CHECKING
+  /* Check for corruption and leaks before freeing */
+  AllocSetCheck();
+#endif
+
+  /* Remember keeper block size for assert below */
+  keepersize = KeeperBlock(set)->endptr - ((char *)set);
+
+  /* Clear chunk freelists */
+  MemSetAligned(set->freelist, 0, sizeof(set->freelist));
+
+  block = set->blocks;
+
+  /* New blocks list will be just the keeper block */
+  set->blocks = KeeperBlock(set);
+
+  while (block != nullptr) {
+    AllocBlockData *next = block->next;
+
+    if (IsKeeperBlock(set, block)) {
+      /* Reset the block, but don't return it to malloc */
+      char *datastart = ((char *)block) + ALLOC_BLOCKHDRSZ;
+
+#ifdef CLOBBER_FREED_MEMORY
+      wipe_mem(datastart, block->freeptr - datastart);
+#else
+      /* wipe_mem() would have done this */
+      MEM_NOACCESS(datastart, block->freeptr - datastart);
+#endif
+      block->freeptr = datastart;
+      block->prev = nullptr;
+      block->next = nullptr;
+    } else {
+      /* Normal case, release the block */
+      set->mem_allocated -= block->endptr - ((char *)block);
+
+#ifdef CLOBBER_FREED_MEMORY
+      wipe_mem(block, block->freeptr - ((char *)block));
+#endif
+      free(block);
+    }
+    block = next;
+  }
+
+  assert(set->mem_allocated == keepersize);
+
+  /* Reset block size allocation sequence, too */
+  set->nextBlockSize = set->initBlockSize;
+}
+
+/*
+ * AllocSetDelete
+ *		Frees all memory which is allocated in the given set,
+ *		in preparation for deletion of the set.
+ *
+ * Unlike AllocSetReset, this *must* free all resources of the set.
+ */
+void MemoryPool::AllocSetDelete() {
+  mysql_mutex_assert_owner(&m_lock);
+
+  AllocSetContext *set = TopMemoryContext;
+  AllocBlockData *block = set->blocks;
+  Size keepersize [[maybe_unused]];
+
+#ifdef MEMORY_CONTEXT_CHECKING
+  /* Check for corruption and leaks before freeing */
+  AllocSetCheck();
+#endif
+
+  /* Remember keeper block size for assert below */
+  keepersize = KeeperBlock(set)->endptr - ((char *)set);
+
+  /* Free all blocks, except the keeper which is part of context header */
+  while (block != nullptr) {
+    AllocBlockData *next = block->next;
+
+    if (!IsKeeperBlock(set, block))
+      set->mem_allocated -= block->endptr - ((char *)block);
+
+#ifdef CLOBBER_FREED_MEMORY
+    wipe_mem(block, block->freeptr - ((char *)block));
+#endif
+
+    if (!IsKeeperBlock(set, block)) free(block);
+
+    block = next;
+  }
+
+  assert(set->mem_allocated == keepersize);
+
+  /* Finally, free the context header, including the keeper block */
+  free(set);
+}
+
+/*
+ * AllocSetGetChunkSpace
+ *		Given a currently-allocated chunk, determine the total space
+ *		it occupies (including all memory-allocation overhead).
+ */
+Size MemoryPool::AllocSetGetChunkSpace(void *pointer) {
+  mysql_mutex_assert_owner(&m_lock);
+
+  MemoryChunk *chunk = PointerGetMemoryChunk(pointer);
+  int fidx;
+
+  /* Allow access to the chunk header. */
+  MEM_DEFINED_IF_ADDRESSABLE(chunk, ALLOC_CHUNKHDRSZ);
+
+  if (MemoryChunkIsExternal(chunk)) {
+    AllocBlockData *block = ExternalChunkGetBlock(chunk);
+
+    /* Disallow access to the chunk header. */
+    MEM_NOACCESS(chunk, ALLOC_CHUNKHDRSZ);
+
+    assert(AllocBlockIsValid(block));
+
+    return block->endptr - (char *)chunk;
+  }
+
+  fidx = MemoryChunkGetValue(chunk);
+  assert(FreeListIdxIsValid(fidx));
+
+  /* Disallow access to the chunk header. */
+  MEM_NOACCESS(chunk, ALLOC_CHUNKHDRSZ);
+
+  return GetChunkSizeFromFreeListIdx(fidx) + ALLOC_CHUNKHDRSZ;
+}
+
+/*
+ * AllocSetGetGroupSpace
+ *		Given a group, determine the total space
+ *		it occupies (including all memory-allocation overhead).
+ */
+Size MemoryPool::AllocSetGetGroupSpace(int64 group_id) {
+  mysql_mutex_assert_owner(&m_lock);
+
+  if (group_id == INT64_MAX) return INT64_MAX;
+
+  const auto it = m_group_map.find(group_id);
+  if (it == m_group_map.end()) return 0;
+
+  MemoryChunk *chunk = it->second;
+
+  assert(chunk != nullptr && chunk->prev == nullptr);
+
+  Size space = 0;
+
+  while (chunk != nullptr) {
+    void *ptr = MemoryChunkGetPointer(chunk);
+    space += AllocSetGetChunkSpace(ptr);
+    chunk = chunk->next;
+  }
+
+  return space;
+}
+
+/*
+ * AllocSetIsEmpty
+ *		Is an allocset empty of any allocated space?
+ */
+bool MemoryPool::AllocSetIsEmpty() {
+  mysql_mutex_assert_owner(&m_lock);
+
+  AllocSetContext *set = TopMemoryContext;
+  assert(AllocSetIsValid(set));
+
+  /*
+   * For now, we say "empty" only if the context is new or just reset. We
+   * could examine the freelists to determine if all space has been freed,
+   * but it's not really worth the trouble for present uses of this
+   * functionality.
+   */
+  return set->isReset;
+}
+
+/*
+ * AllocSetStats
+ *		Compute stats about memory consumption of an allocset.
+ *
+ * printfunc: if not nullptr, pass a human-readable stats string to this.
+ * passthru: pass this pointer through to printfunc.
+ * totals: if not nullptr, add stats about this context into *totals.
+ * print_to_stderr: print stats to stderr if true, elog otherwise.
+ */
+void MemoryPool::AllocSetStats(MemoryStatsPrintFunc printfunc, void *passthru,
+                               MemoryContextCounters *totals,
+                               bool print_to_stderr) {
+  mysql_mutex_assert_owner(&m_lock);
+
+  AllocSetContext *set = TopMemoryContext;
+  Size nblocks = 0;
+  Size freechunks = 0;
+  Size totalspace;
+  Size freespace = 0;
+  AllocBlockData *block;
+  int fidx;
+
+  assert(AllocSetIsValid(set));
+
+  /* Include context header in totalspace */
+  totalspace = MAXALIGN(sizeof(AllocSetContext));
+
+  for (block = set->blocks; block != nullptr; block = block->next) {
+    nblocks++;
+    totalspace += block->endptr - ((char *)block);
+    freespace += block->endptr - block->freeptr;
+  }
+  for (fidx = 0; fidx < ALLOCSET_NUM_FREELISTS; fidx++) {
+    Size chksz = GetChunkSizeFromFreeListIdx(fidx);
+    MemoryChunk *chunk = set->freelist[fidx];
+
+    while (chunk != nullptr) {
+      AllocFreeListLink *link = GetFreeListLink(chunk);
+
+      /* Allow access to the chunk header. */
+      MEM_DEFINED_IF_ADDRESSABLE(chunk, ALLOC_CHUNKHDRSZ);
+      assert((int)MemoryChunkGetValue(chunk) == fidx);
+      MEM_NOACCESS(chunk, ALLOC_CHUNKHDRSZ);
+
+      freechunks++;
+      freespace += chksz + ALLOC_CHUNKHDRSZ;
+
+      MEM_DEFINED_IF_ADDRESSABLE(link, sizeof(AllocFreeListLink));
+      chunk = link->next;
+      MEM_NOACCESS(link, sizeof(AllocFreeListLink));
+    }
+  }
+
+  if (printfunc) {
+    char stats_string[200];
+
+    snprintf(stats_string, sizeof(stats_string),
+             "%zu total in %zu blocks; %zu free (%zu chunks); %zu used",
+             totalspace, nblocks, freespace, freechunks,
+             totalspace - freespace);
+    printfunc(passthru, stats_string, print_to_stderr);
+  }
+
+  if (totals) {
+    totals->nblocks += nblocks;
+    totals->freechunks += freechunks;
+    totals->totalspace += totalspace;
+    totals->freespace += freespace;
+  }
+}
+
+void MemoryPool::MemoryChunkAddGroup(MemoryChunk *chunk, int64 group_id) {
+  if (group_id == INT64_MAX) return;
+
+  const auto it = m_group_map.find(group_id);
+  if (it != m_group_map.end()) {
+    chunk->next = it->second;
+    chunk->prev = nullptr;
+    it->second->prev = chunk;
+    it->second = chunk;
+  } else {
+    chunk->next = nullptr;
+    chunk->prev = nullptr;
+    m_group_map[group_id] = chunk;
+  }
+}
+
+#ifdef MEMORY_CONTEXT_CHECKING
+/*
+ * AllocSetCheck
+ *		Walk through chunks and check consistency of memory.
+ *
+ * NOTE: report errors as WARNING, *not* ERROR or FATAL.  Otherwise you'll
+ * find yourself in an infinite loop when trouble occurs, because this
+ * routine will be entered again when elog cleanup tries to release memory!
+ */
+void MemoryPool::AllocSetCheck() {
+  mysql_mutex_assert_owner(&m_lock);
+
+  AllocSetContext *set = TopMemoryContext;
+  const char *name = set->name;
+  AllocBlockData *prevblock;
+  AllocBlockData *block;
+  Size total_allocated = 0;
+
+  for (prevblock = nullptr, block = set->blocks; block != nullptr;
+       prevblock = block, block = block->next) {
+    char *bpoz = ((char *)block) + ALLOC_BLOCKHDRSZ;
+    long blk_used = block->freeptr - bpoz;
+    long blk_data = 0;
+    long nchunks = 0;
+    bool has_external_chunk = false;
+
+    if (IsKeeperBlock(set, block))
+      total_allocated += block->endptr - ((char *)set);
+    else
+      total_allocated += block->endptr - ((char *)block);
+
+    /*
+     * Empty block - empty can be keeper-block only
+     */
+    if (!blk_used) {
+      if (!IsKeeperBlock(set, block))
+        elog(eloglevel::WARNING, "problem in alloc set %s: empty block %p",
+             name, block);
+    }
+
+    /*
+     * Check block header fields
+     */
+    if (block->prev != prevblock || block->freeptr < bpoz ||
+        block->freeptr > block->endptr)
+      elog(eloglevel::WARNING,
+           "problem in alloc set %s: corrupt header in block %p", name, block);
+
+    /*
+     * Chunk walker
+     */
+    while (bpoz < block->freeptr) {
+      MemoryChunk *chunk = (MemoryChunk *)bpoz;
+      Size chsize, dsize;
+
+      /* Allow access to the chunk header. */
+      MEM_DEFINED_IF_ADDRESSABLE(chunk, ALLOC_CHUNKHDRSZ);
+
+      if (MemoryChunkIsExternal(chunk)) {
+        chsize = block->endptr -
+                 (char *)MemoryChunkGetPointer(chunk); /* aligned chunk size */
+        has_external_chunk = true;
+
+        /* make sure this chunk consumes the entire block */
+        if (chsize + ALLOC_CHUNKHDRSZ != (Size)blk_used)
+          elog(eloglevel::WARNING,
+               "problem in alloc set %s: bad single-chunk %p in block %p", name,
+               chunk, block);
+      } else {
+        int fidx = MemoryChunkGetValue(chunk);
+
+        if (!FreeListIdxIsValid(fidx))
+          elog(eloglevel::WARNING,
+               "problem in alloc set %s: bad chunk size for chunk %p in block "
+               "%p",
+               name, chunk, block);
+
+        chsize = GetChunkSizeFromFreeListIdx(fidx); /* aligned chunk size */
+
+        /*
+         * Check the stored block offset correctly references this
+         * block.
+         */
+        if (block != MemoryChunkGetBlock(chunk))
+          elog(eloglevel::WARNING,
+               "problem in alloc set %s: bad block offset for chunk %p in "
+               "block %p",
+               name, chunk, block);
+      }
+      dsize = chunk->requested_size; /* real data */
+
+      /* an allocated chunk's requested size must be <= the chsize */
+      if (dsize != InvalidAllocSize && dsize > chsize)
+        elog(eloglevel::WARNING,
+             "problem in alloc set %s: req size > alloc size for chunk %p in "
+             "block %p",
+             name, chunk, block);
+
+      /* chsize must not be smaller than the first freelist's size */
+      if (chsize < (1 << ALLOC_MINBITS))
+        elog(eloglevel::WARNING,
+             "problem in alloc set %s: bad size %zu for chunk %p in block %p",
+             name, chsize, chunk, block);
+
+      /*
+       * Check for overwrite of padding space in an allocated chunk.
+       */
+      if (dsize != InvalidAllocSize && dsize < chsize &&
+          !sentinel_ok(chunk, ALLOC_CHUNKHDRSZ + dsize))
+        elog(eloglevel::WARNING,
+             "problem in alloc set %s: detected write past chunk end in block "
+             "%p, chunk %p",
+             name, block, chunk);
+
+      /* if chunk is allocated, disallow access to the chunk header */
+      if (dsize != InvalidAllocSize) MEM_NOACCESS(chunk, ALLOC_CHUNKHDRSZ);
+
+      blk_data += chsize;
+      nchunks++;
+
+      bpoz += ALLOC_CHUNKHDRSZ + chsize;
+    }
+
+    if ((blk_data + (nchunks * ALLOC_CHUNKHDRSZ)) != (Size)blk_used)
+      elog(eloglevel::WARNING,
+           "problem in alloc set %s: found inconsistent memory block %p", name,
+           block);
+
+    if (has_external_chunk && nchunks > 1)
+      elog(eloglevel::WARNING,
+           "problem in alloc set %s: external chunk on non-dedicated block %p",
+           name, block);
+  }
+
+  assert(total_allocated == set->mem_allocated);
+}
+#endif
diff --git a/sql/memory/mp.h b/sql/memory/mp.h
new file mode 100644
index 0000000..08a17a3
--- /dev/null
+++ b/sql/memory/mp.h
@@ -0,0 +1,228 @@
+#include <unordered_map>
+
+#include "palloc.h"
+
+#ifndef MP_H
+#define MP_H
+
+class MemoryPool {
+ public:
+  MemoryPool() = default;
+
+  ~MemoryPool() = default;
+
+  void init();
+  void close();
+
+  /*
+   * Function to handle memory allocation requests of 'size' to allocate
+   * memory into the given 'context'.  The function must handle flags
+   * MCXT_ALLOC_HUGE and MCXT_ALLOC_NO_OOM.  MCXT_ALLOC_ZERO is handled by
+   * the calling function.
+   */
+  void *palloc(Size size, int64 group_id = INT64_MAX) {
+    MemoryPool_arena_mutex arena_mutex(m_lock);
+
+    return AllocSetAlloc(size, group_id);
+  }
+
+  /* call this free_p in case someone #define's free() */
+  void pfree(void *pointer, int64 group_id = INT64_MAX) {
+    MemoryPool_arena_mutex arena_mutex(m_lock);
+
+    AllocSetFree(pointer, group_id);
+  }
+
+  /*
+   * Function to handle a size change request for an existing allocation.
+   * The implementation must handle flags MCXT_ALLOC_HUGE and
+   * MCXT_ALLOC_NO_OOM.  MCXT_ALLOC_ZERO is handled by the calling function.
+   */
+  void *prealloc(void *pointer, Size size) {
+    MemoryPool_arena_mutex arena_mutex(m_lock);
+
+    return AllocSetRealloc(pointer, size);
+  }
+
+  /*
+   * Invalidate all previous allocations in the given memory context and
+   * prepare the context for a new set of allocations.  Implementations may
+   * optionally free() excess memory back to the OS during this time.
+   */
+  void preset() {
+    MemoryPool_arena_mutex arena_mutex(m_lock);
+
+    AllocSetReset();
+  }
+
+  /* Free all memory consumed by the given AllocSetContext *. */
+  void pdelete() {
+    MemoryPool_arena_mutex arena_mutex(m_lock);
+
+    AllocSetDelete();
+  }
+
+  /*
+   * Return the number of bytes consumed by the given pointer within its
+   * memory context, including the overhead of alignment and chunk headers.
+   */
+  Size get_chunk_space(void *pointer) {
+    MemoryPool_arena_mutex arena_mutex(m_lock);
+
+    return AllocSetGetChunkSpace(pointer);
+  }
+
+  /*
+   * Return the number of bytes consumed by the given group within its
+   * memory context, including the overhead of alignment and chunk headers.
+   */
+  Size get_group_space(int64 group_id) {
+    MemoryPool_arena_mutex arena_mutex(m_lock);
+
+    return AllocSetGetGroupSpace(group_id);
+  }
+
+  /*
+   * Return true if the given MemoryContext has not had any allocations
+   * since it was created or last reset.
+   */
+  bool is_empty() {
+    MemoryPool_arena_mutex arena_mutex(m_lock);
+
+    return AllocSetIsEmpty();
+  }
+
+  void stats(MemoryStatsPrintFunc printfunc, void *passthru,
+             MemoryContextCounters *totals, bool print_to_stderr) {
+    MemoryPool_arena_mutex arena_mutex(m_lock);
+
+    AllocSetStats(printfunc, passthru, totals, print_to_stderr);
+  }
+
+#ifdef MEMORY_CONTEXT_CHECKING
+
+  /*
+   * Perform validation checks on the given context and raise any discovered
+   * anomalies as WARNINGs.
+   */
+  void check() {
+    MemoryPool_arena_mutex arena_mutex(m_lock);
+
+    AllocSetCheck();
+  }
+#endif
+
+ private:
+  void *AllocSetAlloc(Size size, int64 group_id);
+
+  void AllocSetFree(void *pointer);
+  void AllocSetFree(void *pointer, int64 group_id);
+
+  void *AllocSetRealloc(void *pointer, Size size);
+  void AllocSetReset();
+  void AllocSetDelete();
+
+  Size AllocSetGetChunkSpace(void *pointer);
+  Size AllocSetGetGroupSpace(int64 group_id);
+
+  bool AllocSetIsEmpty();
+
+  void AllocSetStats(MemoryStatsPrintFunc printfunc, void *passthru,
+                     MemoryContextCounters *totals, bool print_to_stderr);
+
+#ifdef MEMORY_CONTEXT_CHECKING
+  void AllocSetCheck();
+#endif
+
+  AllocSetContext *AllocSetContextCreate(const char *name, Size minContextSize,
+                                         Size initBlockSize, Size maxBlockSize);
+
+  void *AllocSetAllocLarge(Size size, int64 group_id);
+
+  void *AllocSetAllocFromNewBlock(Size size, int fidx, int64 group_id);
+
+  void *AllocSetAllocChunkFromBlock(AllocBlockData *block, Size size,
+                                    Size chunk_size, int fidx, int64 group_id);
+
+  void MemoryChunkAddGroup(MemoryChunk *chunk, int64 group_id);
+
+  /*
+   * Standard top-level contexts. For a description of the purpose of each
+   * of these contexts, refer to src/backend/utils/mmgr/README
+   */
+  AllocSetContext *TopMemoryContext{nullptr};
+
+  mysql_mutex_t m_lock;
+
+  /*
+   * Memory group, sometimes need to release all memory on this group.
+   */
+  std::unordered_map<uint32_t, MemoryChunk *> m_group_map;
+};
+
+/**
+ * Allocate an object of the given type. Use like this:
+ *
+ *   Foo *foo = new (mem_pool) Foo();
+ *
+ * Note that unlike regular operator new, this will not throw exceptions.
+ * However, it can return nullptr if the capacity of the MemoryPool has
+ * been reached. This is allowed since it is not a replacement for global
+ * operator new, and thus isn't used automatically by e.g. standard library
+ * containers.
+ *
+ * TODO: This syntax is confusing in that it could look like allocating
+ * a MemoryPool using regular placement new. We should make a less
+ * ambiguous syntax, e.g. new (On(mem_pool)) Foo().
+ */
+
+inline void *operator new(size_t size, MemoryPool *mem_pool,
+                          const std::nothrow_t &arg
+                          [[maybe_unused]] = std::nothrow) noexcept {
+  return mem_pool->palloc(size);
+}
+
+inline void *operator new(size_t size, MemoryPool *mem_pool, int64 group_id,
+                          const std::nothrow_t &arg
+                          [[maybe_unused]] = std::nothrow) noexcept {
+  return mem_pool->palloc(size, group_id);
+}
+
+inline void *operator new[](size_t size, MemoryPool *mem_pool,
+                            const std::nothrow_t &arg
+                            [[maybe_unused]] = std::nothrow) noexcept {
+  return mem_pool->palloc(size);
+}
+
+inline void *operator new[](size_t size, MemoryPool *mem_pool, int64 group_id,
+                            const std::nothrow_t &arg
+                            [[maybe_unused]] = std::nothrow) noexcept {
+  return mem_pool->palloc(size, group_id);
+}
+
+inline void operator delete(void *pointer, MemoryPool *mem_pool,
+                            const std::nothrow_t &arg
+                            [[maybe_unused]] = std::nothrow) noexcept {
+  mem_pool->pfree(pointer);
+}
+
+inline void operator delete(void *pointer, MemoryPool *mem_pool, int64 group_id,
+                            const std::nothrow_t &arg
+                            [[maybe_unused]] = std::nothrow) noexcept {
+  mem_pool->pfree(pointer, group_id);
+}
+
+inline void operator delete[](void *pointer, MemoryPool *mem_pool,
+                              const std::nothrow_t &arg
+                              [[maybe_unused]] = std::nothrow) noexcept {
+  mem_pool->pfree(pointer);
+}
+
+inline void operator delete[](void *pointer, MemoryPool *mem_pool,
+                              int64 group_id,
+                              const std::nothrow_t &arg
+                              [[maybe_unused]] = std::nothrow) noexcept {
+  mem_pool->pfree(pointer, group_id);
+}
+
+#endif
diff --git a/sql/memory/palloc.h b/sql/memory/palloc.h
new file mode 100644
index 0000000..787e29c
--- /dev/null
+++ b/sql/memory/palloc.h
@@ -0,0 +1,633 @@
+/*-------------------------------------------------------------------------
+ *
+ * palloc.h
+ *	  POSTGRES memory allocator definitions.
+ *
+ * This file contains the basic memory allocation interface that is
+ * needed by almost every backend module.  It is included directly by
+ * postgres.h, so the definitions here are automatically available
+ * everywhere.  Keep it lean!
+ *
+ * Memory allocation occurs within "contexts".  Every chunk obtained from
+ * palloc()/MemoryContextAlloc() is allocated within a specific context.
+ * The entire contents of a context can be freed easily and quickly by
+ * resetting or deleting the context --- this is both faster and less
+ * prone to memory-leakage bugs than releasing chunks individually.
+ * We organize contexts into context trees to allow fine-grain control
+ * over chunk lifetime while preserving the certainty that we will free
+ * everything that should be freed.  See utils/mmgr/README for more info.
+ *
+ *
+ * Portions Copyright (c) 1996-2025, PostgreSQL Global Development Group
+ * Portions Copyright (c) 1994, Regents of the University of California
+ *
+ * src/include/utils/palloc.h
+ *
+ *-------------------------------------------------------------------------
+ */
+#ifndef PALLOC_H
+#define PALLOC_H
+
+#include <assert.h>
+
+#include <new>
+
+#include "include/my_alloc.h"
+#include "include/my_inttypes.h"
+#include "my_loglevel.h"
+#include "mysql/psi/mysql_mutex.h"
+
+// #define MEMORY_CONTEXT_CHECKING
+
+enum class eloglevel {
+  LOG = loglevel::SYSTEM_LEVEL,
+  ERROR = loglevel::ERROR_LEVEL,
+  WARNING = loglevel::WARNING_LEVEL,
+  INFO = loglevel::INFORMATION_LEVEL
+};
+
+/*
+ * Size
+ *		Size of any memory resident object, as returned by sizeof.
+ */
+typedef size_t Size;
+
+/*--------------------
+ * Chunk freelist k holds chunks of size 1 << (k + ALLOC_MINBITS),
+ * for k = 0 .. ALLOCSET_NUM_FREELISTS-1.
+ *
+ * Note that all chunks in the freelists have power-of-2 sizes.  This
+ * improves recyclability: we may waste some space, but the wasted space
+ * should stay pretty constant as requests are made and released.
+ *
+ * A request too large for the last freelist is handled by allocating a
+ * dedicated block from malloc().  The block still has a block header and
+ * chunk header, but when the chunk is freed we'll return the whole block
+ * to malloc(), not put it on our freelists.
+ *
+ * CAUTION: ALLOC_MINBITS must be large enough so that
+ * 1<<ALLOC_MINBITS is at least MAXALIGN,
+ * or we may fail to align the smallest chunks adequately.
+ * 8-byte alignment is enough on all currently known machines.  This 8-byte
+ * minimum also allows us to store a pointer to the next freelist item within
+ * the chunk of memory itself.
+ *
+ * With the current parameters, request sizes up to 8K are treated as chunks,
+ * larger requests go into dedicated blocks.  Change ALLOCSET_NUM_FREELISTS
+ * to adjust the boundary point; and adjust ALLOCSET_SEPARATE_THRESHOLD in
+ * memutils.h to agree.  (Note: in contexts with small maxBlockSize, we may
+ * set the allocChunkLimit to less than 8K, so as to avoid space wastage.)
+ *--------------------
+ */
+
+#define ALLOC_MINBITS 3 /* smallest chunk size is 8 bytes */
+#define ALLOCSET_NUM_FREELISTS 11
+#define ALLOC_CHUNK_LIMIT (1 << (ALLOCSET_NUM_FREELISTS - 1 + ALLOC_MINBITS))
+/* Size of largest chunk that we use a fixed size for */
+#define ALLOC_CHUNK_FRACTION 4
+/* We allow chunks to be at most 1/4 of maxBlockSize (less overhead) */
+
+/* Define as the maximum alignment requirement of any C data type. */
+#define MAXIMUM_ALIGNOF 8
+
+/* ----------------
+ * Alignment macros: align a length or address appropriately for a given type.
+ * The fooALIGN() macros round up to a multiple of the required alignment,
+ * while the fooALIGN_DOWN() macros round down.  The latter are more useful
+ * for problems like "how many X-sized structures will fit in a page?".
+ *
+ * NOTE: TYPEALIGN[_DOWN] will not work if ALIGNVAL is not a power of 2.
+ * That case seems extremely unlikely to be needed in practice, however.
+ *
+ * NOTE: MAXIMUM_ALIGNOF, and hence MAXALIGN(), intentionally exclude any
+ * larger-than-8-byte types the compiler might have.
+ * ----------------
+ */
+
+#define TYPEALIGN(ALIGNVAL, LEN) \
+  (((uintptr_t)(LEN) + ((ALIGNVAL) - 1)) & ~((uintptr_t)((ALIGNVAL) - 1)))
+
+#define MAXALIGN(LEN) TYPEALIGN(MAXIMUM_ALIGNOF, (LEN))
+
+/*
+ * Recommended default alloc parameters, suitable for "ordinary" contexts
+ * that might hold quite a lot of data.
+ */
+#define ALLOCSET_DEFAULT_MINSIZE 0
+#define ALLOCSET_DEFAULT_INITSIZE (8 * 1024)
+#define ALLOCSET_DEFAULT_MAXSIZE (8 * 1024 * 1024)
+#define ALLOCSET_DEFAULT_SIZES \
+  ALLOCSET_DEFAULT_MINSIZE, ALLOCSET_DEFAULT_INITSIZE, ALLOCSET_DEFAULT_MAXSIZE
+
+/*--------------------
+ * Chunk freelist k holds chunks of size 1 << (k + ALLOC_MINBITS),
+ * for k = 0 .. ALLOCSET_NUM_FREELISTS-1.
+ *
+ * Note that all chunks in the freelists have power-of-2 sizes.  This
+ * improves recyclability: we may waste some space, but the wasted space
+ * should stay pretty constant as requests are made and released.
+ *
+ * A request too large for the last freelist is handled by allocating a
+ * dedicated block from malloc().  The block still has a block header and
+ * chunk header, but when the chunk is freed we'll return the whole block
+ * to malloc(), not put it on our freelists.
+ *
+ * CAUTION: ALLOC_MINBITS must be large enough so that
+ * 1<<ALLOC_MINBITS is at least MAXALIGN,
+ * or we may fail to align the smallest chunks adequately.
+ * 8-byte alignment is enough on all currently known machines.  This 8-byte
+ * minimum also allows us to store a pointer to the next freelist item within
+ * the chunk of memory itself.
+ *
+ * With the current parameters, request sizes up to 8K are treated as chunks,
+ * larger requests go into dedicated blocks.  Change ALLOCSET_NUM_FREELISTS
+ * to adjust the boundary point; and adjust ALLOCSET_SEPARATE_THRESHOLD in
+ * memutils.h to agree.  (Note: in contexts with small maxBlockSize, we may
+ * set the allocChunkLimit to less than 8K, so as to avoid space wastage.)
+ *--------------------
+ */
+
+#define ALLOC_MINBITS 3 /* smallest chunk size is 8 bytes */
+#define ALLOCSET_NUM_FREELISTS 11
+#define ALLOC_CHUNK_LIMIT (1 << (ALLOCSET_NUM_FREELISTS - 1 + ALLOC_MINBITS))
+/* Size of largest chunk that we use a fixed size for */
+#define ALLOC_CHUNK_FRACTION 4
+/* We allow chunks to be at most 1/4 of maxBlockSize (less overhead) */
+
+/*
+ * The number of bits that 8-byte memory chunk headers can use to encode the
+ * MemoryContextMethodID.
+ */
+#define MEMORY_CONTEXT_METHODID_BITS 4
+#define MEMORY_CONTEXT_METHODID_MASK \
+  ((((uint64)1) << MEMORY_CONTEXT_METHODID_BITS) - 1)
+
+/*
+ * MemoryContextMethodID
+ *		A unique identifier for each AllocSetContext * implementation
+ *which indicates the index into the mcxt_methods[] array. See mcxt.c.
+ *
+ * For robust error detection, ensure that MemoryContextMethodID has a value
+ * for each possible bit-pattern of MEMORY_CONTEXT_METHODID_MASK, and make
+ * dummy entries for unused IDs in the mcxt_methods[] array.  We also try
+ * to avoid using bit-patterns as valid IDs if they are likely to occur in
+ * garbage data, or if they could falsely match on chunks that are really from
+ * malloc not palloc.  (We can't tell that for most malloc implementations,
+ * but it happens that glibc stores flag bits in the same place where we put
+ * the MemoryContextMethodID, so the possible values are predictable for it.)
+ */
+typedef enum MemoryContextMethodID {
+  MCTX_0_RESERVED_UNUSEDMEM_ID, /* 0000 occurs in never-used memory */
+  MCTX_1_RESERVED_GLIBC_ID,     /* glibc malloc'd chunks usually match 0001 */
+  MCTX_2_RESERVED_GLIBC_ID,     /* glibc malloc'd chunks > 128kB match 0010 */
+  MCTX_ASET_ID,
+  MCTX_GENERATION_ID,
+  MCTX_SLAB_ID,
+  MCTX_ALIGNED_REDIRECT_ID,
+  MCTX_BUMP_ID,
+  MCTX_8_UNUSED_ID,
+  MCTX_9_UNUSED_ID,
+  MCTX_10_UNUSED_ID,
+  MCTX_11_UNUSED_ID,
+  MCTX_12_UNUSED_ID,
+  MCTX_13_UNUSED_ID,
+  MCTX_14_UNUSED_ID,
+  MCTX_15_RESERVED_WIPEDMEM_ID /* 1111 occurs in wipe_mem'd memory */
+} MemoryContextMethodID;
+
+/*
+ * Recommended default alloc parameters, suitable for "ordinary" contexts
+ * that might hold quite a lot of data.
+ */
+#define ALLOCSET_DEFAULT_MINSIZE 0
+#define ALLOCSET_DEFAULT_INITSIZE (8 * 1024)
+#define ALLOCSET_DEFAULT_MAXSIZE (8 * 1024 * 1024)
+#define ALLOCSET_DEFAULT_SIZES \
+  ALLOCSET_DEFAULT_MINSIZE, ALLOCSET_DEFAULT_INITSIZE, ALLOCSET_DEFAULT_MAXSIZE
+
+/*
+ * Recommended alloc parameters for "small" contexts that are never expected
+ * to contain much data (for example, a context to contain a query plan).
+ */
+#define ALLOCSET_SMALL_MINSIZE 0
+#define ALLOCSET_SMALL_INITSIZE (1 * 1024)
+#define ALLOCSET_SMALL_MAXSIZE (8 * 1024)
+#define ALLOCSET_SMALL_SIZES \
+  ALLOCSET_SMALL_MINSIZE, ALLOCSET_SMALL_INITSIZE, ALLOCSET_SMALL_MAXSIZE
+
+/*
+ * Recommended alloc parameters for contexts that should start out small,
+ * but might sometimes grow big.
+ */
+#define ALLOCSET_START_SMALL_SIZES \
+  ALLOCSET_SMALL_MINSIZE, ALLOCSET_SMALL_INITSIZE, ALLOCSET_DEFAULT_MAXSIZE
+
+/*
+ * Threshold above which a request in an AllocSet context is certain to be
+ * allocated separately (and thereby have constant allocation overhead).
+ * Few callers should be interested in this, but tuplesort/tuplestore need
+ * to know it.
+ */
+#define ALLOCSET_SEPARATE_THRESHOLD 8192
+
+#define SLAB_DEFAULT_BLOCK_SIZE (8 * 1024)
+#define SLAB_LARGE_BLOCK_SIZE (8 * 1024 * 1024)
+
+#if __WORDSIZE == 64
+#define HAVE_LONG_INT_64 1
+#else
+#define HAVE_LONG_LONG_INT_64 1
+#endif
+
+/*
+ * 64-bit integers
+ */
+#ifdef HAVE_LONG_INT_64
+/* Plain "long int" fits, use it */
+
+#define INT64CONST(x) (x##L)
+#define UINT64CONST(x) (x##UL)
+#elif defined(HAVE_LONG_LONG_INT_64)
+/* We have working support for "long long int", use that */
+
+#define INT64CONST(x) (x##LL)
+#define UINT64CONST(x) (x##ULL)
+#else
+/* neither HAVE_LONG_INT_64 nor HAVE_LONG_LONG_INT_64 */
+#error must have a working 64-bit integer datatype
+#endif
+
+/*
+ * MaxAllocSize, MaxAllocHugeSize
+ *		Quasi-arbitrary limits on size of allocations.
+ *
+ * Note:
+ *		There is no guarantee that smaller allocations will succeed, but
+ *		larger requests will be summarily denied.
+ *
+ * palloc() enforces MaxAllocSize, chosen to correspond to the limiting size
+ * of varlena objects under TOAST.  See VARSIZE_4B() and related macros in
+ * postgres.h.  Many datatypes assume that any allocatable size can be
+ * represented in a varlena header.  This limit also permits a caller to use
+ * an "int" variable for an index into or length of an allocation.  Callers
+ * careful to avoid these hazards can access the higher limit with
+ * MemoryContextAllocHuge().  Both limits permit code to assume that it may
+ * compute twice an allocation's size without overflow.
+ */
+#define MaxAllocSize ((Size)0x3fffffff) /* 1 gigabyte - 1 */
+
+#define AllocSizeIsValid(size) ((Size)(size) <= MaxAllocSize)
+
+/* Must be less than SIZE_MAX */
+#define MaxAllocHugeSize (SIZE_MAX / 2)
+
+#define InvalidAllocSize SIZE_MAX
+
+#define AllocHugeSizeIsValid(size) ((Size)(size) <= MaxAllocHugeSize)
+
+/*
+ * Array giving the position of the left-most set bit for each possible
+ * byte value.  We count the right-most position as the 0th bit, and the
+ * left-most the 7th bit.  The 0th entry of the array should not be used.
+ *
+ * Note: this is not used by the functions in pg_bitutils.h when
+ * HAVE__BUILTIN_CLZ is defined, but we provide it anyway, so that
+ * extensions possibly compiled with a different compiler can use it.
+ */
+const uint8 pg_leftmost_one_pos[256] = {
+    0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4,
+    4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,
+    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6,
+    6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
+    6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
+    6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
+    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
+    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
+    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
+    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
+    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7};
+
+/*
+ * Macros to support compile-time assertion checks.
+ *
+ * If the "condition" (a compile-time-constant expression) evaluates to false,
+ * throw a compile error using the "errmessage" (a string literal).
+ *
+ * C11 has _Static_assert(), and most C99 compilers already support that.  For
+ * portability, we wrap it into StaticAssertDecl().  _Static_assert() is a
+ * "declaration", and so it must be placed where for example a variable
+ * declaration would be valid.  As long as we compile with
+ * -Wno-declaration-after-statement, that also means it cannot be placed after
+ * statements in a function.  Macros StaticAssertStmt() and StaticAssertExpr()
+ * make it safe to use as a statement or in an expression, respectively.
+ *
+ * For compilers without _Static_assert(), we fall back on a kluge that
+ * assumes the compiler will complain about a negative width for a struct
+ * bit-field.  This will not include a helpful error message, but it beats not
+ * getting an error at all.
+ */
+#ifndef __cplusplus
+#ifdef HAVE__STATIC_ASSERT
+#define StaticAssertDecl(condition, errmessage) \
+  _Static_assert(condition, errmessage)
+#define StaticAssertStmt(condition, errmessage) \
+  do {                                          \
+    _Static_assert(condition, errmessage);      \
+  } while (0)
+#define StaticAssertExpr(condition, errmessage) \
+  ((void)({                                     \
+    StaticAssertStmt(condition, errmessage);    \
+    true;                                       \
+  }))
+#else /* !HAVE__STATIC_ASSERT */
+#define StaticAssertDecl(condition, errmessage) \
+  extern void static_assert_func(               \
+      int static_assert_failure[(condition) ? 1 : -1])
+#define StaticAssertStmt(condition, errmessage) \
+  ((void)sizeof(struct { int static_assert_failure : (condition) ? 1 : -1; }))
+#define StaticAssertExpr(condition, errmessage) \
+  StaticAssertStmt(condition, errmessage)
+#endif /* HAVE__STATIC_ASSERT */
+#else  /* C++ */
+#if defined(__cpp_static_assert) && __cpp_static_assert >= 200410
+#define StaticAssertDecl(condition, errmessage) \
+  static_assert(condition, errmessage)
+#define StaticAssertStmt(condition, errmessage) \
+  static_assert(condition, errmessage)
+#define StaticAssertExpr(condition, errmessage) \
+  ({ static_assert(condition, errmessage); })
+#else /* !__cpp_static_assert */
+#define StaticAssertDecl(condition, errmessage) \
+  extern void static_assert_func(               \
+      int static_assert_failure[(condition) ? 1 : -1])
+#define StaticAssertStmt(condition, errmessage)         \
+  do {                                                  \
+    struct static_assert_struct {                       \
+      int static_assert_failure : (condition) ? 1 : -1; \
+    };                                                  \
+  } while (0)
+#define StaticAssertExpr(condition, errmessage) \
+  ((void)({ StaticAssertStmt(condition, errmessage); }))
+#endif /* __cpp_static_assert */
+#endif /* C++ */
+
+/*
+ * pg_nodiscard means the compiler should warn if the result of a function
+ * call is ignored.  The name "nodiscard" is chosen in alignment with
+ * (possibly future) C and C++ standards.  For maximum compatibility, use it
+ * as a function declaration specifier, so it goes before the return type.
+ */
+#ifdef __GNUC__
+#define pg_nodiscard __attribute__((warn_unused_result))
+#else
+#define pg_nodiscard
+#endif
+
+/*
+ * Size
+ *		Size of any memory resident object, as returned by sizeof.
+ */
+typedef size_t Size;
+
+/*
+ * A memory context can have callback functions registered on it.  Any such
+ * function will be called once just before the context is next reset or
+ * deleted.  The MemoryContextCallback struct describing such a callback
+ * typically would be allocated within the context itself, thereby avoiding
+ * any need to manage it explicitly (the reset/delete action will free it).
+ */
+typedef void (*MemoryContextCallbackFunction)(void *arg);
+
+typedef struct MemoryContextCallback {
+  MemoryContextCallbackFunction func; /* function to call */
+  void *arg;                          /* argument to pass it */
+  struct MemoryContextCallback *next; /* next in list of callbacks */
+} MemoryContextCallback;
+
+/*
+ * Flags for MemoryContextAllocExtended.
+ */
+#define MCXT_ALLOC_HUGE 0x01   /* allow huge allocation (> 1 GB) */
+#define MCXT_ALLOC_NO_OOM 0x02 /* no failure if out-of-memory */
+#define MCXT_ALLOC_ZERO 0x04   /* zero allocated memory */
+
+/*
+ * Rather than repeatedly creating and deleting memory contexts, we keep some
+ * freed contexts in freelists so that we can hand them out again with little
+ * work.  Before putting a context in a freelist, we reset it so that it has
+ * only its initial malloc chunk and no others.  To be a candidate for a
+ * freelist, a context must have the same minContextSize/initBlockSize as
+ * other contexts in the list; but its maxBlockSize is irrelevant since that
+ * doesn't affect the size of the initial chunk.
+ *
+ * We currently provide one freelist for ALLOCSET_DEFAULT_SIZES contexts
+ * and one for ALLOCSET_SMALL_SIZES contexts; the latter works for
+ * ALLOCSET_START_SMALL_SIZES too, since only the maxBlockSize differs.
+ *
+ * Ordinarily, we re-use freelist contexts in last-in-first-out order, in
+ * hopes of improving locality of reference.  But if there get to be too
+ * many contexts in the list, we'd prefer to drop the most-recently-created
+ * contexts in hopes of keeping the process memory map compact.
+ * We approximate that by simply deleting all existing entries when the list
+ * overflows, on the assumption that queries that allocate a lot of contexts
+ * will probably free them in more or less reverse order of allocation.
+ *
+ * Contexts in a freelist are chained via their nextchild pointers.
+ */
+#define MAX_FREE_CONTEXTS 100 /* arbitrary limit on freelist length */
+
+/* Get a bit mask of the bits set in non-long aligned addresses */
+#define LONG_ALIGN_MASK (sizeof(long) - 1)
+#define MEMSET_LOOP_LIMIT 1024
+
+/*
+ * MemSetAligned is the same as MemSet except it omits the test to see if
+ * "start" is word-aligned.  This is okay to use if the caller knows a-priori
+ * that the pointer is suitably aligned (typically, because he just got it
+ * from palloc(), which always delivers a max-aligned pointer).
+ */
+#define MemSetAligned(start, val, len)                         \
+  do {                                                         \
+    long *_start = (long *)(start);                            \
+    int _val = (val);                                          \
+    Size _len = (len);                                         \
+                                                               \
+    if ((_len & LONG_ALIGN_MASK) == 0 && _val == 0 &&          \
+        _len <= MEMSET_LOOP_LIMIT && MEMSET_LOOP_LIMIT != 0) { \
+      long *_stop = (long *)((char *)_start + _len);           \
+      while (_start < _stop) *_start++ = 0;                    \
+    } else                                                     \
+      memset(_start, _val, _len);                              \
+  } while (0)
+
+/*
+ * The maximum allowed value that MemoryContexts can store in the value
+ * field.  Must be 1 less than a power of 2.
+ */
+#define MEMORYCHUNK_MAX_VALUE UINT64CONST(0x3FFFFFFF)
+
+/*
+ * The maximum distance in bytes that a MemoryChunk can be offset from the
+ * block that is storing the chunk.  Must be 1 less than a power of 2.
+ */
+#define MEMORYCHUNK_MAX_BLOCKOFFSET UINT64CONST(0x3FFFFFFF)
+
+/*
+ * As above, but mask out the lowest-order (always zero) bit as this is shared
+ * with the MemoryChunkGetValue field.
+ */
+#define MEMORYCHUNK_BLOCKOFFSET_MASK UINT64CONST(0x3FFFFFFE)
+
+/* define the least significant base-0 bit of each portion of the hdrmask */
+#define MEMORYCHUNK_EXTERNAL_BASEBIT MEMORY_CONTEXT_METHODID_BITS
+#define MEMORYCHUNK_VALUE_BASEBIT (MEMORYCHUNK_EXTERNAL_BASEBIT + 1)
+#define MEMORYCHUNK_BLOCKOFFSET_BASEBIT (MEMORYCHUNK_VALUE_BASEBIT + 29)
+
+/*
+ * A magic number for storing in the free bits of an external chunk.  This
+ * must mask out the bits used for storing the MemoryContextMethodID and the
+ * external bit.
+ */
+#define MEMORYCHUNK_MAGIC                                       \
+  (UINT64CONST(0xB1A8DB858EB6EFBA) >> MEMORYCHUNK_VALUE_BASEBIT \
+                                          << MEMORYCHUNK_VALUE_BASEBIT)
+
+/* private macros for making the inline functions below more simple */
+#define HdrMaskIsExternal(hdrmask) \
+  ((hdrmask) & (((uint64)1) << MEMORYCHUNK_EXTERNAL_BASEBIT))
+#define HdrMaskGetValue(hdrmask) \
+  (((hdrmask) >> MEMORYCHUNK_VALUE_BASEBIT) & MEMORYCHUNK_MAX_VALUE)
+
+/*
+ * Shift the block offset down to the 0th bit position and mask off the single
+ * bit that's shared with the MemoryChunkGetValue field.
+ */
+#define HdrMaskBlockOffset(hdrmask)                 \
+  (((hdrmask) >> MEMORYCHUNK_BLOCKOFFSET_BASEBIT) & \
+   MEMORYCHUNK_BLOCKOFFSET_MASK)
+
+/* For external chunks only, check the magic number matches */
+#define HdrMaskCheckMagic(hdrmask) \
+  (MEMORYCHUNK_MAGIC ==            \
+   ((hdrmask) >> MEMORYCHUNK_VALUE_BASEBIT << MEMORYCHUNK_VALUE_BASEBIT))
+
+/*
+ * AllocBlockData *
+ *		An AllocBlockData * is the unit of memory that is obtained by
+ *aset.c from malloc().  It contains one or more MemoryChunks, which are the
+ *units requested by palloc() and freed by pfree(). MemoryChunks cannot be
+ *returned to malloc() individually, instead they are put on freelists by
+ *pfree() and re-used by the next palloc() that has a matching request size.
+ *
+ *		AllocBlockData is the header data for a block --- the usable
+ *space within the block begins at the next alignment boundary.
+ */
+struct AllocBlockData {
+  AllocBlockData *prev; /* prev block in aset's blocks list, if any */
+  AllocBlockData *next; /* next block in aset's blocks list, if any */
+  char *freeptr;        /* start of free space in this block */
+  char *endptr;         /* end of space in this block */
+};
+
+struct MemoryChunk {
+#ifdef MEMORY_CONTEXT_CHECKING
+  Size requested_size;
+#endif
+
+  MemoryChunk *prev{nullptr};
+  MemoryChunk *next{nullptr};
+
+  /* bitfield for storing details about the chunk */
+  uint64 hdrmask; /* must be last */
+};
+
+/*
+ * MemoryContext
+ *		A logical context in which memory allocations occur.
+ *
+ * MemoryContext itself is an abstract type that can have multiple
+ * implementations.
+ * The function pointers in MemoryContextMethods define one specific
+ * implementation of MemoryContext --- they are a virtual function table
+ * in C++ terms.
+ *
+ * Node types that are actual implementations of memory contexts must
+ * begin with the same fields as MemoryContextData.
+ *
+ * Note: for largely historical reasons, typedef MemoryContext is a pointer
+ * to the context struct rather than the struct type itself.
+ */
+
+typedef void (*MemoryStatsPrintFunc)(void *passthru, const char *stats_string,
+                                     bool print_to_stderr);
+
+/*
+ * MemoryContextCounters
+ *		Summarization state for MemoryContextStats collection.
+ *
+ * The set of counters in this struct is biased towards AllocSet; if we ever
+ * add any context types that are based on fundamentally different approaches,
+ * we might need more or different counters here.  A possible API spec then
+ * would be to print only nonzero counters, but for now we just summarize in
+ * the format historically used by AllocSet.
+ */
+struct MemoryContextCounters {
+  Size nblocks;    /* Total number of malloc blocks */
+  Size freechunks; /* Total number of free chunks */
+  Size totalspace; /* Total bytes requested from malloc */
+  Size freespace;  /* The unused portion of totalspace */
+};
+
+/*
+ * AllocFreeListLink
+ *		When pfreeing memory, if we maintain a freelist for the given
+ *chunk's size then we use a AllocFreeListLink to point to the current item in
+ *		the AllocSetContext's freelist and then set the given freelist
+ *element to point to the chunk being freed.
+ */
+struct AllocFreeListLink {
+  MemoryChunk *next;
+};
+
+/*
+ * AllocSetContext is our standard implementation of AllocSetContext *.
+ *
+ * Note: header.isReset means there is nothing for AllocSetReset to do.
+ * This is different from the aset being physically empty (empty blocks list)
+ * because we will still have a keeper block.  It's also different from the set
+ * being logically empty, because we don't attempt to detect pfree'ing the
+ * last active chunk.
+ */
+struct AllocSetContext {
+  /* these two fields are placed here to minimize alignment wastage: */
+  bool isReset;            /* T = no space alloced since last reset */
+  bool allowInCritSection; /* allow palloc in critical section */
+  Size mem_allocated;      /* track memory allocated for this context */
+  const char *name;        /* context name (just for debugging) */
+  MemoryContextCallback *reset_cbs; /* list of reset/delete callbacks */
+
+  /* Info about storage allocated in this context: */
+  AllocBlockData *blocks; /* head of list of blocks in this set */
+  MemoryChunk *freelist[ALLOCSET_NUM_FREELISTS]; /* free chunk lists */
+  /* Allocation parameters for this context: */
+  uint32 initBlockSize;   /* initial block size */
+  uint32 maxBlockSize;    /* maximum block size */
+  uint32 nextBlockSize;   /* next block size to allocate */
+  uint32 allocChunkLimit; /* effective chunk size limit */
+  /* freelist this context could be put in, or -1 if not a candidate: */
+  int freeListIndex; /* index in context_freelists[], or -1 */
+};
+
+class MemoryPool_arena_mutex {
+ public:
+  MemoryPool_arena_mutex(mysql_mutex_t &lock) : m_lock(lock) {
+    mysql_mutex_lock(&m_lock);
+  }
+
+  ~MemoryPool_arena_mutex() { mysql_mutex_unlock(&m_lock); }
+
+ private:
+  mysql_mutex_t &m_lock;
+};
+
+#endif
diff --git a/sql/mysqld.cc b/sql/mysqld.cc
index 7ad3582..9e8b91b 100644
--- a/sql/mysqld.cc
+++ b/sql/mysqld.cc
@@ -888,7 +888,9 @@ MySQL clients support the protocol:
 #include "sql/sql_parallel.h"
 #include "time_recorder.h"
 #include "base/logging.h"
-
+#include <base/string_printf.h>
+#include "sql/sql_fast_query_cache.h"
+#include "storage/perfschema/pfs_visitor.h"
 #ifdef WITH_PERFSCHEMA_STORAGE_ENGINE
 #include "storage/perfschema/pfs_server.h"
 #endif /* WITH_PERFSCHEMA_STORAGE_ENGINE */
@@ -1182,6 +1184,8 @@ ulonglong_typ temptable_max_mmap;
 bool temptable_use_mmap;
 static char compiled_default_collation_name[] = MYSQL_DEFAULT_COLLATION_NAME;
 static bool binlog_format_used = false;
+bool  fast_query_cache_enable;
+ulong fast_query_cache_single_max_memory;
 
 LEX_STRING opt_init_connect, opt_init_replica;
 
@@ -1397,9 +1401,11 @@ ulong_typ replica_max_allowed_packet = 0;
 ulong_typ binlog_stmt_cache_size = 0;
 int32 opt_binlog_max_flush_queue_time = 0;
 long opt_binlog_group_commit_sync_delay = 0;
-ulong_typ opt_binlog_group_commit_sync_no_delay_count = 0;
+ulonglong_typ fast_query_cache_hits_count = 0;
+ulonglong_typ fast_query_cache_invalidate_count = 0;
+ulong opt_binlog_group_commit_sync_no_delay_count = 0;
 ulonglong_typ max_binlog_stmt_cache_size = 0;
-ulong_typ refresh_version; /* Increments on each reload */
+ulong refresh_version; /* Increments on each reload */
 std::atomic<query_id_t> atomic_global_query_id{1};
 ulong_typ aborted_threads;
 ulong_typ delayed_insert_timeout, delayed_insert_limit, delayed_queue_size;
@@ -2000,6 +2006,9 @@ bool dynamic_plugins_are_initialized = false;
 static const char *default_dbug_option;
 #endif
 
+/* Fast query cache global instance. */
+static Fast_query_cache s_fast_query_cache;
+
 bool opt_use_ssl = true;
 bool opt_use_admin_ssl = true;
 ulong_typ opt_ssl_fips_mode = SSL_FIPS_MODE_OFF;
@@ -2933,6 +2942,7 @@ static void clean_up(bool print_message) {
   servers_free(true);
   acl_free(true);
   grant_free();
+  s_fast_query_cache.cleanup();
   hostname_cache_free();
   range_optimizer_free();
   item_func_sleep_free();
@@ -6471,6 +6481,7 @@ static int init_server_components() {
   */
   mdl_init();
   partitioning_init();
+  s_fast_query_cache.initialize();
   if (table_def_init() || hostname_cache_init(host_cache_size))
     unireg_abort(MYSQLD_ABORT_EXIT);
 
@@ -10567,6 +10578,14 @@ SHOW_VAR status_vars[] = {
      SHOW_SCOPE_GLOBAL},
     {"Resource_group_supported", (char *)show_resource_group_support, SHOW_FUNC,
      SHOW_SCOPE_GLOBAL},
+    {"fast_query_cache_map_memory", (char *)&fast_query_cache_map_memory,
+     SHOW_LONG, SHOW_SCOPE_GLOBAL},
+    {"fast_query_cache_map_size", (char *)&fast_query_cache_map_size, SHOW_LONG,
+     SHOW_SCOPE_GLOBAL},
+    {"fast_query_cache_hits_count", (char *)&fast_query_cache_hits_count,SHOW_LONG, 
+     SHOW_SCOPE_GLOBAL},
+    {"fast_query_cache_invalidate_count", (char *)&fast_query_cache_invalidate_count,SHOW_LONG, 
+     SHOW_SCOPE_GLOBAL},
     {NullS, NullS, SHOW_LONG, SHOW_SCOPE_ALL}};
 
 void add_terminator(vector<my_option> *options) {
@@ -12399,6 +12418,7 @@ PSI_mutex_key key_mutex_replica_worker_hash;
 PSI_mutex_key key_monitor_info_run_lock;
 PSI_mutex_key key_LOCK_delegate_connection_mutex;
 PSI_mutex_key key_LOCK_group_replication_connection_mutex;
+PSI_mutex_key key_Memory_pool_mutex;
 
 /* clang-format off */
 static PSI_mutex_info all_server_mutexes[]=
@@ -12491,6 +12511,7 @@ static PSI_mutex_info all_server_mutexes[]=
   { &key_LOCK_group_replication_connection_mutex, "LOCK_group_replication_connection_mutex", PSI_FLAG_SINGLETON, 0, PSI_DOCUMENT_ME},
 { &key_LOCK_authentication_policy, "LOCK_authentication_policy", PSI_FLAG_SINGLETON, 0, "A lock to ensure execution of CREATE USER or ALTER USER sql and SET @@global.authentication_policy variable are serialized"},
   { &key_LOCK_global_conn_mem_limit, "LOCK_global_conn_mem_limit", PSI_FLAG_SINGLETON, 0, PSI_DOCUMENT_ME},
+  { &key_Memory_pool_mutex, "Memory_pool_mutex", 0, 0, PSI_DOCUMENT_ME},
   { &key_LOCK_pq_threads_running, "LOCK_pq_threads_running", PSI_FLAG_SINGLETON, 0, PSI_DOCUMENT_ME},
   { &key_LOCK_g_filter_rules, "LOCK_g_filter_rules", PSI_FLAG_SINGLETON, 0, PSI_DOCUMENT_ME}
 };
@@ -12717,6 +12738,8 @@ PSI_stage_info stage_alter_inplace_commit= { 0, "committing alter table to stora
 PSI_stage_info stage_changing_source= { 0, "Changing replication source", 0, PSI_DOCUMENT_ME};
 PSI_stage_info stage_checking_source_version= { 0, "Checking source version", 0, PSI_DOCUMENT_ME};
 PSI_stage_info stage_checking_permissions= { 0, "checking permissions", 0, PSI_DOCUMENT_ME};
+PSI_stage_info stage_checking_privileges_on_fast_cached_query= { 0, "checking privileges on fast cached query", 0, PSI_DOCUMENT_ME};
+PSI_stage_info stage_checking_fast_query_cache_for_query= { 0, "checking fast query cache for query", 0};
 PSI_stage_info stage_cleaning_up= { 0, "cleaning up", 0, PSI_DOCUMENT_ME};
 PSI_stage_info stage_closing_tables= { 0, "closing tables", 0, PSI_DOCUMENT_ME};
 PSI_stage_info stage_compressing_gtid_table= { 0, "Compressing gtid_executed table", 0, PSI_DOCUMENT_ME};
@@ -12757,6 +12780,7 @@ PSI_stage_info stage_rename_result_table= { 0, "rename result table", 0, PSI_DOC
 PSI_stage_info stage_requesting_binlog_dump= { 0, "Requesting binlog dump", 0, PSI_DOCUMENT_ME};
 PSI_stage_info stage_searching_rows_for_update= { 0, "Searching rows for update", 0, PSI_DOCUMENT_ME};
 PSI_stage_info stage_sending_binlog_event_to_replica= { 0, "Sending binlog event to replica", 0, PSI_DOCUMENT_ME};
+PSI_stage_info stage_sending_fast_cached_result_to_client= { 0, "sending cached result to client", 0, PSI_DOCUMENT_ME};
 PSI_stage_info stage_setup= { 0, "setup", 0, PSI_DOCUMENT_ME};
 PSI_stage_info stage_replica_has_read_all_relay_log= { 0, "Replica has read all relay log; waiting for more updates", 0, PSI_DOCUMENT_ME};
 PSI_stage_info stage_replica_reconnecting_after_failed_binlog_dump_request{ 0, "Reconnecting after a failed binlog dump request", 0, PSI_DOCUMENT_ME};
@@ -12820,6 +12844,8 @@ PSI_stage_info *all_server_stages[] = {
     &stage_changing_source,
     &stage_checking_source_version,
     &stage_checking_permissions,
+    &stage_checking_privileges_on_fast_cached_query,
+    &stage_checking_fast_query_cache_for_query,
     &stage_cleaning_up,
     &stage_closing_tables,
     &stage_compressing_gtid_table,
@@ -12860,6 +12886,7 @@ PSI_stage_info *all_server_stages[] = {
     &stage_requesting_binlog_dump,
     &stage_searching_rows_for_update,
     &stage_sending_binlog_event_to_replica,
+    &stage_sending_fast_cached_result_to_client,
     &stage_setup,
     &stage_replica_has_read_all_relay_log,
     &stage_replica_reconnecting_after_failed_binlog_dump_request,
diff --git a/sql/mysqld.h b/sql/mysqld.h
index f1911e1..eb719ed 100644
--- a/sql/mysqld.h
+++ b/sql/mysqld.h
@@ -335,11 +335,15 @@ extern char server_build_id[42];
 extern const char *server_build_id_ptr;
 #endif
 extern const double log_10[309];
-extern ulong_typ binlog_cache_use, binlog_cache_disk_use;
-extern ulong_typ binlog_stmt_cache_use, binlog_stmt_cache_disk_use;
-extern ulong_typ aborted_threads;
-extern ulong_typ delayed_insert_timeout;
-extern ulong_typ delayed_insert_limit, delayed_queue_size;
+extern ulong binlog_cache_use, binlog_cache_disk_use;
+extern ulong binlog_stmt_cache_use, binlog_stmt_cache_disk_use;
+extern ulong aborted_threads;
+extern ulonglong_typ fast_query_cache_hits_count;
+extern ulonglong_typ fast_query_cache_invalidate_count;
+extern bool  fast_query_cache_enable;
+extern ulong fast_query_cache_single_max_memory;
+extern ulong delayed_insert_timeout;
+extern ulong delayed_insert_limit, delayed_queue_size;
 extern std::atomic<int32> atomic_replica_open_temp_tables;
 extern ulong_typ slow_launch_time;
 extern ulong_typ table_cache_size;
@@ -507,6 +511,7 @@ extern PSI_mutex_key key_LOCK_group_replication_connection_mutex;
 
 extern PSI_mutex_key key_commit_order_manager_mutex;
 extern PSI_mutex_key key_mutex_replica_worker_hash;
+extern PSI_mutex_key key_Memory_pool_mutex;
 
 extern PSI_rwlock_key key_rwlock_LOCK_logger;
 extern PSI_rwlock_key key_rwlock_channel_map_lock;
@@ -594,6 +599,8 @@ extern PSI_stage_info stage_alter_inplace_commit;
 extern PSI_stage_info stage_changing_source;
 extern PSI_stage_info stage_checking_source_version;
 extern PSI_stage_info stage_checking_permissions;
+extern PSI_stage_info stage_checking_privileges_on_fast_cached_query;
+extern PSI_stage_info stage_checking_fast_query_cache_for_query;
 extern PSI_stage_info stage_cleaning_up;
 extern PSI_stage_info stage_closing_tables;
 extern PSI_stage_info stage_compressing_gtid_table;
@@ -635,6 +642,7 @@ extern PSI_stage_info stage_rename_result_table;
 extern PSI_stage_info stage_requesting_binlog_dump;
 extern PSI_stage_info stage_searching_rows_for_update;
 extern PSI_stage_info stage_sending_binlog_event_to_replica;
+extern PSI_stage_info stage_sending_fast_cached_result_to_client;
 extern PSI_stage_info stage_setup;
 extern PSI_stage_info stage_replica_has_read_all_relay_log;
 extern PSI_stage_info
diff --git a/sql/sp_instr.cc b/sql/sp_instr.cc
index ad2625c..f471e2d 100644
--- a/sql/sp_instr.cc
+++ b/sql/sp_instr.cc
@@ -334,6 +334,9 @@ bool sp_lex_instr::reset_lex_and_exec_core(THD *thd_g, uint *nextp,
 
   thd_g->set_query_id(next_query_id());
 
+  //function or procedure don't cached
+  thd_g->lex->safe_to_cache_query = false;    
+  
   if (thd_g->locked_tables_mode <= LTM_LOCK_TABLES) {
     /*
       This statement will enter/leave prelocked mode on its own.
diff --git a/sql/sql_base.cc b/sql/sql_base.cc
index 2ce2f34..b0d8c8b 100644
--- a/sql/sql_base.cc
+++ b/sql/sql_base.cc
@@ -152,6 +152,7 @@
 #include "sql_string.h"
 #include "template_utils.h"
 #include "thr_mutex.h"
+#include "sql/sql_fast_query_cache.h"
 
 using std::equal_to;
 using std::hash;
@@ -10285,6 +10286,24 @@ void tdc_remove_table(THD *thd_g, enum_tdc_remove_table_type remove_type,
   auto remove_table = [&](Table_definition_cache::iterator my_it) {
     if (my_it == table_def_cache->end()) return;
     TABLE_SHARE *share = my_it->second.get();
+
+    /**When DDL, drop table, truncate table, drop database, truncate database
+     * occurs, tdc_remove_table will be called to refresh TABLE_SHARE and TABLE.
+     * FAST QUERY CACHE needs to add deletion logic here
+     */
+    if (fast_query_cache_enable) {
+      Fast_query_cache::getInstance().qc_invalidate_table(share->db, share->table_name);
+
+      /*meanwhile write redo to slave node.*/
+      handlerton *ddse = ha_resolve_by_legacy_type(thd_g, DB_TYPE_INNODB);
+
+      if(ddse->qc_invalidate_table) {
+        char table_full_name[NAME_LEN * 5];
+        sprintf(table_full_name, "%s/%s",share->db.str, share->table_name.str);
+
+        ddse->qc_invalidate_table(thd_g, table_full_name);
+      }
+    }
     /*
       Since share->ref_count is incremented when a table share is opened
       in get_table_share(), before LOCK_open is temporarily released, it
diff --git a/sql/sql_class.cc b/sql/sql_class.cc
index a24ec3a..da08fc0 100644
--- a/sql/sql_class.cc
+++ b/sql/sql_class.cc
@@ -119,6 +119,7 @@
 #include "sql/dd/impl/upgrade/dd.h"
 #include "base/logging.h"
 #include "sql/sql_parallel.h"
+#include "sql/sql_fast_query_cache.h"
 
 class Parse_tree_root;
 
@@ -1101,6 +1102,10 @@ Sql_condition *THD::raise_condition(uint sql_errno, const char *sqlstate,
   MYSQL_LOG_ERROR(sql_errno, PSI_ERROR_OPERATION_RAISED);
   if (handle_condition(sql_errno, sqlstate, &level, msg)) return nullptr;
 
+  if(fast_query_cache_enable) {
+    Fast_query_cache::getInstance().abort(this);
+  }
+
   Diagnostics_area *da = get_stmt_da();
   if (level == Sql_condition::SL_ERROR) {
     /*
@@ -1193,6 +1198,8 @@ void THD::init(void) {
   memset(&status_var, 0, sizeof(status_var));
   binlog_row_event_extra_data = nullptr;
 
+  variables.fast_query_cache = fast_query_cache_enable;
+
   if (variables.sql_log_bin)
     variables.option_bits |= OPTION_BIN_LOG;
   else
diff --git a/sql/sql_class.h b/sql/sql_class.h
index a991396..0c91f1e 100644
--- a/sql/sql_class.h
+++ b/sql/sql_class.h
@@ -141,6 +141,7 @@ struct TABLE_LIST;
 struct timeval;
 struct User_level_lock;
 struct YYLTYPE;
+class Fast_query_cache_result;
 
 class ResultSet;
 
@@ -1179,6 +1180,16 @@ class THD : public MDL_context_owner,
   */
   static const char *const DEFAULT_WHERE;
 
+  /*
+   When query_story is completed, it is set to true. 
+   The subsequent decision on whether to store the result is based on this variable.
+  */
+  bool fast_query_cache_store_query{false};
+  Fast_query_cache_result* fast_query_cache_result {nullptr};
+  /* Save table_db & table_name to list for fast_query_cache */
+  std::list<LEX_STRING> m_qc_stmt_table_list;
+  bool prepared_stmt_query{false};
+
   /** Additional network instrumentation for the server only. */
   NET_SERVER m_net_server_extension;
   /**
diff --git a/sql/sql_delete.cc b/sql/sql_delete.cc
index 47b1004..038cdea 100644
--- a/sql/sql_delete.cc
+++ b/sql/sql_delete.cc
@@ -87,6 +87,7 @@
 #include "sql/transaction_info.h"
 #include "sql/trigger_def.h"
 #include "sql/uniques.h"  // Unique
+#include "sql/sql_fast_query_cache.h"
 
 class COND_EQUAL;
 class Item_exists_subselect;
@@ -646,6 +647,10 @@ bool Sql_cmd_delete::delete_from_single_table(THD *thd_g) {
 cleanup:
   assert(!lex->is_explain());
 
+  if(deleted_rows && fast_query_cache_enable) {
+    Fast_query_cache::getInstance().qc_invalidate_single(thd_g, delete_table_ref);
+  }
+
   if (!transactional_table && deleted_rows > 0)
     thd_g->get_transaction()->mark_modified_non_trans_table(
         Transaction_ctx::STMT);
diff --git a/sql/sql_digest.cc b/sql/sql_digest.cc
index 8351345..c7fad1b 100644
--- a/sql/sql_digest.cc
+++ b/sql/sql_digest.cc
@@ -701,3 +701,22 @@ sql_digest_state *digest_reduce_token(sql_digest_state *state, uint token_left,
 
   return state;
 }
+
+/**
+* @brief Calculate the SHA256 hash value of the query string
+*
+* Calculate the hash value of the given query string using the SHA256 algorithm and store the result in hash_key.
+*
+* @param query_string Input query string
+* @param hash_key String used to store the calculated hash value
+ */
+void compute_query_digest_hash(std::string &query_string, std::string &hash_key) {
+  unsigned char sha256sum[SHA256_DIGEST_LENGTH];
+  (void)SHA_EVP256(pointer_cast<const unsigned char *>(query_string.data()),
+                   query_string.size(), sha256sum);
+
+  hash_key.reserve(DIGEST_HASH_TO_STRING_LENGTH);
+  hash_key.resize(DIGEST_HASH_TO_STRING_LENGTH);
+  
+  DIGEST_HASH_TO_STRING(sha256sum, hash_key.data());
+}
diff --git a/sql/sql_digest.h b/sql/sql_digest.h
index 91b01ec..f081a45 100644
--- a/sql/sql_digest.h
+++ b/sql/sql_digest.h
@@ -25,6 +25,7 @@
 
 #include <string.h>
 #include <sys/types.h>
+#include <string>
 
 #include "my_inttypes.h"  // IWYU pragma: keep
 
@@ -181,4 +182,6 @@ void compute_digest_hash(const sql_digest_storage *digest_storage,
 void compute_digest_text(const sql_digest_storage *digest_storage,
                          String *digest_text);
 
+/*Calculate the SHA256 hash value of the query string*/
+void compute_query_digest_hash(std::string &query_string, std::string &hash_key);
 #endif
diff --git a/sql/sql_fast_query_cache.cc b/sql/sql_fast_query_cache.cc
new file mode 100644
index 0000000..3f8194f
--- /dev/null
+++ b/sql/sql_fast_query_cache.cc
@@ -0,0 +1,1431 @@
+/* Copyright (c) 2000, 2023, Oracle and/or its affiliates.
+
+   This program is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License, version 2.0,
+   as published by the Free Software Foundation.
+
+   This program is also distributed with certain software (including
+   but not limited to OpenSSL) that is licensed under separate terms,
+   as designated in a particular file or component or in included license
+   documentation.  The authors of MySQL hereby grant you an additional
+   permission to link the program and your derivative works with the
+   separately licensed software that they have included with MySQL.
+
+   This program is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License, version 2.0, for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program */
+#include "sql_fast_query_cache.h"
+
+#include <sys/types.h>
+#include <cstring>
+#include <string>
+#include "handler.h"
+#include "mysqld.h"
+#include "opt_trace.h"
+#include "sql/auth/auth_acls.h"  // SELECT_ACL
+#include "sql/current_thd.h"
+#include "sql/protocol_classic.h"
+#include "sql/sql_class.h"
+#include "sql/table.h"
+#include "sql_base.h"  // TMP_TABLE_KEY_EXTRA
+#include "sql_lex.h"
+#include "sql_parse.h"  // check_table_access
+#include "sql_table.h"
+#include "transaction.h"
+#include "tztime.h"  // struct Time_zone
+
+#include "my_alloc.h"
+#include "mysql/psi/mysql_rwlock.h"
+#include "sql/handler.h"
+#include "sql/sql_digest.h"
+
+#if defined(HAVE_PAUSE_INSTRUCTION)
+#define UT_RELAX_CPU() __asm__ __volatile__("pause")
+#elif defined(HAVE_FAKE_PAUSE_INSTRUCTION)
+#define UT_RELAX_CPU() __asm__ __volatile__("rep; nop")
+#elif defined _WIN32
+#define UT_RELAX_CPU() YieldProcessor()
+#elif defined(__aarch64__)
+#define UT_RELAX_CPU() __asm__ __volatile__("isb" ::: "memory")
+#else
+#define UT_RELAX_CPU() __asm__ __volatile__("" ::: "memory")
+#endif
+
+Fast_query_cache *Fast_query_cache::instance = nullptr;
+
+// fast query cache map max size
+std::atomic<ulonglong_typ> fast_query_cache_map_max_mem_size;
+ulong fast_query_cache_map_size = 0;
+ulong fast_query_cache_map_memory = 0;
+
+/**
+ * @brief Moves a node to the head of the LRU list
+ *
+ * Moves the given node to the head of the LRU list. If the node is not in the
+ * list, no operation is performed.
+ *
+ * @param node Pointer to the node to be moved
+ */
+void Fast_query_cache::lru_move_node(Fast_query_cache_node *node) {
+  if (node == nullptr) return;
+
+  if (node != head) {
+    assert(node->prev != nullptr);
+
+    if (node == tail) {
+      assert(node->next == nullptr);
+
+      tail = node->prev;
+      tail->next = nullptr;
+    } else {
+      assert(node->next != nullptr);
+
+      node->prev->next = node->next;
+      node->next->prev = node->prev;
+    }
+
+    node->prev = nullptr;
+    node->next = head;
+    head->prev = node;
+
+    head = node;
+  }
+}
+
+/**
+ * @brief Inserts query result into fast query cache
+ *
+ * Inserts query result into the fast query cache. If the cache is full,
+ * uses LRU algorithm to evict old data to make space.
+ *
+ * @param query_hash Query hash value
+ * @param fast_query_cache_result Pointer to query result object
+ * @param result_size Size of query result object
+ *
+ * @return Returns true if query result already exists in cache; otherwise false
+ */
+bool Fast_query_cache::insert(query_t &query_hash,
+                              Fast_query_cache_result *fast_query_cache_result,
+                              size_t result_size) {
+  assert(fast_query_cache_result);
+  assert(fast_query_cache_result->end_of_result == false);
+
+  // 1. Acquire write lock
+  Fast_query_cache_wrlock lock(&m_lock_cache);
+
+  // Skip if already exists in map (query->result)
+  if (m_fast_query_cache_map.find(query_hash) != m_fast_query_cache_map.end())
+    return true;
+
+  // Apply LRU access queue (not strict, Query_cache_node size not counted yet)
+  if (fast_query_cache_map_memory + result_size >
+      fast_query_cache_map_max_mem_size.load()) {
+    while (!lru_read_cache.empty()) {
+      lru_move_node(lru_read_cache.dequeue());
+    }
+  }
+
+  // Keep evicting via LRU until there's enough space for result_size
+  while (tail != nullptr && fast_query_cache_map_memory + result_size >
+                                fast_query_cache_map_max_mem_size.load()) {
+    if (lru_eliminate()) {
+      return true;
+    }
+  }
+
+  m_fast_query_cache_map.insert(
+      std::make_pair(query_hash, fast_query_cache_result));
+  // Add object to LRU list head
+  Fast_query_cache_node *node =
+      new (&memory_pool, fast_query_cache_result->get_group_id())
+          Fast_query_cache_node(query_hash);
+
+  if (head == nullptr) {
+    assert(tail == nullptr);
+
+    head = node;
+    tail = node;
+  } else {
+    assert(head->prev == nullptr);
+
+    head->prev = node;
+    node->next = head;
+    head = node;
+  }
+
+  // Establish Fast_query_cache_result->query_cache_node connection
+  fast_query_cache_result->set_lru_ptr(node);
+
+  // Update memory size
+  result_size =
+      memory_pool.get_group_space(fast_query_cache_result->get_group_id());
+  fast_query_cache_map_memory += result_size;
+  fast_query_cache_result->current_result_size = result_size;
+  fast_query_cache_map_size++;
+
+  return false;
+}
+
+/**
+ * @brief Destructor for cleaning up Fast_query_cache_result object resources
+ *
+ * This destructor cleans up the table list (table_list) and result buffer list
+ * (result_buffer_list).
+ *
+ * @note
+ * When this destructor is called, memory occupied by table_list and
+ * result_buffer_list will be automatically released.
+ */
+Fast_query_cache_result::~Fast_query_cache_result() {
+  // Clean up table list
+  table_list.clear();
+  result_buffer_list.clear();
+}
+
+/**
+ * @brief Gets query result from fast query cache
+ *
+ * Retrieves query result matching the given query hash from fast query cache.
+ *
+ * @param query_hash Query hash value
+ * @return Returns pointer to matching query result if found; otherwise nullptr
+ */
+Fast_query_cache_result *Fast_query_cache::get(query_t &query_hash) {
+  Fast_query_cache_rdlock lock(&m_lock_cache);
+
+  const auto it = m_fast_query_cache_map.find(query_hash);
+  if (it != m_fast_query_cache_map.end()) {
+    Fast_query_cache_result *cache_body = it->second;
+    assert(cache_body);
+    Fast_query_cache_node *node = it->second->get_lru_ptr();
+
+    // Put current node into LRU access queue
+    lru_read_cache.enqueue(node);
+
+    cache_body->ref_count_add_one();
+    return cache_body;
+  }
+
+  return nullptr;
+}
+
+/**
+ * @brief Checks if specified query hash exists in query cache
+ *
+ * Checks if an entry matching the given query hash exists in Fast_query_cache.
+ *
+ * @param query_hash Query hash to check
+ * @return Returns true if cache contains entry matching query_hash; otherwise
+ * false
+ */
+bool Fast_query_cache::exist(query_t &query_hash) {
+  Fast_query_cache_rdlock lock(&m_lock_cache);
+
+  return m_fast_query_cache_map.find(query_hash) !=
+         m_fast_query_cache_map.end();
+}
+
+/**
+ * @brief Discards cache by table name
+ *
+ * Discards cache related to the given table name. If query statement is
+ * specified, only discards cache related to that query statement.
+ *  m_table_cache_map              m_fast_query_cache_map
+ * [t1, sql1]                     [sql1, result1]
+ * [t1, sql2]                     [sql2, result2]
+ * [t2, sql2]                     [sql2, result2]  Points to same address
+ * [t2, sql3]                     [sql3, result3]
+ * For example, when cleaning t2:
+ * 1. First find SQL set related to t2 from m_table_cache_map [t2, sql2][t2,
+ * sql3].
+ * 2. Then traverse m_fast_query_cache_map, find corresponding [sql2, result2],
+ * [sql3, result3] and delete.
+ * 3. Then find sql2 also involves t1, so go back to m_table_cache_map
+ *    to find table information related to sql and delete [t1, sql2]
+ * 4. Finally only [t1, sql1]                     [sql1, result1] remain
+ */
+void Fast_query_cache::discard_cache_by_table(std::string &table,
+                                              const std::string &query = "") {
+  Fast_query_cache_wrlock lock(&m_lock_cache);
+  Fast_query_cache_wrlock lock_table(&m_lock_table);
+
+  // Abort scenario or large result scenario not counted in statistics
+  if (query.empty()) {
+    fast_query_cache_invalidate_count++;
+  }
+
+  /* Optimization: if map is empty, return directly */
+  if (fast_query_cache_map_size == 0) return;
+
+  const auto &it_table_cache = m_table_cache_map.find(table);
+  if (it_table_cache == m_table_cache_map.end()) {
+    return;
+  }
+
+  // Apply LRU access queue
+  while (!lru_read_cache.empty()) {
+    lru_move_node(lru_read_cache.dequeue());
+  }
+
+  const auto &query_hash_vct = it_table_cache->second;
+  // 1. First find set<query> collection, then traverse
+  for (auto &it_query : query_hash_vct) {
+    // If query is specified, only delete cache for specified query
+    if (!query.empty() && it_query != query) {
+      continue;
+    }
+
+    // 2. Invalidate m_large_result_query_map first
+    auto it_large_query = m_large_result_query_map.find(it_query);
+    if (it_large_query != m_large_result_query_map.end()) {
+      m_large_result_query_map.erase(it_large_query);
+    }
+
+    // 3. For each query, find corresponding result in m_fast_query_cache_map
+    const auto &it_pc = m_fast_query_cache_map.find(it_query);
+    if (it_pc == m_fast_query_cache_map.end()) {
+      continue;
+    }
+
+    Fast_query_cache_result *cache_body = it_pc->second;
+    assert(cache_body != nullptr);
+
+    while (cache_body->ref_count_load() > 0) {
+      UT_RELAX_CPU();
+    }
+
+    // 4. Remove corresponding fast_query_cahce_node from LRU list
+    Fast_query_cache_node *node = cache_body->get_lru_ptr();
+
+    if (head == tail) {
+      head = nullptr;
+      tail = nullptr;
+    } else if (node == head) {
+      head = head->next;
+      head->prev = nullptr;
+    } else if (node == tail) {
+      tail = tail->prev;
+      tail->next = nullptr;
+    } else {
+      node->prev->next = node->next;
+      node->next->prev = node->prev;
+    }
+
+    // 5. Release Fast_query_cache_node memory
+    if (node != nullptr) {
+      node->~Fast_query_cache_node();
+    }
+
+    // 6. Update table_cache_map
+    std::list<LEX_STRING> &table_ids = cache_body->get_table_ids();
+
+    for (auto it_table = table_ids.begin(); it_table != table_ids.end();
+         it_table++) {
+      std::string key(it_table->str, (it_table->length - 1));
+      if (table == key) {
+        // 6.1 If current table, skip
+        continue;
+      }
+
+      // 6.2 Find other tables involved by this query and delete from
+      // m_table_cache_map
+      auto it_table_cache2 = m_table_cache_map.find(key);
+      if (it_table_cache2 != m_table_cache_map.end()) {
+        auto &query_hash_vct2 = it_table_cache2->second;
+        auto it_pc2 = query_hash_vct2.find(it_query);
+        if (it_pc2 != query_hash_vct2.end()) {
+          query_hash_vct2.erase(it_pc2);
+        }
+
+        if (query_hash_vct2.empty()) {
+          // If table's related query is empty, delete from m_table_cache_map
+          m_table_cache_map.erase(it_table_cache2);
+        }
+      }
+    }
+    // 7. Delete Fast_query_cache_result, while cleaning corresponding memory
+    MemoryPool *mp = Fast_query_cache::getInstance().get_mp();
+    int64 group_id = cache_body->get_group_id();
+    ulong body_size = mp->get_group_space(group_id);
+
+    cache_body->~Fast_query_cache_result();
+    operator delete(cache_body, mp, group_id);
+
+    // doule check
+    assert(mp->get_group_space(group_id) == 0);
+
+    // 8. Update memory usage
+    fast_query_cache_map_memory -= body_size;
+    fast_query_cache_map_size--;
+
+    m_fast_query_cache_map.erase(it_pc);
+  }
+
+  m_table_cache_map.erase(it_table_cache);
+}
+
+/**
+ @brief Performs LRU eviction
+ */
+bool Fast_query_cache::lru_eliminate() {
+  const auto it_fast_query_cache =
+      m_fast_query_cache_map.find(tail->query_hash);
+  if (it_fast_query_cache == m_fast_query_cache_map.end()) {
+    // ...
+    return true;
+  }
+
+  Fast_query_cache_result *cache_body = it_fast_query_cache->second;
+  assert(cache_body);
+
+  /*
+    Try to delete oldest hash from plan_cache_map
+    Prevent cleanup while get copy is not completed
+  */
+  if (unlikely(cache_body->ref_count_load() != 0)) {
+    return true;
+  }
+
+  // Prevent cleanup before data is fully prepared (very low probability)
+  if (unlikely(cache_body->end_of_result == false)) {
+    return true;
+  }
+
+  int64 group_id = cache_body->get_group_id();
+  ulong body_size = memory_pool.get_group_space(group_id);
+
+  // Traverse tables involved by current query_cache
+  for (LEX_STRING &table_key : cache_body->get_table_ids()) {
+    // Find hash of involved tables
+    std::string key(table_key.str, (table_key.length - 1));
+    auto it = m_table_cache_map.find(key);
+    if (it != m_table_cache_map.end()) {
+      it->second.erase(tail->query_hash);
+    }
+  }
+
+  // Delete oldest hash from LRU list
+  Fast_query_cache_node *cur = tail;
+  if (head != tail) {
+    tail = tail->prev;
+    tail->next = nullptr;
+  } else {
+    head = nullptr;
+    tail = nullptr;
+  }
+
+  cur->~Fast_query_cache_node();
+
+  cache_body->~Fast_query_cache_result();
+  operator delete(cache_body, &memory_pool, group_id);
+
+  m_fast_query_cache_map.erase(it_fast_query_cache);
+  fast_query_cache_map_memory -= body_size;
+  fast_query_cache_map_size--;
+
+  return false;
+}
+
+/**
+  Send a single memory block from the fast query cache.
+
+  Respects the client/server protocol limits for the
+  size of the network packet, and splits a large block
+  in pieces to ensure that individual piece doesn't exceed
+  the maximal allowed size of the network packet (16M).
+
+  @param[in] net NET handler
+  @param[in] packet packet to send
+  @param[in] len packet length
+
+  @return Operation status
+    @retval false On success
+    @retval true On error
+*/
+static bool send_data_in_chunks(NET *net, const uchar_t *packet, ulong len) {
+  /*
+    On the client we may require more memory than max_allowed_packet
+    to keep, both, the truncated last logical packet, and the
+    compressed next packet.  This never (or in practice never)
+    happens without compression, since without compression it's very
+    unlikely that a) a truncated logical packet would remain on the
+    client when it's time to read the next packet b) a subsequent
+    logical packet that is being read would be so large that
+    size-of-new-packet + size-of-old-packet-tail >
+    max_allowed_packet.  To remedy this issue, we send data in 1MB
+    sized packets, that's below the current client default of 16MB
+    for max_allowed_packet, but large enough to ensure there is no
+    unnecessary overhead from too many syscalls per result set.
+  */
+  static const ulong MAX_CHUNK_LENGTH = 1024 * 1024;
+
+  while (len > MAX_CHUNK_LENGTH) {
+    if (net_write_packet(net, packet, MAX_CHUNK_LENGTH)) return true;
+    packet += MAX_CHUNK_LENGTH;
+    len -= MAX_CHUNK_LENGTH;
+  }
+  if (len && net_write_packet(net, packet, len)) return true;
+
+  return false;
+}
+
+/**
+ * @brief Checks if fast query cache is enabled
+ *
+ * Determines if fast query cache is enabled based on fast_query_cache_enable
+ * flag in thread variables.
+ *
+ * @param thd_g Thread object pointer (may be unused)
+ * @return Returns true if fast query cache is enabled; otherwise false
+ */
+bool use_fast_query_cache() { return fast_query_cache_enable; }
+
+/*
+  Collect information about table types, check that tables are cachable and
+  count them
+
+  SYNOPSIS
+    process_and_count_tables()
+    tables_used     table list for processing
+    tables_type     pointer to variable for table types collection
+
+  RETURN
+    0   error
+    >0  number of tables
+*/
+int Fast_query_cache::process_and_count_tables(THD *thd_g, Table_ref *tables_used,
+                                               uint8 *tables_type) {
+  DBUG_TRACE;
+  int table_count = 0;
+  for (; tables_used; tables_used = tables_used->next_global) {
+    table_count++;
+    /*
+      Disable any attempt to store this statement if there are
+      column level grants on any referenced tables.
+      The grant.want_privileges flag was set to 1 in the
+      check_grant() function earlier if the Table_ref object
+      had any associated column privileges.
+
+      We need to check that the Table_ref object isn't part
+      of a VIEW definition because we want to be able to cache
+      views.
+
+      Tables underlying a MERGE table does not have useful privilege
+      information in their grant objects, so skip these tables from the test.
+    */
+    if (tables_used->belong_to_view == NULL &&
+        (!tables_used->parent_l ||
+         tables_used->parent_l->table->file->ht->db_type !=
+             DB_TYPE_MRG_MYISAM)) {
+      if ((tables_used->grant.privilege & SELECT_ACL) == 0) {
+        DBUG_PRINT("fast qcache", ("Don't cache statement as it refers to "
+                                   "tables with column privileges."));
+        thd_g->lex->safe_to_cache_query = false;
+        return 0;
+      }
+    }
+    if (tables_used->is_view()) {
+      *tables_type |= HA_CACHE_TBL_NONTRANSACT;
+    } else {
+      if (tables_used->is_derived()) {
+        DBUG_PRINT("fast qcache", ("table: %s", tables_used->alias));
+        table_count--;
+        DBUG_PRINT("fast qcache", ("derived table skipped"));
+        continue;
+      }
+      DBUG_PRINT("fast qcache", ("table: %s  db:  %s", tables_used->table_name,
+                                 tables_used->db));
+      *tables_type |= tables_used->table->file->table_cache_type();
+
+      /*
+        table_alias_charset used here because it depends of
+        lower_case_table_names variable
+      */
+      if (tables_used->table->s->tmp_table != NO_TMP_TABLE ||
+          (*tables_type & HA_CACHE_TBL_NOCACHE) ||
+          check_if_system_table_for_query_cache(
+              tables_used->table->s->db.str,
+              tables_used->table->s->table_name.str)) {
+        DBUG_PRINT("fast qcache", ("select not cacheable: temporary or system"
+                                   "other non-cacheable table(s)"));
+        return 0;
+      }
+    }
+  }
+  return table_count;
+}
+
+/*
+  If fast query is cacheable return number tables in query
+  (query without tables are not cached)
+*/
+int Fast_query_cache::is_cacheable(THD *thd_g, LEX *lex, Table_ref *tables_used,
+                                   uint8 *tables_type) {
+  int table_count;
+  DBUG_TRACE;
+
+  if (lex->sql_command == SQLCOM_SELECT && lex->safe_to_cache_query) {
+    if (!(table_count =
+              process_and_count_tables(thd_g, tables_used, tables_type)))
+      return 0;
+
+    if (thd_g->in_multi_stmt_transaction_mode() &&
+        ((*tables_type) & HA_CACHE_TBL_TRANSACT)) {
+      DBUG_PRINT("fast qcache", ("not in autocommin mode"));
+      return 0;
+    }
+    DBUG_PRINT("fast qcache", ("select is using %d tables", table_count));
+    return table_count;
+  }
+
+  return 0;
+}
+
+/*
+  Check handler allowance to cache query with these tables
+
+  SYNOPSYS
+    Fast_query_cache::ask_handler_allowance()
+    thd_g - thread handlers
+    tables_used - tables list used in query
+
+  RETURN
+    0 - caching allowed
+    1 - caching disallowed
+*/
+bool Fast_query_cache::ask_handler_allowance(THD *thd_g, Table_ref *tables_used) {
+  DBUG_TRACE;
+
+  for (; tables_used; tables_used = tables_used->next_global) {
+    TABLE *table;
+    handler *handler;
+    if (!(table = tables_used->table)) continue;
+    handler = table->file;
+    // Allow caching of queries with materialized derived tables or views
+    if (tables_used->uses_materialization()) {
+      /*
+        Skip the derived table itself, but process its underlying tables and
+        other tables that follow.
+      */
+      continue;
+    }
+
+    if (!handler->register_query_cache_table(
+            thd_g, const_cast<char *>(table->s->normalized_path.str),
+            table->s->normalized_path.length, &tables_used->callback_func,
+            &tables_used->engine_data)) {
+      DBUG_PRINT("fast qcache", ("Handler does not allow caching for %s.%s",
+                                 tables_used->db, tables_used->alias));
+      thd_g->lex->safe_to_cache_query = 0;  // Don't try to cache this
+      return true;
+    }
+  }
+  return false;
+}
+
+static bool check_table_is_tmp(Table_ref *tables_used) {
+  for (; tables_used; tables_used = tables_used->next_global) {
+    TABLE *table;
+    if (!(table = tables_used->table)) continue;
+    if (table->s->tmp_table != NO_TMP_TABLE) return true;
+  }
+  return false;
+}
+
+/*****************************************************************************
+  interface
+*****************************************************************************/
+/**
+ * @brief Stores query table information in fast query cache
+ *
+ * This function saves table information involved in the query to hashmap.
+ * First checks certain conditions like whether query is cacheable, uses temp
+ * tables, has transaction tracking enabled, etc. If conditions are met,
+ * traverses table reference list and inserts query digest into corresponding
+ * cache map.
+ *
+ * @param thd_g Current thread object
+ * @param tables_used Table reference list
+ */
+void Fast_query_cache::store_query(THD *thd_g, Table_ref *tables_used) {
+  DBUG_TRACE;
+
+  if (thd_g->m_qc_stmt_table_list.size() != 0) {
+    thd_g->m_qc_stmt_table_list.clear();
+  }
+
+  if (thd_g->fast_query_cache_result != nullptr)
+    thd_g->fast_query_cache_result = nullptr;
+
+  size_t local_tables;
+  query_t query_digest, query_string;
+  /*
+    Testing 'query_cache_size' without a lock here is safe: the thing
+    we may loose is that the query won't be cached, but we save on
+    mutex locking in the case when query cache is disabled or the
+    query is uncachable.
+
+    See also a note on double-check.
+  */
+  if (thd_g->locked_tables_mode || fast_query_cache_map_max_mem_size.load() == 0)
+    return;
+
+  if (check_table_is_tmp(tables_used)) return;
+
+  /*
+    Do not store queries while tracking transaction state.
+    The tracker already flags queries that actually have
+    transaction tracker items, but this will make behavior
+    more straight forward.
+  */
+  if (thd_g->variables.session_track_transaction_info != TX_TRACK_NONE) return;
+
+  /*
+    The query cache is only supported for the classic protocols.
+    Although protocol_callback.cc is not compiled in embedded, there
+    are other protocols. A check outside the non-embedded block is
+    better.
+  */
+  if (!thd_g->is_classic_protocol()) return;
+
+  uint8 tables_type = 0;
+
+  if ((local_tables = is_cacheable(thd_g, thd_g->lex, tables_used, &tables_type)) ==
+      0)
+    return;
+
+  if (ask_handler_allowance(thd_g, tables_used)) {
+    return;
+  }
+
+  if (callback_func == nullptr && tables_used != nullptr)
+    callback_func = tables_used->callback_func;
+
+  // 1. First generate digest for db##query
+  query_string.append(thd_g->db().str, thd_g->db().length);
+  query_string.append("##");
+  query_string.append(thd_g->query().str, thd_g->query().length);
+  compute_query_digest_hash(query_string, query_digest);
+
+  // If exists in this map, means same query was previously abandoned due to
+  // large result set
+  if (m_large_result_query_map.find(query_string) !=
+      m_large_result_query_map.end())
+    return;
+
+  // 2. Acquire lock
+  Fast_query_cache_wrlock lock(&m_lock_table);
+
+  // 3. Traverse and insert into m_table_cache_map one by one
+  for (; tables_used; tables_used = tables_used->next_global) {
+    std::string table_string;
+    if (tables_used->is_view()) {
+      // View processing logic
+      table_string.append(tables_used->db, tables_used->db_length);
+      table_string.append("##");
+      table_string.append(tables_used->table_name,
+                          tables_used->table_name_length);
+    } else {
+      // Normal table processing logic
+      table_string.append(tables_used->table->s->db.str,
+                          tables_used->table->s->db.length);
+      table_string.append("##");
+      table_string.append(tables_used->table->s->table_name.str,
+                          tables_used->table->s->table_name.length);
+    }
+    auto it = m_table_cache_map.find(table_string);
+
+    // Check if table_name already exists in m_table_cache_map.
+    if (it != m_table_cache_map.end()) {
+      // If found, insert query_digest into corresponding set
+      auto result = it->second.insert(query_string);
+
+      // Optimization: if query already exists, no need to continue traversing
+      // and inserting
+      if (result.second == false) {
+        thd_g->fast_query_cache_store_query = false;
+        return;
+      }
+    } else {
+      // If not found, insert new table_name and query_digest
+      std::set<query_t> query_set = {
+          query_string};  // Create new set and initialize
+      m_table_cache_map[table_string] =
+          query_set;  // [db##table_name, digest(db##sql)]
+    }
+
+    // insert into stmt table lists
+    auto make_lex_string = [thd_g](std::string string) -> LEX_STRING {
+      char *pos;
+      size_t total_len = string.size() + 1;
+      int64 group_id = thd_g->query_id;
+      MemoryPool *mp = Fast_query_cache::getInstance().get_mp();
+
+      if ((pos = static_cast<char *>(new (mp, group_id) char[total_len]))) {
+        std::strcpy(pos, string.c_str());
+        pos[total_len] = 0;
+      }
+
+      return {pos, total_len};
+    };
+    thd_g->m_qc_stmt_table_list.push_back(make_lex_string(table_string));
+  }
+
+  // 4. Set flag
+  thd_g->fast_query_cache_store_query = true;
+
+  return;
+}
+
+/*****************************************************************************
+  Functions to store things into the query cache
+*****************************************************************************/
+
+/**
+ * @brief Fast query cache insertion function
+ *
+ * Inserts query result into fast query cache.
+ *
+ * @param packet Query result packet pointer
+ * @param length Query result packet length
+ * @param pkt_nr Packet number
+ */
+void fast_query_cache_insert(const uchar_t *packet, ulong length,
+                             unsigned pkt_nr [[maybe_unused]]) {
+  THD *thd_g = current_thd;
+  ulong result_size = 0;
+  std::string table_string;
+  std::string query_string;
+
+  /*
+    Current_thd can be NULL when a new connection is immediately ended
+    due to "Too many connections". thd_g->store_globals() has not been
+    called at this time and hence my_thread_setspecific_ptr(THR_THD,
+    this) has not been called for this thread.
+  */
+  if (!thd_g) return;
+
+  // No need to insert, meaning this SQL was previously inserted but abandoned
+  // due to large result set
+  if (thd_g->fast_query_cache_store_query == false) return;
+
+  assert(thd_g->m_qc_stmt_table_list.size() != 0);
+
+  query_string.append(thd_g->db().str, thd_g->db().length);
+  query_string.append("##");
+  query_string.append(thd_g->query().str, thd_g->query().length);
+
+  // Build result_data information
+  MemoryPool *mp = Fast_query_cache::getInstance().get_mp();
+  int64 group_id = thd_g->query_id;
+
+  Result_data *result_buffer = new (mp, group_id) Result_data();
+  result_buffer->length = length;
+
+  result_buffer->buf = static_cast<uchar_t *>(mp->palloc(length, group_id));
+  memcpy(result_buffer->buf, packet, length);
+
+  /*
+    No need to build Fast_query_cache_result object, directly store
+    result_buffer, because result set needs to be sent multiple times. Only
+    build during first time
+  */
+  if (thd_g->fast_query_cache_result != nullptr) {
+    result_size = mp->get_group_space(group_id);
+
+    if (result_size >
+        thd_g->fast_query_cache_result->get_query_cache_result_max_mem_size()) {
+      /*
+       Exceeds maximum memory size for single query cache, first erase
+       this thd_g->fast_query_cache_result object
+      */
+      auto tbl_string = thd_g->m_qc_stmt_table_list.front();
+      table_string.append(tbl_string.str, tbl_string.length);
+
+      fast_query_cache_map_memory +=
+          (result_size - thd_g->fast_query_cache_result->current_result_size);
+      Fast_query_cache::getInstance().discard_cache_by_table(table_string,
+                                                             query_string);
+
+      // Insert this sql into m_large_result_query_map
+      Fast_query_cache::getInstance().m_large_result_query_map.insert(
+          query_string);
+
+      // Since the upper limit is exceeded, no further result will be inserted
+      thd_g->fast_query_cache_store_query = false;
+      thd_g->fast_query_cache_result = nullptr;
+
+      return;
+    } else {
+      // Otherwise, insert directly into result_buffer
+      Fast_query_cache_wrlock lock(
+          &Fast_query_cache::getInstance().m_lock_cache);
+
+      thd_g->fast_query_cache_result->result_buffer_list.push_back(result_buffer);
+      // Update fast_query_cache_map_memory
+      fast_query_cache_map_memory +=
+          (result_size - thd_g->fast_query_cache_result->current_result_size);
+      thd_g->fast_query_cache_result->current_result_size = result_size;
+    }
+  } else {
+    // Build the Fast_query_cache_result object for the first time
+    std::list<LEX_STRING> table_list;
+
+    table_list = std::move(thd_g->m_qc_stmt_table_list);
+
+    Fast_query_cache_result *fast_query_cache_result =
+        new (mp, group_id) Fast_query_cache_result(
+            table_list, group_id, fast_query_cache_single_max_memory);
+
+    fast_query_cache_result->result_buffer_list.push_back(result_buffer);
+
+    // insert into Fast_query_cache_result map
+    result_size = mp->get_group_space(group_id);
+    bool result = Fast_query_cache::getInstance().insert(
+        query_string, fast_query_cache_result, result_size);
+
+    if (result == true) {
+      // insert failed
+      fast_query_cache_result->~Fast_query_cache_result();
+      // operator delete(fast_query_cache_result, mp, group_id);
+      mp->pfree(fast_query_cache_result, group_id);
+      thd_g->fast_query_cache_result = nullptr;
+    } else {
+      // After successful insertion, the thd_g->fast_query_cache_result object is
+      // updated
+      thd_g->fast_query_cache_result = fast_query_cache_result;
+      thd_g->fast_query_cache_result->current_result_size = result_size;
+    }
+  }
+
+  return;
+}
+
+/**
+ * @brief Aborts fast query cache operation
+ *
+ * This function is used to abort fast query cache operation, clean up related
+ * cache content, and reset relevant flags.
+ *
+ * @param thd_g Thread object pointer containing current query context information
+ */
+void Fast_query_cache::abort(THD *thd_g) {
+  DBUG_TRACE;
+
+  Fast_query_cache_result *query_cache_result = thd_g->fast_query_cache_result;
+
+  /* See the comment on double-check. */
+  if (!use_fast_query_cache() || thd_g->fast_query_cache_store_query == false)
+    return;
+
+  /*
+    First clean up query content in m_table_cache_map,
+    because during abort there's 99.99% probability that table_cache_map was
+    stored.
+
+     Then clean up Fast_query_cache_result object in fast_query_cache, this may
+    not necessarily exist
+  */
+
+  Table_ref *tbl_ref = thd_g->lex->query_tables;
+
+  if (unlikely(tbl_ref == nullptr)) return;
+
+  {
+    std::string table_string, query_string;
+    table_string.append(tbl_ref->table->s->db.str,
+                        tbl_ref->table->s->db.length);
+    table_string.append("##");
+    table_string.append(tbl_ref->table->s->table_name.str,
+                        tbl_ref->table->s->table_name.length);
+
+    query_string.append(thd_g->db().str, thd_g->db().length);
+    query_string.append("##");
+    query_string.append(thd_g->query().str, thd_g->query().length);
+    // compute_query_digest_hash(query_string, query_digest);
+
+    discard_cache_by_table(table_string, query_string);
+  }
+
+  // Finally, reset flags
+  thd_g->fast_query_cache_store_query = false;
+  thd_g->fast_query_cache_result = nullptr;
+  query_cache_result->end_of_result = true;
+  return;
+}
+
+/**
+ * @brief Operation performed at end of query result
+ *
+ * This function is called when query result ends, used to handle fast query
+ * cache related operations.
+ *
+ * @param thd_g Database connection object pointer
+ */
+void Fast_query_cache::end_of_result(THD *thd_g) {
+  DBUG_TRACE;
+
+  Fast_query_cache_result *query_cache_result = thd_g->fast_query_cache_result;
+  ulonglong_typ current_found_rows = thd_g->current_found_rows;
+
+  /* See the comment on double-check. */
+  if (query_cache_result == nullptr) return;
+
+  if (thd_g->killed || thd_g->is_error()) {
+    abort(thd_g);
+    return;
+  }
+
+  /* Ensure that only complete results are cached. */
+  assert(thd_g->get_stmt_da()->is_eof());
+
+  // Update row count
+  //query_cache_result->current_found_rows = current_found_rows;
+  query_cache_result->end_of_result = true;
+
+  // Reset flags
+  query_cache_result = nullptr;
+  thd_g->fast_query_cache_store_query = false;
+  return;
+}
+
+/*
+  Check if the query is in the cache. If it was cached, send it
+  to the user.
+
+  @param thd_g Pointer to the thread handler
+  @param sql A reference to the sql statement *
+
+  @return status code
+  @retval -1  Query was not cached.
+  @retval 1  The query was cached and user was sent the result.
+  @retval 0 The query was cached but we didn't have rights to use it.
+
+*/
+
+int Fast_query_cache::send_result_to_client(THD *thd_g, const LEX_CSTRING &sql) {
+  query_t query_hash;
+  std::string query_string;
+  query_t query_digest;
+  Fast_query_cache_result *query_result = nullptr;
+  std::list<LEX_STRING> table_lists;
+  Table_ref tbl_ref;
+
+  DBUG_TRACE;
+
+  /*
+    Testing 'query_cache_size' without a lock here is safe: the thing
+    we may loose is that the query won't be served from cache, but we
+    save on mutex locking in the case when query cache is disabled.
+
+    See also a note on double-check.
+  */
+  if (thd_g->locked_tables_mode) return -1;
+
+  /*
+    Don't work with Query_cache if the state of XA transaction is
+    either IDLE or PREPARED. If we didn't do so we would get an
+    assert fired later in the function trx_start_if_not_started_low()
+    that is called when we are checking that query cache is allowed at
+    this moment to operate on an InnoDB table.
+  */
+  if (thd_g->get_transaction()->xid_state()->check_xa_idle_or_prepared(false))
+    return -1;
+
+  /*
+    Don't allow serving from Query_cache while tracking transaction
+    state. This is a safeguard in case an otherwise matching query
+    was added to the cache before tracking was turned on.
+  */
+  if (thd_g->variables.session_track_transaction_info != TX_TRACK_NONE) return -1;
+
+  if (!thd_g->lex->safe_to_cache_query) {
+    DBUG_PRINT("fast_qcache", ("SELECT is non-cacheable"));
+    return -1;
+  }
+
+  {
+    uint i = 0;
+    /*
+      Skip '(' characters in queries like following:
+      (select a from t1) union (select a from t1);
+    */
+    while (sql.str[i] == '(') i++;
+
+    /*
+      Test if the query is a SELECT
+      (pre-space is removed in dispatch_command).
+
+      First '/' looks like comment before command it is not
+      frequently appeared in real life, consequently we can
+      check all such queries, too.
+    */
+    if ((my_toupper(system_charset_info, sql.str[i]) != 'S' ||
+         my_toupper(system_charset_info, sql.str[i + 1]) != 'E' ||
+         my_toupper(system_charset_info, sql.str[i + 2]) != 'L' ||
+         my_toupper(system_charset_info, sql.str[i + 3]) != 'E' ||
+         my_toupper(system_charset_info, sql.str[i + 4]) != 'C' ||
+         my_toupper(system_charset_info, sql.str[i + 5]) != 'T') &&
+        (sql.str[i] != '/' || sql.length < i + 6)) {
+      DBUG_PRINT("fast_qcache", ("The statement is not a SELECT; Not cached"));
+      return -1;
+    }
+  }
+
+  THD_STAGE_INFO(thd_g, stage_checking_fast_query_cache_for_query);
+  ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
+  // 1. First generate query digest, then call get method to check if exists
+  query_string.append(thd_g->db().str, thd_g->db().length);
+  query_string.append("##");
+  query_string.append(thd_g->query().str, thd_g->query().length);
+  // compute_query_digest_hash(query_string, query_digest);
+
+  query_result = get(query_string);
+
+  if (query_result == NULL) {
+    DBUG_PRINT("fast_qcache", ("No query in query hash or no results"));
+    return -1;
+  }
+
+  DBUG_PRINT("fast_qcache",
+             ("Query result in query hash 0x%lx", (ulong)query_result));
+
+  // 2. Very low probability: this SQL is still copying Result_data
+  if (unlikely(query_result->end_of_result == false)) {
+    goto err;
+  }
+
+  /*
+    We only need to clear the diagnostics area when we actually
+    find the query, as in all other cases, we'll go through
+    regular parsing and execution, where the DA will be reset
+    as needed, anyway.
+
+    We're not pushing/popping a private DA here the way we do for
+    parsing; if we got this far, we know we've got a SELECT on our
+    hands and not a diagnotics statement that might need the
+    previous statement's diagnostics area, so we just clear the DA.
+
+    We're doing it here and not in the caller as there's three of
+    them (PS, SP, interactive).  Doing it any earlier in this routine
+    would reset the DA in "SELECT @@error_count"/"SELECT @@warning_count"
+    before we can save the counts we'll need later (QC will see the
+    SELECT go into this branch, but since we haven't parsed yet, we
+    don't know yet that it's one of those legacy variables that require
+    saving and basically turn SELECT into a sort of, sort of not
+    diagnostics command.  Ugly stuff.
+  */
+  thd_g->get_stmt_da()->reset_diagnostics_area();
+  thd_g->get_stmt_da()->reset_condition_info(thd_g);
+
+  // 3. Check permissions for all involved tables
+  THD_STAGE_INFO(thd_g, stage_checking_privileges_on_fast_cached_query);
+  /*
+      Check that we have not temporary tables with same names of tables
+      of this query. If we have such tables, we will not send data from
+      query cache, because temporary tables hide real tables by which
+      query in query cache was made.
+   */
+  if (thd_g->temporary_tables) {
+    DBUG_PRINT("fast_qcache", ("Temporary tables detected"));
+
+    goto err;
+  }
+
+  for (LEX_STRING &table_key : query_result->get_table_ids()) {
+    // table_key format: DB##table_name
+    const char *sep = strstr(table_key.str, "##");
+    char db[256] = {0};
+    char table_name[256] = {0};
+
+    strncpy(db, table_key.str, sep - table_key.str);
+    strncpy(table_name, sep + 2, table_key.length - (sep - table_key.str + 2));
+
+    tbl_ref.db = db;
+    tbl_ref.alias = tbl_ref.table_name = table_name;
+
+    // Check table permissions
+    if (check_table_access(thd_g, SELECT_ACL, &tbl_ref, false, 1, true)) {
+      DBUG_PRINT(
+          "fast qcache",
+          ("probably no SELECT access to %s.%s =>  return to normal processing",
+           tbl_ref.db, tbl_ref.alias));
+
+      thd_g->lex->safe_to_cache_query = 0;  // Don't try to cache this
+
+      goto err;  // Privilege error
+    }
+
+    // Check select permissions
+    if ((tbl_ref.grant.privilege & SELECT_ACL) == 0) {
+      DBUG_PRINT("fast_qcache", ("Need to check column privileges for %s.%s",
+                                 tbl_ref.db, tbl_ref.alias));
+
+      thd_g->lex->safe_to_cache_query = 0;  // Don't try to cache this
+      goto err;                          // Privilege error
+    }
+
+    if (callback_func) {
+      char result[FN_REFLEN + 1];
+      ulonglong_typ engine_data = 0;
+      sprintf(result, "%s/%s", tbl_ref.db, tbl_ref.table_name);
+      if (!(callback_func)(thd_g, result, strlen(result), &engine_data)) {
+        DBUG_PRINT("fast_qcache", ("callback_func() returned error"));
+
+        thd_g->lex->safe_to_cache_query = 0;
+        goto err;
+      }
+    }
+  }
+  /*
+    4. Send cached result to client
+  */
+  THD_STAGE_INFO(thd_g, stage_sending_fast_cached_result_to_client);
+  // Traverse result_buffer_list and send data
+
+  for (auto &result : query_result->result_buffer_list) {
+    if (send_data_in_chunks(thd_g->get_protocol_classic()->get_net(), result->buf,
+                            result->length))
+      break;  // Client aborted
+    // Keep packet number updated
+    // thd_g->get_protocol_classic()->set_pkt_nr(query->last_pkt_nr);
+  }
+
+  // Can only perform LRU eviction after all data is sent
+  query_result->ref_count_sub_one();
+  fast_query_cache_hits_count++;
+
+  thd_g->current_found_rows = query_result->current_found_rows;
+  thd_g->update_previous_found_rows();
+  thd_g->clear_current_query_costs();
+  thd_g->save_current_query_costs();
+
+  {
+    Opt_trace_start ots(thd_g, NULL, SQLCOM_SELECT, NULL, thd_g->query().str,
+                        thd_g->query().length, NULL,
+                        thd_g->variables.character_set_client);
+
+    Opt_trace_object(&thd_g->opt_trace).add("query_result_read_from_cache", true);
+  }
+
+  /*
+    End the statement transaction potentially started by an
+    engine callback. We ignore the return value for now,
+    since as long as EOF packet is part of the query cache
+    response, we can't handle it anyway.
+  */
+  (void)trans_commit_stmt(thd_g);
+  if (!thd_g->get_stmt_da()->is_set()) thd_g->get_stmt_da()->disable_status();
+
+  return 1;  // Result sent to client
+
+err:
+  query_result->ref_count_sub_one();
+  return 0;  // Query was not cached
+}
+
+/*
+  Invalidate locked for write
+
+  SYNOPSIS
+    Fast_query_cache::invalidate()
+    tables_used - table list
+
+  NOTE
+    can be used only for opened tables
+*/
+void Fast_query_cache::qc_invalidate(Table_ref *tables_used) {
+  DBUG_TRACE;
+
+  if (!use_fast_query_cache()) return;
+
+  for (; tables_used; tables_used = tables_used->next_local) {
+    std::string table_string;
+
+    if (tables_used->is_view()) {
+      // view processing logic
+      table_string.append(tables_used->db, tables_used->db_length);
+      table_string.append("##");
+      table_string.append(tables_used->table_name,
+                          tables_used->table_name_length);
+    } else {
+      // Normal table processing logic
+      table_string.append(tables_used->table->s->db.str,
+                          tables_used->table->s->db.length);
+      table_string.append("##");
+      table_string.append(tables_used->table->s->table_name.str,
+                          tables_used->table->s->table_name.length);
+    }
+
+    discard_cache_by_table(table_string);
+  }
+
+  return;
+}
+
+void Fast_query_cache::qc_invalidate(THD *thd_g, Table_ref *tables_used) {
+  DBUG_TRACE;
+
+  if (!use_fast_query_cache()) return;
+
+  for (; tables_used; tables_used = tables_used->next_local) {
+    qc_invalidate_single(thd_g, tables_used);
+  }
+
+  return;
+}
+
+/**
+  Remove all cached queries that use the given table.
+
+  @param thd_g                 Thread handle
+  @param table_used          Table_ref representing the table to be
+                             invalidated.
+*/
+
+void Fast_query_cache::qc_invalidate_single(THD *thd_g [[maybe_unused]],
+                                            Table_ref *table_used) {
+  DBUG_TRACE;
+  if (!use_fast_query_cache()) return;
+
+  if (table_used->is_view_or_derived()) return;
+
+  qc_invalidate_table(table_used->table);
+
+  return;
+}
+
+/**
+ * @brief Invalidates table in fast query cache
+ *
+ * Marks the specified table as invalid in fast query cache, i.e., deletes all
+ * query results in cache related to that table.
+ *
+ * @param thd_g THD pointer, current thread handle (may be unused)
+ * @param table Table object pointer pointing to table to be invalidated
+ */
+void Fast_query_cache::qc_invalidate_table(TABLE *table) {
+  DBUG_TRACE;
+
+  std::string table_string;
+
+  table_string.append(table->s->db.str, table->s->db.length);
+  table_string.append("##");
+  table_string.append(table->s->table_name.str, table->s->table_name.length);
+
+  discard_cache_by_table(table_string);
+
+  return;
+}
+
+/**
+ * @brief Clears entries for specified table from cache
+ *
+ * Removes all entries related to specified table from query cache based on
+ * given database name and table name.
+ *
+ * @param db Database name, represented as LEX_CSTRING type
+ * @param table Table name, represented as LEX_CSTRING type
+ *
+ * @return No return value
+ */
+void Fast_query_cache::qc_invalidate_table(LEX_CSTRING db, LEX_CSTRING table) {
+  DBUG_TRACE;
+
+  std::string table_string;
+
+  table_string.append(db.str, db.length);
+  table_string.append("##");
+  table_string.append(table.str, table.length);
+
+  discard_cache_by_table(table_string);
+
+  return;
+}
+
+/**
+  This is a convenience function used by the innodb plugin.
+*/
+extern "C" void mysql_fast_query_cache_invalidate(char *fullname) {
+  DBUG_TRACE;
+
+  std::string table_string = fullname;
+  size_t pos = 0;
+  while ((pos = table_string.find('/', pos)) != std::string::npos) {
+    table_string.replace(pos, 1, "##");
+    pos += 2;
+  }
+
+  Fast_query_cache::getInstance().discard_cache_by_table(table_string);
+}
+
+/**
+ * @brief Clears data for specific table in cache
+ *
+ * This function is used to clear data for specific table in cache when used in
+ * transactions.
+ *
+ * @param thd_g Thread handle containing all thread information
+ * @param table Table whose cache is to be cleared
+ */
+void Fast_query_cache::qc_invalidate_table(THD *thd_g [[maybe_unused]],
+                                           TABLE *table) {
+  DBUG_TRACE;
+  if (!use_fast_query_cache()) return;
+
+  qc_invalidate_table(table);
+
+  return;
+}
+
+/**
+ * @brief Flushes all query cache
+ *
+ * This function is used to flush all data in currently used fast query cache.
+ *
+ * @note Ensure fast query cache is enabled before calling this function.
+ */
+void Fast_query_cache::flush_all_query_cache() {
+  DBUG_TRACE;
+  if (!use_fast_query_cache()) return;
+
+  std::set<std::string> table_sets;
+  for (auto it = m_table_cache_map.begin(); it != m_table_cache_map.end();
+       ++it) {
+    table_sets.insert(it->first);
+  }
+
+  for (auto table_name : table_sets) {
+    discard_cache_by_table(table_name);
+  }
+
+  m_table_cache_map.clear();
+
+  fast_query_cache_map_size = 0;
+  fast_query_cache_map_memory = 0;
+  fast_query_cache_hits_count = 0;
+}
+
+/**
+ * @brief Flushes tables in query cache
+ *
+ * Traverses given table reference chain and flushes query results for all
+ * tables in cache.
+ *
+ * @param tables Head pointer of table reference chain
+ */
+void Fast_query_cache::flush_query_cache_table(Table_ref *tables) {
+  for (; tables; tables = tables->next_local) {
+    std::string table_string;
+
+    table_string.append(tables->db, tables->db_length);
+    table_string.append("##");
+    table_string.append(tables->table_name, tables->table_name_length);
+
+    discard_cache_by_table(table_string);
+  }
+}
\ No newline at end of file
diff --git a/sql/sql_fast_query_cache.h b/sql/sql_fast_query_cache.h
new file mode 100644
index 0000000..f6c302f
--- /dev/null
+++ b/sql/sql_fast_query_cache.h
@@ -0,0 +1,333 @@
+/* Copyright (c) 2001, 2025, Oracle and/or its affiliates.
+
+   This program is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License, version 2.0,
+   as published by the Free Software Foundation.
+
+   This program is also distributed with certain software (including
+   but not limited to OpenSSL) that is licensed under separate terms,
+   as designated in a particular file or component or in included license
+   documentation.  The authors of MySQL hereby grant you an additional
+   permission to link the program and your derivative works with the
+   separately licensed software that they have included with MySQL.
+
+   This program is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License, version 2.0, for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software Foundation,
+   51 Franklin Street, Suite 500, Boston, MA 02110-1335 USA */
+
+#ifndef _SQL_FAST_QUERY_CACHE_H
+#define _SQL_FAST_QUERY_CACHE_H
+
+#include <cstdarg>
+#include <cstdio>
+#include <list>
+#include <unordered_map>
+#include <vector>
+#include <set>
+
+#include "memory_debugging.h"
+#include "mysql/components/services/log_builtins.h"
+#include "mysqld_error.h"
+#include "sql/mysqld.h"
+#include "mysql/psi/mysql_rwlock.h"
+#include "my_base.h"
+#include "sql/sql_locale.h"
+#include "sql/memory/mp.h"
+#include "sql/sql_lex.h"
+
+typedef ulonglong_typ sql_mode_t;
+typedef std::string query_t;
+typedef std::string db_name_t;
+typedef std::unordered_map<std::string, std::set<std::string>> table_cache_map_t;
+
+class MY_LOCALE;
+
+// Capacity for LRU read cache
+constexpr const uint lru_read_cache_capacity = 16 * 1024;
+// Current memory usage of fast query cache map
+extern ulong fast_query_cache_map_memory;
+// Maximum memory size for fast query cache map
+extern std::atomic<ulonglong_typ> fast_query_cache_map_max_mem_size;
+
+struct Fast_query_cache_node;
+
+
+class LruReadCacheQueue {
+ private:
+  std::atomic<size_t> head_{0};
+  std::atomic<size_t> tail_{0};
+  std::atomic<size_t> count_{0};
+  Fast_query_cache_node *buffer_[lru_read_cache_capacity];
+
+ public:
+  LruReadCacheQueue() = default;
+
+  ~LruReadCacheQueue() {}
+  
+
+  bool enqueue(Fast_query_cache_node *value) {
+    size_t currentTail = tail_.load();
+    size_t nextTail = (currentTail + 1) % lru_read_cache_capacity;
+
+    // Use CAS to update tail_ pointer
+    while (!tail_.compare_exchange_weak(currentTail, nextTail)) {
+      // If CAS fails (meaning tail_ was modified by another thread), 
+      // recalculate nextTail and retry
+      currentTail = tail_.load();
+      nextTail = (currentTail + 1) % lru_read_cache_capacity;
+    }
+
+    // If CAS succeeds, put value into buffer
+    buffer_[currentTail] = value;
+    count_.fetch_add(1);
+    return true;
+  }
+
+  // Dequeue operation is performed within write lock and doesn't need additional thread safety
+  Fast_query_cache_node *dequeue() {
+    // If queue is full, current head might be invalid, need to set to tail node
+    if (count_.load() >= lru_read_cache_capacity) {
+      head_.store(tail_.load());
+      count_.store(lru_read_cache_capacity);
+    }
+
+    size_t currentHead = head_.load();
+
+    if (empty()) return nullptr;
+
+    Fast_query_cache_node *value = buffer_[currentHead];
+    size_t nextHead = (currentHead + 1) % lru_read_cache_capacity;
+
+    head_.store(nextHead);
+
+    count_.fetch_sub(1);
+    return value;
+  }
+
+  bool empty() const { return count_.load() == 0; }
+};
+
+///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
+class Fast_query_cache_rdlock {
+  mysql_rwlock_t *m_lock;
+
+ public:
+  Fast_query_cache_rdlock() {}
+  Fast_query_cache_rdlock(mysql_rwlock_t *lock) : m_lock(lock) {
+    mysql_rwlock_rdlock(m_lock);
+  }
+
+  ~Fast_query_cache_rdlock() { mysql_rwlock_unlock(m_lock); }
+};
+
+class Fast_query_cache_wrlock {
+  mysql_rwlock_t *m_lock;
+
+ public:
+  Fast_query_cache_wrlock() {}
+  Fast_query_cache_wrlock(mysql_rwlock_t *lock) : m_lock(lock) {
+    mysql_rwlock_wrlock(m_lock);
+  }
+
+  ~Fast_query_cache_wrlock() { mysql_rwlock_unlock(m_lock); }
+};
+
+
+struct Fast_query_cache_node {
+  
+  query_t query_hash;          // Format: db##query, prevents same query from different dbs colliding
+  Fast_query_cache_node *prev;
+  Fast_query_cache_node *next;
+  Fast_query_cache_node(const query_t &hash)
+      : query_hash(hash), prev(nullptr), next(nullptr) {}
+  ~Fast_query_cache_node() {
+    prev = nullptr;
+    next = nullptr;
+    query_hash.clear();
+  }
+};
+
+class Result_data {
+ public:
+  uint32 length;
+  uchar_t  *buf;
+
+  Result_data()
+  {
+    length = 0; 
+    buf = nullptr;
+  }
+  ~Result_data()
+  {
+    buf = nullptr;
+    length = 0;
+  }
+};
+
+//
+class Fast_query_cache_result {
+ private:
+  std::atomic<uint32> ref_count;           // Reference count - how many threads are currently accessing this query_result
+  std::list<LEX_STRING> table_list;         // List of tables (db##table) involved in the SQL
+  Fast_query_cache_node *lru_ptr;         // Pointer to LRU-maintained fast_query_cache_node object
+  int64 m_group_id{UINT32_MAX};            // Unique query_id for each query, can get buffer usage via memory_pool.get_group_space(group_id)
+  size_t query_cache_result_max_mem_size;  // Total result + query + table size, large results consume too much buffer
+  
+ public:
+  bool end_of_result {false};
+  ulong current_found_rows{0};
+  ulong current_result_size{0};   // Current buffer allocated for result (Note: different from result_buffer_list, includes header buffer space)
+  std::list<Result_data *> result_buffer_list;  // Cached result buffers
+
+ public:
+  Fast_query_cache_result(std::list<LEX_STRING> &tab_ids, 
+                          int64 group_id = INT32_MAX, 
+                          size_t query_cache_result_max_mem_size = 1024 * 1024)
+                          : ref_count(0), 
+                            table_list(tab_ids), 
+                            lru_ptr(nullptr), 
+                            m_group_id(group_id), 
+                            query_cache_result_max_mem_size(query_cache_result_max_mem_size) 
+                            {
+                              if(query_cache_result_max_mem_size > fast_query_cache_map_max_mem_size.load() / 16)
+                                query_cache_result_max_mem_size = fast_query_cache_map_max_mem_size.load() / 16;
+                            }
+
+  ~Fast_query_cache_result();
+
+  uint32 ref_count_load() { return ref_count.load(); }
+  void ref_count_add_one() { ref_count.fetch_add(1); }
+  void ref_count_sub_one() { ref_count.fetch_sub(1); }
+
+  Fast_query_cache_node *get_lru_ptr() { return lru_ptr; }
+
+  void set_lru_ptr(Fast_query_cache_node *ptr) { lru_ptr = ptr; }
+  std::list<LEX_STRING> &get_table_ids() { return table_list; }
+  int64 get_group_id() { return m_group_id; }
+  size_t get_query_cache_result_max_mem_size() { return query_cache_result_max_mem_size; }
+};
+
+class Fast_query_cache {
+ private:
+  static Fast_query_cache *instance;
+  // static std::once_flag instance_flag;
+
+ public:
+  Fast_query_cache() = default;
+  ~Fast_query_cache() = default;
+
+  static Fast_query_cache &getInstance() {
+    assert(instance != nullptr);
+    return *instance;
+  }
+
+  void initialize() {
+#ifdef HAVE_PSI_INTERFACE
+    static PSI_rwlock_info fast_query_cache_rwlock[] = {
+        {&key_rwlock_LOCK_query_cache, "m_lock_cache", 0, 0, ""}};
+    mysql_rwlock_register("sql", fast_query_cache_rwlock, 1);
+    mysql_rwlock_init(key_rwlock_LOCK_query_cache, &m_lock_cache);
+
+    static PSI_rwlock_info fast_query_table_cache_rwlock[] = {
+        {&key_rwlock_LOCK_table_cache, "m_lock_table", 0, 0, ""}};
+    mysql_rwlock_register("sql", fast_query_table_cache_rwlock, 1);
+    mysql_rwlock_init(key_rwlock_LOCK_table_cache, &m_lock_table);
+#else
+    mysql_rwlock_init(0, &m_lock_cache);
+    mysql_rwlock_init(0, &m_lock_table);
+#endif
+
+    // mysql_mutex_init(key_LOCK_lru, &m_lock_lru, MY_MUTEX_INIT_FAST);
+    memory_pool.init();
+    head = nullptr;
+    tail = nullptr;
+    instance = this;
+    callback_func = nullptr;
+  }
+
+  void cleanup() {
+    mysql_rwlock_destroy(&m_lock_cache);
+    mysql_rwlock_destroy(&m_lock_table);
+    m_table_cache_map.clear();
+    m_large_result_query_map.clear();
+    m_fast_query_cache_map.clear();
+    callback_func = nullptr;
+
+    while (head != nullptr) {
+      auto node = head->next;
+      head->~Fast_query_cache_node();
+      head = node;
+    }
+
+    memory_pool.close();
+  }
+
+  Fast_query_cache(Fast_query_cache const &) = delete;  // Copy construct
+  Fast_query_cache(Fast_query_cache &&) = delete;       // Move construct
+  Fast_query_cache &operator=(Fast_query_cache const &) = delete;  // Copy assign
+  Fast_query_cache &operator=(Fast_query_cache &&) = delete;       // Move assign
+
+ public:
+  mysql_rwlock_t m_lock_cache;  // fast query cache
+  mysql_rwlock_t m_lock_table;  //lock m_table_cache_map
+#ifdef HAVE_PSI_INTERFACE
+  PSI_rwlock_key key_rwlock_LOCK_query_cache;
+  PSI_rwlock_key key_rwlock_LOCK_table_cache;
+#endif /* HAVE_PSI_INTERFACE */
+
+  /* query -> result */
+  std::unordered_map<query_t, Fast_query_cache_result *> m_fast_query_cache_map;
+  /* Mapping from db##table to set of queries, then each query maps to a result */
+  table_cache_map_t m_table_cache_map;      // Format: [db##table_name, digest(db##sql)]
+  // When result set exceeds limit, it won't be cached. To prevent future attempts,
+  // queries are added to this set. Subsequent same SQL won't be pre-cached.
+  // This map also follows invalidation mechanism same as m_table_cache_map.
+  std::unordered_set<std::string> m_large_result_query_map;
+  // LRU head and tail pointers
+  Fast_query_cache_node *head{nullptr};
+  Fast_query_cache_node *tail{nullptr};
+  // LRU access queue
+  LruReadCacheQueue lru_read_cache;
+
+  MemoryPool memory_pool;
+
+  qc_engine_callback callback_func;   // Storage engine callback function, used in RR mode to determine if query result is visible to current transaction
+
+
+ public:
+  bool lru_eliminate();
+  void lru_move_node(Fast_query_cache_node *node);
+  Fast_query_cache_result *get(query_t &query_hash);
+  bool exist(query_t &query_hash);
+  bool insert(query_t &query_hash, Fast_query_cache_result *fast_query_cache_result, size_t result_size);
+  void discard_cache_by_table(query_t &table, const std::string &query);
+
+  MemoryPool *get_mp() { return &memory_pool; }
+
+ private:
+  int is_cacheable(THD *thd_g, LEX *lex, Table_ref *tables_used, uint8 *tables_type);
+  int process_and_count_tables(THD *thd_g, Table_ref *tables_used, uint8 *tables_type);
+  bool ask_handler_allowance(THD *thd_g, Table_ref *tables_used);
+ public:
+  int send_result_to_client(THD *thd_g, const LEX_CSTRING &sql);
+  void store_query(THD *thd_g, Table_ref *tables_used);
+  void end_of_result(THD *thd_g);
+  void abort(THD *thd_g);
+  void qc_invalidate(Table_ref *tables_used);
+  void qc_invalidate(THD *thd_g, Table_ref *tables_used);
+  void qc_invalidate_single(THD *thd_g, Table_ref *table_used);
+  void qc_invalidate_table(TABLE *table);
+  void qc_invalidate_table(LEX_CSTRING db, LEX_CSTRING table);
+  void qc_invalidate_table(THD *thd_g, TABLE *table);
+  void flush_all_query_cache();
+  void flush_query_cache_table(Table_ref *tables);
+};
+
+extern ulong fast_query_cache_map_size ;
+
+#endif
\ No newline at end of file
diff --git a/sql/sql_insert.cc b/sql/sql_insert.cc
index 2f8ebb4..1db60e0 100644
--- a/sql/sql_insert.cc
+++ b/sql/sql_insert.cc
@@ -108,7 +108,7 @@
 #include "sql_string.h"
 #include "template_utils.h"
 #include "thr_lock.h"
-
+#include "sql/sql_fast_query_cache.h"
 namespace dd {
 class Table;
 }  // namespace dd
@@ -685,6 +685,16 @@ bool Sql_cmd_insert_values::execute_inner(THD *thd_g) {
     const bool changed [[maybe_unused]] =
         info.stats.copied || info.stats.deleted || info.stats.updated;
 
+    if(changed && fast_query_cache_enable){
+       /*
+        Invalidate the table in the query cache if something changed.
+        For the transactional algorithm to work the invalidation must be
+        before binlog writing and ha_autocommit_or_rollback
+      */
+      Fast_query_cache::getInstance().qc_invalidate_single(thd_g, lex->insert_table_leaf);
+      DEBUG_SYNC(thd_g, "wait_after_query_cache_invalidate");
+    }
+
     if (!has_error ||
         thd_g->get_transaction()->cannot_safely_rollback(Transaction_ctx::STMT)) {
       if (mysql_bin_log.is_open()) {
@@ -2475,6 +2485,10 @@ bool Query_result_insert::send_eof(THD *thd_g) {
 
   changed = (info.stats.copied || info.stats.deleted || info.stats.updated);
 
+  if (changed && fast_query_cache_enable) {
+      Fast_query_cache::getInstance().qc_invalidate_table(thd_g, table);
+  }
+
   /*
     INSERT ... SELECT on non-transactional table which changes any rows
     must be marked as unsafe to rollback.
@@ -2600,6 +2614,10 @@ void Query_result_insert::abort_result_set(THD *thd_g) {
       zero, so no check for that is made.
     */
     changed = (info.stats.copied || info.stats.deleted || info.stats.updated);
+    if (changed && fast_query_cache_enable) {
+      Fast_query_cache::getInstance().qc_invalidate_table(thd_g, table);
+    }
+
     transactional_table = table->file->has_transactions();
     if (thd_g->get_transaction()->cannot_safely_rollback(Transaction_ctx::STMT)) {
       if (mysql_bin_log.is_open()) {
diff --git a/sql/sql_load.cc b/sql/sql_load.cc
index d71f6eb..342f695 100644
--- a/sql/sql_load.cc
+++ b/sql/sql_load.cc
@@ -91,6 +91,7 @@
 #include "sql/trigger_def.h"
 #include "sql_string.h"
 #include "thr_lock.h"
+#include "sql/sql_fast_query_cache.h"
 
 class READ_INFO;
 
@@ -591,6 +592,13 @@ bool Sql_cmd_load_table::execute_inner(THD *thd_g,
 
   killed_status = error ? thd_g->killed.load() : THD::NOT_KILLED;
 
+  /*
+    We must invalidate the table in query cache before binlog writing and
+    ha_autocommit_...
+  */
+  if (fast_query_cache_enable)
+    Fast_query_cache::getInstance().qc_invalidate_single(thd_g, insert_table_ref);
+
   if (error) {
     if (m_is_local_file) read_info.skip_data_till_eof();
 
diff --git a/sql/sql_parse.cc b/sql/sql_parse.cc
index 2a70cdc..48ce5b6 100644
--- a/sql/sql_parse.cc
+++ b/sql/sql_parse.cc
@@ -183,6 +183,7 @@
 #include "violite.h"
 #include "bvar/bvar.h"
 #include "time_recorder.h"
+#include "sql/sql_fast_query_cache.h"
 
 #ifdef WITH_LOCK_ORDER
 #include "sql/debug_lock_order.h"
@@ -2280,6 +2281,9 @@ bool dispatch_command(THD *thd_g, const COM_DATA *com_data,
         thd_g->update_slow_query_status();
         thd_g->send_statement_status();
 
+        if(fast_query_cache_enable && thd_g->fast_query_cache_store_query)
+          Fast_query_cache::getInstance().end_of_result(thd_g);
+
         const std::string &cn = Command_names::str_global(command);
         mysql_audit_notify(thd_g, AUDIT_EVENT(MYSQL_AUDIT_GENERAL_STATUS),
                            thd_g->get_stmt_da()->is_error()
@@ -2658,6 +2662,9 @@ done:
 
   thd_g->rpl_thd_ctx.session_gtids_ctx().notify_after_response_packet(thd_g);
 
+  if (fast_query_cache_enable && thd_g->fast_query_cache_store_query)
+    Fast_query_cache::getInstance().end_of_result(thd_g);
+
   if (!thd_g->is_error() && !thd_g->killed)
     mysql_audit_notify(thd_g, AUDIT_EVENT(MYSQL_AUDIT_GENERAL_RESULT), 0, nullptr,
                        0);
@@ -4400,6 +4407,14 @@ int mysql_execute_command(THD *thd_g, bool first_level) {
         if (flush_tables_for_export(thd_g, all_tables)) goto error;
         my_ok(thd_g);
         break;
+      } else if(first_table && fast_query_cache_enable && lex->type & REFRESH_QUERY_CACHE) {
+        if (check_table_access(thd_g, LOCK_TABLES_ACL | SELECT_ACL, all_tables,
+                               false, UINT_MAX, false))
+          goto error;
+        
+        Fast_query_cache::getInstance().flush_query_cache_table(all_tables);
+        my_ok(thd_g);
+        break;
       }
 
       /*
@@ -5415,6 +5430,9 @@ void THD::reset_for_next_command() {
   thd_g->derived_tables_processing = false;
   thd_g->parsing_system_view = false;
 
+  thd_g->prepared_stmt_query = false;
+  thd_g->m_qc_stmt_table_list.clear();
+
   // Need explicit setting, else demand all privileges to a table.
   thd_g->want_privilege = ~NO_ACCESS;
 
@@ -5509,6 +5527,29 @@ void dispatch_sql_command(THD *thd_g, Parser_state *parser_state) {
   invoke_pre_parse_rewrite_plugins(thd_g);
   thd_g->m_parser_state = nullptr;
 
+  int result = 0;
+  if(fast_query_cache_enable) {
+    result = Fast_query_cache::getInstance().send_result_to_client(thd_g, thd_g->query());
+  } 
+
+  if (result > 0) {
+    /*
+      Query cache hit. We need to write the general log here if
+      we haven't already logged the statement earlier due to --log-raw.
+      Right now, we only cache SELECT results; if the cache ever
+      becomes more generic, we should also cache the rewritten
+      query-string together with the original query-string (which
+      we'd still use for the matching) when we first execute the
+      query, and then use the obfuscated query-string for logging
+      here when the query is given again.
+    */
+    if (opt_general_log_raw) {
+      query_logger.general_log_write(thd_g, COM_QUERY, thd_g->query().str,
+                                     thd_g->query().length);
+    }
+    return ;
+  }
+
   // we produce digest if it's not explicitly turned off
   // by setting maximum digest length to zero
   if (get_max_digest_length() != 0)
diff --git a/sql/sql_partition_admin.cc b/sql/sql_partition_admin.cc
index 1e6d467..a6f537b 100644
--- a/sql/sql_partition_admin.cc
+++ b/sql/sql_partition_admin.cc
@@ -64,6 +64,7 @@
 #include "sql/transaction.h"  // trans_commit_stmt
 #include "sql_string.h"
 #include "thr_lock.h"
+#include "sql/sql_fast_query_cache.h"
 
 class partition_element;
 
@@ -538,6 +539,9 @@ bool Sql_cmd_alter_table_exchange_partition::exchange_partition(
 
   my_ok(thd_g);
 
+   if (fast_query_cache_enable)
+    Fast_query_cache::getInstance().qc_invalidate(thd_g, table_list);
+
   return false;
 }
 
diff --git a/sql/sql_prepare.cc b/sql/sql_prepare.cc
index 4e869c5..46e852e 100644
--- a/sql/sql_prepare.cc
+++ b/sql/sql_prepare.cc
@@ -178,6 +178,7 @@ When one supplies long data for a placeholder:
 #include "sql/window.h"
 #include "sql_string.h"
 #include "violite.h"
+#include "sql/sql_fast_query_cache.h"
 
 namespace resourcegroups {
 class Resource_group;
@@ -3630,13 +3631,22 @@ bool Prepared_statement::execute(THD *thd_g, String *expanded_query,
   rewrite_query_if_needed(thd_g);
   log_execute_line(thd_g);
 
-  thd_g->binlog_need_explicit_defaults_ts =
+  int result = 0;
+  if(fast_query_cache_enable) {
+    thd_g->prepared_stmt_query = true;
+    result = Fast_query_cache::getInstance().send_result_to_client(thd_g, thd_g->query());
+  }
+
+  if (result <= 0) {
+    thd_g->binlog_need_explicit_defaults_ts =
       m_lex->binlog_need_explicit_defaults_ts;
-  resource_group_switched = mgr_ptr->switch_resource_group_if_needed(
+      
+    resource_group_switched = mgr_ptr->switch_resource_group_if_needed(
       thd_g, &src_res_grp, &dest_res_grp, &ticket, &cur_ticket);
-
-  status = mysql_execute_command(thd_g, true);
-  if (status) return true;
+    
+    status = mysql_execute_command(thd_g, true);
+    if (status) return true;
+  }
 
   if (open_cursor) {
     if (m_cursor == nullptr && m_cursor_result != nullptr) {
diff --git a/sql/sql_reload.cc b/sql/sql_reload.cc
index 684eea9..752ca5a 100644
--- a/sql/sql_reload.cc
+++ b/sql/sql_reload.cc
@@ -55,6 +55,7 @@
 #include "sql/sql_servers.h"  // servers_reload
 #include "sql/system_variables.h"
 #include "sql/table.h"
+#include "sql/sql_fast_query_cache.h"
 
 /**
   Check the privileges required to execute a FLUSH command
@@ -194,6 +195,12 @@ bool handle_reload_request(THD *thd_g, unsigned long options, TABLE_LIST *tables
     options |= REFRESH_ERROR_LOG;
   }
 
+  if(options & REFRESH_QUERY_CACHE) {
+    /*
+     Flush fast query cache*/
+     Fast_query_cache::getInstance().flush_all_query_cache();
+  }
+
   if (options & REFRESH_ERROR_LOG) {
     if (reopen_error_log()) result = true;
   }
diff --git a/sql/sql_select.cc b/sql/sql_select.cc
index 84ca709..d6eba38 100644
--- a/sql/sql_select.cc
+++ b/sql/sql_select.cc
@@ -127,6 +127,7 @@
 #include "sql_string.h"
 #include "template_utils.h"
 #include "thr_lock.h"
+#include "sql/sql_fast_query_cache.h"
 
 using std::max;
 using std::min;
@@ -673,6 +674,17 @@ bool Sql_cmd_dml::execute(THD *thd_g) {
   */
   if (!is_empty_query()) {
     if (lock_tables(thd_g, lex->query_tables, lex->table_count, 0)) goto err_p;
+
+     /*
+      Register query result in cache.
+      Tables must be locked before storing the query in the query cache.
+      Transactional engines must be signalled that the statement has started,
+      by calling external_lock().
+    */
+   if(lex->query_tables && fast_query_cache_enable && 
+      lex->sql_command == SQLCOM_SELECT) {
+      Fast_query_cache::getInstance().store_query(thd_g, lex->query_tables);
+   }
   }
 
   /* Evaluate snapshot expressions */
diff --git a/sql/sql_update.cc b/sql/sql_update.cc
index 2447dbd..6bebce2 100644
--- a/sql/sql_update.cc
+++ b/sql/sql_update.cc
@@ -117,6 +117,7 @@
 #include "sql/visible_fields.h"
 #include "template_utils.h"
 #include "thr_lock.h"
+#include "sql/sql_fast_query_cache.h"
 
 class COND_EQUAL;
 
@@ -1125,6 +1126,14 @@ bool Sql_cmd_update::update_single_table(THD *thd_g) {
   iterator.reset();
 
   /*
+    Invalidate the table in the query cache if something changed.
+    This must be before binlog writing and ha_autocommit_...
+  */
+  if(updated_rows && fast_query_cache_enable) {
+    Fast_query_cache::getInstance().qc_invalidate_single(thd_g, table_list->updatable_base_table());
+  }
+
+  /*
     error < 0 means really no error at all: we processed all rows until the
     last one without error. error > 0 means an error (e.g. unique key
     violation and no IGNORE or REPLACE). error == 0 is also an error (if
diff --git a/sql/sql_view.cc b/sql/sql_view.cc
index 0cb3be0..df7d2b5 100644
--- a/sql/sql_view.cc
+++ b/sql/sql_view.cc
@@ -85,6 +85,7 @@
 #include "sql/transaction.h"
 #include "sql_string.h"
 #include "thr_lock.h"
+#include "sql/sql_fast_query_cache.h"
 
 namespace dd {
 class Schema;
@@ -1884,6 +1885,16 @@ bool mysql_drop_view(THD *thd_g, TABLE_LIST *views) {
     return true;
   }
 
+  /*
+    Invalidate the table in the query cache if something changed.
+    This must be before binlog writing and ha_autocommit_...
+  */
+  if(fast_query_cache_enable) {
+    for (Table_ref *view = views; view; view = view->next_local) {
+      Fast_query_cache::getInstance().qc_invalidate_single(thd_g, view);
+    }
+  }
+
   if (trans_commit_stmt(thd_g) || trans_commit(thd_g)) return true;
 
   my_ok(thd_g);
diff --git a/sql/sql_yacc.yy b/sql/sql_yacc.yy
index 86dce47..beb0e00 100644
--- a/sql/sql_yacc.yy
+++ b/sql/sql_yacc.yy
@@ -1388,8 +1388,10 @@ void warn_about_deprecated_binary(THD *thd_g)
 %token<lexer.keyword> MAX_REPLACE_TIME_SYM     1206   /* MYSQL */
 %token<lexer.keyword> IP_WHITELIST_SYM           1207   /* MYSQL */
 
+
 //gaiadb token as follows:
 %token<lexer.keyword> REVERSE_WORDS_SYM 1208
+%token<lexer.keyword> QC_SYM                    1209   /*GAIADB*/
 
 /*
   Precedence rules used to resolve the ambiguity when using keywords as idents
@@ -14254,6 +14256,13 @@ flush_options:
               MYSQL_YYABORT;
           }
           opt_flush_lock {}
+        | QC_SYM TABLE_SYM  opt_table_list
+          {
+            Lex->type|= REFRESH_QUERY_CACHE;
+            if (Select->add_tables(YYTHD, $3, TL_OPTION_UPDATING,
+                                   YYPS->m_lock_type, YYPS->m_mdl_type))
+              MYSQL_YYABORT;
+          }  
         | flush_options_list
         ;
 
@@ -14328,6 +14337,8 @@ flush_option:
           { Lex->type|= REFRESH_USER_RESOURCES; }
         | OPTIMIZER_COSTS_SYM
           { Lex->type|= REFRESH_OPTIMIZER_COSTS; }
+        | QC_SYM  TABLES
+          { Lex->type|= REFRESH_QUERY_CACHE;}
         ;
 
 opt_table_list:
@@ -15615,6 +15626,7 @@ ident_keywords_unambiguous:
         | PROCESSLIST_SYM
         | PROFILES_SYM
         | PROFILE_SYM
+        | QC_SYM
         | QUARTER_SYM
         | QUERY_SYM
         | QUICK
diff --git a/sql/sys_vars.cc b/sql/sys_vars.cc
index ff6ea05..c21d945 100644
--- a/sql/sys_vars.cc
+++ b/sql/sys_vars.cc
@@ -142,6 +142,7 @@
 #include "sql/sql_parallel.h"
 #include <base/logging.h>
 #include "storage/perfschema/pfs_gaia_mdl_logs.h"
+#include "sql/sql_fast_query_cache.h"
 
 #ifdef _WIN32
 #include "sql/named_pipe.h"
@@ -170,6 +171,11 @@ static constexpr const unsigned long SCHEMA_DEF_CACHE_DEFAULT{256};
 static constexpr const unsigned long STORED_PROGRAM_DEF_CACHE_DEFAULT{256};
 static constexpr const unsigned long TABLESPACE_DEF_CACHE_DEFAULT{256};
 
+static constexpr const uint64_t KiB{1024};
+static constexpr const uint64_t MiB{1024 * KiB};
+static constexpr const uint64_t GiB{1024 * MiB};
+static constexpr const uint64_t TiB{1024 * GiB};
+
 /**
   We must have room for at least 400 table definitions in the table
   cache, since otherwise there is no chance prepared
@@ -2417,6 +2423,24 @@ static Sys_var_ulong Sys_interactive_timeout(
     SESSION_VAR(net_interactive_timeout), CMD_LINE(REQUIRED_ARG),
     VALID_RANGE(1, LONG_TIMEOUT), DEFAULT(NET_WAIT_TIMEOUT), BLOCK_SIZE(1));
 
+static Sys_var_bool Sys_enable_fast_query_cache(
+    "fast_query_cache_enable",
+    "Indicate whether to turn on the fast query cache switch or not",
+    GLOBAL_VAR(fast_query_cache_enable), CMD_LINE(OPT_ARG), DEFAULT(false));
+
+static Sys_var_ulong Sys_fast_query_cache_max_mem_size(
+    "fast_query_cache_single_mem_threshold", 
+    "Single fast query cache memory threshold",
+    READ_ONLY NON_PERSIST GLOBAL_VAR(fast_query_cache_single_max_memory), CMD_LINE(OPT_ARG),
+    VALID_RANGE(16 * KiB, 16 * MiB), DEFAULT(MiB),
+    BLOCK_SIZE(KiB));
+
+static Sys_var_ulong Sys_fast_query_cache_map_max_mem_size(
+    "fast_query_cache_total_mem_threshold", "Fast query cache usage total memory threshold",
+    READ_ONLY NON_PERSIST GLOBAL_VAR(fast_query_cache_map_max_mem_size), CMD_LINE(OPT_ARG),
+    VALID_RANGE(16 * MiB, 2 * 1024 * MiB), DEFAULT(256 * MiB),
+    BLOCK_SIZE(KiB));
+
 static Sys_var_ulong Sys_join_buffer_size(
     "join_buffer_size", "The size of the buffer that is used for full joins",
     HINT_UPDATEABLE SESSION_VAR(join_buff_size), CMD_LINE(REQUIRED_ARG),
diff --git a/sql/system_variables.h b/sql/system_variables.h
index 47d5811..25ab869 100644
--- a/sql/system_variables.h
+++ b/sql/system_variables.h
@@ -270,13 +270,14 @@ struct System_variables {
   ulong_typ transaction_isolation;
   ulong_typ updatable_views_with_limit;
   uint max_user_connections;
-  ulong_typ my_aes_mode;
-  ulong_typ ssl_fips_mode;
-  ulong_typ user_field_max_execution_time;
-  ulong_typ user_field_max_insert_time;
-  ulong_typ user_field_max_update_time;
-  ulong_typ user_field_max_delete_time;
-  ulong_typ user_field_max_replace_time;
+  ulong my_aes_mode;
+  ulong ssl_fips_mode;
+  ulong user_field_max_execution_time;
+  ulong user_field_max_insert_time;
+  ulong user_field_max_update_time;
+  ulong user_field_max_delete_time;
+  ulong user_field_max_replace_time;
+  bool fast_query_cache;
   /**
     Controls what resultset metadata will be sent to the client.
     @sa enum_resultset_metadata
diff --git a/sql/table.h b/sql/table.h
index 08c57c7..4372134 100644
--- a/sql/table.h
+++ b/sql/table.h
@@ -3682,6 +3682,11 @@ struct TABLE_LIST {
  public:
   GRANT_INFO grant;
 
+  /* data needed by some engines in query cache */
+  ulonglong_typ engine_data;
+  /* callback function for asking handler about caching in query cache */
+  qc_engine_callback callback_func{0};
+  
  public:
   /// True if right argument of LEFT JOIN; false in other cases (i.e. if left
   /// argument of LEFT JOIN, if argument of INNER JOIN; RIGHT JOINs are
diff --git a/storage/innobase/gaiadb/include/rpl0conductor.h b/storage/innobase/gaiadb/include/rpl0conductor.h
index a655988..33f63f4 100644
--- a/storage/innobase/gaiadb/include/rpl0conductor.h
+++ b/storage/innobase/gaiadb/include/rpl0conductor.h
@@ -142,6 +142,8 @@ class RplConductor {
   void set_trx_sys_max_id_inited();
   bool trx_sys_max_id_inited();
 
+  void rpl_invadite_table(char* table_name);
+
   byte *log_buf();
   std::mutex& log_buf_mutex();
   size_t log_buf_size();
diff --git a/storage/innobase/gaiadb/rpl/rpl0conductor.cc b/storage/innobase/gaiadb/rpl/rpl0conductor.cc
index e2ecc6b..585a23e 100644
--- a/storage/innobase/gaiadb/rpl/rpl0conductor.cc
+++ b/storage/innobase/gaiadb/rpl/rpl0conductor.cc
@@ -37,6 +37,8 @@ this program; if not, write to the Free Software Foundation, Inc.,
 #include "rpl0rpl.h"
 #include "sql/current_thd.h"
 #include "log0log.h"
+#include "baidu/rpc/reloadable_flags.h"
+#include "ha_prototypes.h"
 
 namespace rpl {
 
@@ -587,4 +589,8 @@ void RplConductor::recycle_assembly_before_lsn(lsn_t lsn) {
   assembly_bus_.recycle_before_lsn(lsn);
 }
 
+void RplConductor::rpl_invadite_table(char* table_name) {
+  innobase_invalidate_query_cache(table_name);
+}
+
 }  // namespace rpl
diff --git a/storage/innobase/gaiadb/rpl/rpl0parse.cc b/storage/innobase/gaiadb/rpl/rpl0parse.cc
index 7249536..eddc94b 100644
--- a/storage/innobase/gaiadb/rpl/rpl0parse.cc
+++ b/storage/innobase/gaiadb/rpl/rpl0parse.cc
@@ -290,6 +290,7 @@ inline uint64_t parse_one_log_rec(byte *ptr, byte *end_ptr, mlog_id_t *type,
            raw_type == MLOG_GAIA_STATS_UPDATE ||
            raw_type == MLOG_GAIA_BINLOG_DATA ||
            raw_type == MLOG_GAIA_FILE_DELETE_LSN ||
+           raw_type == MLOG_GAIA_QC_INVIDATE_OP ||
            raw_type == MLOG_GAIA_IBUF_MERGE_ENTRY_COUNTER ||
            raw_type == MLOG_GAIA_COLUMNAR_OPERATOR);
       body_start = innodb::Parser::mlog_parse_initial_log_record(
@@ -592,6 +593,19 @@ bool RplParser::parse_one_mtr(byte *start_ptr, byte *end_ptr, uint64_t mtr_len,
         assembly->update_last_rpl_time(time);
         break;
       }
+      case MLOG_GAIA_QC_INVIDATE_OP: {
+        int table_length = mach_read_from_4(body);
+        body += 4;
+
+        char table_name[1024];
+        memcpy(table_name, body, table_length);
+        table_name[table_length] = '\0';
+
+        ib::system() << "Parse MLOG_GAIA_QC_INVIDATE_OP  table_length:" << table_length << "  table_name:" << table_name;
+
+        g_rpl_conductor->rpl_invadite_table(table_name);
+        break;
+      }
       case MLOG_GAIA_RPL_FENCE: {
         if (srv_gaia_secondary_ignore_fence_log) {
           break;
@@ -741,6 +755,7 @@ inline bool RplParser::is_page_mod_record_type(mlog_id_t type) {
           type != MLOG_GAIA_RPL_FENCE && type != MLOG_GAIA_STATS_UPDATE &&
           type != MLOG_GAIA_BINLOG_DATA &&
           type != MLOG_GAIA_FILE_DELETE_LSN &&
+          type != MLOG_GAIA_QC_INVIDATE_OP &&
           type != MLOG_GAIA_IBUF_MERGE_ENTRY_COUNTER &&
           type != MLOG_GAIA_COLUMNAR_OPERATOR);
 }
diff --git a/storage/innobase/handler/ha_innodb.cc b/storage/innobase/handler/ha_innodb.cc
index 5bdd2e8..77f3c61 100644
--- a/storage/innobase/handler/ha_innodb.cc
+++ b/storage/innobase/handler/ha_innodb.cc
@@ -1901,6 +1901,11 @@ static int innobase_alter_tablespace(handlerton *hton, THD *thd_g,
 */
 static const char *innobase_get_tablespace_filename_ext();
 
+/** Invalidate table name and write redo.
+@param[in]      thd             Connection
+@param[in]      name            table name */
+static void innobase_qc_invalidate_table(THD *thd, const char *name);
+
 /** Free tablespace resources. */
 static void innodb_space_shutdown() {
   DBUG_TRACE;
@@ -5577,6 +5582,8 @@ static int innodb_init(void *p) {
   innobase_hton->panic = innodb_shutdown;
   innobase_hton->partition_flags = innobase_partition_flags;
 
+  innobase_hton->qc_invalidate_table = innobase_qc_invalidate_table;
+
   innobase_hton->start_consistent_snapshot =
       innobase_start_trx_and_assign_read_view;
 
@@ -6869,6 +6876,17 @@ uint ha_innobase::max_supported_key_length() const {
   }
 }
 
+/**
+Determines if table caching is supported.
+@return HA_CACHE_TBL_ASKTRANSACT */
+
+uint8
+ha_innobase::table_cache_type()
+/*===========================*/
+{
+	return(HA_CACHE_TBL_ASKTRANSACT);
+}
+
 /** Determines if the primary key is clustered index.
  @return true */
 
@@ -7489,6 +7507,24 @@ static dict_index_t *innobase_index_lookup(
   return (share->idx_trans_tbl.index_mapping[keynr]);
 }
 
+
+void innobase_qc_invalidate_table(THD *thd, const char *name) {
+  dict_table_t *table = nullptr;
+
+  table = dd_table_open_on_name_in_mem(name, false);
+
+  if(table == nullptr) {
+    table = dd_table_open_on_name(thd, nullptr, name , false, DICT_ERR_IGNORE_NONE);
+  }
+
+  if(table != nullptr && !table->is_system_table) {
+
+    write_single_invalidate_qc_redo(table->name.m_name);
+
+    dd_table_close(table, thd, nullptr, false);
+  }
+}
+
 /** Set the autoinc column max value. This should only be called from
 ha_innobase::open, therefore there's no need for a covering lock. */
 void ha_innobase::innobase_initialize_autoinc() {
@@ -25170,3 +25206,190 @@ static bool innobase_notify_async_active_trx() {
   a_rb_trx->cond_n_wait_original();
   return true;
 }
+
+
+/*	BACKGROUND INFO: HOW THE MYSQL QUERY CACHE WORKS WITH INNODB
+	------------------------------------------------------------
+
+1) The use of the query cache for TBL is disabled when there is an
+uncommitted change to TBL.
+
+2) When a change to TBL commits, InnoDB stores the current value of
+its global trx id counter, let us denote it by INV_TRX_ID, to the table object
+in the InnoDB data dictionary, and does only allow such transactions whose
+id <= INV_TRX_ID to use the query cache.
+
+3) When InnoDB does an INSERT/DELETE/UPDATE to a table TBL, or an implicit
+modification because an ON DELETE CASCADE, we invalidate the MySQL query cache
+of TBL immediately.
+
+How this is implemented inside InnoDB:
+
+1) Since every modification always sets an IX type table lock on the InnoDB
+table, it is easy to check if there can be uncommitted modifications for a
+table: just check if there are locks in the lock list of the table.
+
+2) When a transaction inside InnoDB commits, it reads the global trx id
+counter and stores the value INV_TRX_ID to the tables on which it had a lock.
+
+3) If there is an implicit table change from ON DELETE CASCADE or SET NULL,
+InnoDB calls an invalidate method for the MySQL query cache for that table.
+
+How this is implemented inside sql_cache.cc:
+
+1) The query cache for an InnoDB table TBL is invalidated immediately at an
+INSERT/UPDATE/DELETE, just like in the case of MyISAM. No need to delay
+invalidation to the transaction commit.
+
+2) To store or retrieve a value from the query cache of an InnoDB table TBL,
+any query must first ask InnoDB's permission. We must pass the thd as a
+parameter because InnoDB will look at the trx id, if any, associated with
+that thd. Also the full_name which is used as key to search for the table
+object. The full_name is a string containing the normalized path to the
+table in the canonical format.
+
+3) Use of the query cache for InnoDB tables is now allowed also when
+AUTOCOMMIT==0 or we are inside BEGIN ... COMMIT. Thus transactions no longer
+put restrictions on the use of the query cache.
+*/
+
+/******************************************************************//**
+The MySQL query cache uses this to check from InnoDB if the query cache at
+the moment is allowed to operate on an InnoDB table. The SQL query must
+be a non-locking SELECT.
+
+The query cache is allowed to operate on certain query only if this function
+returns TRUE for all tables in the query.
+
+If thd is not in the autocommit state, this function also starts a new
+transaction for thd if there is no active trx yet, and assigns a consistent
+read view to it if there is no read view yet.
+
+Why a deadlock of threads is not possible: the query cache calls this function
+at the start of a SELECT processing. Then the calling thread cannot be
+holding any InnoDB semaphores. The calling thread is holding the
+query cache mutex, and this function will reserve the InnoDB trx_sys->mutex.
+Thus, the 'rank' in sync0mutex.h of the MySQL query cache mutex is above
+the InnoDB trx_sys->mutex.
+@return TRUE if permitted, FALSE if not; note that the value FALSE
+does not mean we should invalidate the query cache: invalidation is
+called explicitly */
+static bool
+innobase_query_caching_of_table_permitted(
+/*======================================*/
+	THD*	thd,		/*!< in: thd of the user who is trying to
+				store a result to the query cache or
+				retrieve it */
+	char*	full_name,	/*!< in: normalized path to the table */
+	uint	full_name_len,	/*!< in: length of the normalized path
+				to the table */
+	ulonglong_typ *unused)	/*!< unused for this engine */
+{
+	bool	is_autocommit;
+	char	norm_name[1000];
+	trx_t*	trx = check_trx_exists(thd);
+
+	ut_a(full_name_len < 999);
+
+	if (trx->isolation_level == TRX_ISO_SERIALIZABLE) {
+		/* In the SERIALIZABLE mode we add LOCK IN SHARE MODE to every
+		plain SELECT if AUTOCOMMIT is not on. */
+
+		return(static_cast<bool>(false));
+	}
+
+	if (trx->has_search_latch) {
+		sql_print_error("The calling thread is holding the adaptive"
+				" search, latch though calling"
+				" innobase_query_caching_of_table_permitted.");
+		trx_print(stderr, trx, 1024);
+	}
+
+	innobase_srv_conc_force_exit_innodb(trx);
+
+	if (!thd_test_options(thd, OPTION_NOT_AUTOCOMMIT | OPTION_BEGIN)) {
+
+		is_autocommit = true;
+	} else {
+		is_autocommit = false;
+
+	}
+
+	if (is_autocommit && trx->n_mysql_tables_in_use == 0) {
+		/* We are going to retrieve the query result from the query
+		cache. This cannot be a store operation to the query cache
+		because then MySQL would have locks on tables already.
+
+		TODO: if the user has used LOCK TABLES to lock the table,
+		then we open a transaction in the call of row_.. below.
+		That trx can stay open until UNLOCK TABLES. The same problem
+		exists even if we do not use the query cache. MySQL should be
+		modified so that it ALWAYS calls some cleanup function when
+		the processing of a query ends!
+
+		We can imagine we instantaneously serialize this consistent
+		read trx to the current trx id counter. If trx2 would have
+		changed the tables of a query result stored in the cache, and
+		trx2 would have already committed, making the result obsolete,
+		then trx2 would have already invalidated the cache. Thus we
+		can trust the result in the cache is ok for this query. */
+
+		return((bool)true);
+	}
+
+	/* Normalize the table name to InnoDB format */
+	normalize_table_name(norm_name, full_name);
+
+	innobase_register_trx(innodb_hton_ptr, thd, trx);
+
+	if (row_search_check_if_query_cache_permitted(trx, norm_name)) {
+
+		return(static_cast<bool>(true));
+	}
+
+	return(static_cast<bool>(false));
+}
+
+/*****************************************************************//**
+Invalidates the MySQL query cache for the table. */
+void innobase_invalidate_query_cache(char* table_name) {
+  /*auto table = dd_table_open_on_id(table_id, nullptr, nullptr, false, true);
+
+  if (table == nullptr) {
+    return ;
+  }
+  ib::system(ER_IB_MSG_1208) << " qc invalidate table_id: " << table_id << "table_name: " << table->name.m_name;
+  */
+  mysql_fast_query_cache_invalidate(table_name);
+
+  //dd_table_close(table, nullptr, nullptr, false);
+  return ;
+}
+
+/*******************************************************************//**
+Ask InnoDB if a query to a table can be cached.
+@return TRUE if query caching of the table is permitted */
+
+bool
+ha_innobase::register_query_cache_table(
+/*====================================*/
+	THD*		thd,		/*!< in: user thread handle */
+	char*		table_key,	/*!< in: normalized path to the
+					table */
+	size_t		key_length,	/*!< in: length of the normalized
+					path to the table */
+	qc_engine_callback*
+			call_back,	/*!< out: pointer to function for
+					checking if query caching
+					is permitted */
+	ulonglong_typ	*engine_data)	/*!< in/out: data to call_back */
+{
+	*engine_data = 0;
+
+	*call_back = innobase_query_caching_of_table_permitted;
+
+	return(innobase_query_caching_of_table_permitted(
+			thd, table_key,
+			static_cast<uint>(key_length),
+			engine_data));
+}
diff --git a/storage/innobase/handler/ha_innodb.h b/storage/innobase/handler/ha_innodb.h
index 49806ad..71d3759 100644
--- a/storage/innobase/handler/ha_innodb.h
+++ b/storage/innobase/handler/ha_innodb.h
@@ -341,6 +341,18 @@ class ha_innobase : public handler {
 
   bool get_foreign_dup_key(char *, uint, char *, uint) override;
 
+  uint8 table_cache_type();
+
+  /**
+	Ask handler about permission to cache table during query registration
+	*/
+	bool register_query_cache_table(
+		THD*			thd,
+		char*			table_key,
+		size_t			key_length,
+		qc_engine_callback*	call_back,
+		ulonglong_typ*		engine_data);
+
   bool primary_key_is_clustered() const override;
 
   int cmp_ref(const uchar_t *ref1, const uchar_t *ref2) const override;
diff --git a/storage/innobase/handler/ha_innopart.cc b/storage/innobase/handler/ha_innopart.cc
index 71220c5..6f8bc33 100644
--- a/storage/innobase/handler/ha_innopart.cc
+++ b/storage/innobase/handler/ha_innopart.cc
@@ -3186,6 +3186,22 @@ int ha_innopart::truncate_partition_low(dd::Table *dd_table) {
   return error;
 }
 
+/*
+  Is table cache supported
+
+  SYNOPSIS
+    table_cache_type()
+
+*/
+
+uint8 ha_innopart::table_cache_type()
+{
+  DBUG_TRACE;
+
+  return (HA_CACHE_TBL_ASKTRANSACT);
+}
+
+
 /** Total number of rows in all used partitions.
 Returns the exact number of records that this client can see using this
 handler object.
diff --git a/storage/innobase/handler/ha_innopart.h b/storage/innobase/handler/ha_innopart.h
index baf1ea5..3d4af70 100644
--- a/storage/innobase/handler/ha_innopart.h
+++ b/storage/innobase/handler/ha_innopart.h
@@ -232,6 +232,29 @@ class ha_innopart : public ha_innobase,
   @retval       Pointer to clone or NULL if error. */
   handler *clone(const char *name, MEM_ROOT *mem_root) override;
 
+  /** Check and register a table in the query cache.
+	Ask InnoDB if a query to a table can be cached.
+	@param[in]	thd		User thread handle.
+	@param[in]	table_key	Normalized path to the table.
+	@param[in]	key_length	Lenght of table_key.
+	@param[out]	call_back	Function pointer for checking if data
+	has changed.
+	@param[in,out]	engine_data	Data for call_back (not used).
+	@return TRUE if query caching of the table is permitted. */
+	bool register_query_cache_table(
+		THD*			thd,
+		char*			table_key,
+		size_t			key_length,
+		qc_engine_callback*	call_back[[maybe_unused]],
+		ulonglong_typ*		engine_data)
+	{
+		/* Currently this would need to go through every
+		[sub] partition in the table to see if any of them has changed.
+		See row_search_check_if_query_cache_permitted().
+		So disabled until we can avoid check all partitions. */
+		return(true);
+	}
+
   /** \defgroup ONLINE_ALTER_TABLE_INTERFACE On-line ALTER TABLE interface
   @see handler0alter.cc
   @{ */
@@ -416,6 +439,12 @@ class ha_innopart : public ha_innobase,
 
   ha_rows estimate_rows_upper_bound() override;
 
+  /*
+    table_cache_type is implemented by the underlying handler but all
+    underlying handlers must have the same implementation for it to work.
+  */
+  virtual uint8 table_cache_type();
+
   uint alter_table_flags(uint flags);
 
   void update_create_info(HA_CREATE_INFO *create_info) override;
diff --git a/storage/innobase/include/dict0mem.h b/storage/innobase/include/dict0mem.h
index b9e1e86..121bbbc 100644
--- a/storage/innobase/include/dict0mem.h
+++ b/storage/innobase/include/dict0mem.h
@@ -2143,6 +2143,12 @@ struct dict_table_t {
   foreign key checks running on it. */
   std::atomic<ulint> n_foreign_key_checks_running;
 
+  /** Transactions whose view low limit is greater than this number are
+	not allowed to store to the MySQL query cache or retrieve from it.
+	When a trx with undo logs commits, it sets this to the value of the
+	current time. */
+	trx_id_t				query_cache_inv_id;
+
   /** Transaction id that last touched the table definition. Either when
   loading the definition or CREATE TABLE, or ALTER TABLE (prepare,
   commit, and rollback phases). */
diff --git a/storage/innobase/include/ha_prototypes.h b/storage/innobase/include/ha_prototypes.h
index 2060d3b..d8ada67 100644
--- a/storage/innobase/include/ha_prototypes.h
+++ b/storage/innobase/include/ha_prototypes.h
@@ -162,6 +162,13 @@ const char *innobase_basename(const char *path_name);
  @return true if thd_g is executing SELECT */
 bool thd_is_query_block(const THD *thd_g); /*!< in: thread handle */
 
+/*****************************************************************//**
+Invalidates the MySQL query cache for the table. */
+/** interface for gaiadb rpl */
+void innobase_invalidate_query_cache(char *table_name);
+
+void write_single_invalidate_qc_redo(char *table_name);
+
 /** Makes all characters in a NUL-terminated UTF-8 string lower case. */
 void innobase_casedn_str(char *a); /*!< in/out: string to put in lower case */
 
diff --git a/storage/innobase/include/mtr0log.ic b/storage/innobase/include/mtr0log.ic
index fd36bd0..7f5e169 100644
--- a/storage/innobase/include/mtr0log.ic
+++ b/storage/innobase/include/mtr0log.ic
@@ -83,6 +83,7 @@ static inline bool mtr_page_modification_type(mlog_id_t type) {
     case MLOG_GAIA_ALLOC_EXTENT:
     case MLOG_FILE_DELETE:
     case MLOG_GAIA_FILE_DELETE_LSN:
+    case MLOG_GAIA_QC_INVIDATE_OP:
     case MLOG_GAIA_BINLOG_DATA:
     case MLOG_GAIA_INDEX_LOCK:
     case MLOG_GAIA_RPL_FENCE:
diff --git a/storage/innobase/include/mtr0types.h b/storage/innobase/include/mtr0types.h
index 379efdd..fefab7c 100644
--- a/storage/innobase/include/mtr0types.h
+++ b/storage/innobase/include/mtr0types.h
@@ -279,6 +279,8 @@ enum mlog_id_t {
   MLOG_LIST_START_DELETE = 88,
 
   /** log type in gaiadb */
+  MLOG_GAIA_QC_INVIDATE_OP = 114,
+
   MLOG_GAIA_IBUF_MERGE_ENTRY_COUNTER = 115,
   MLOG_GAIA_COLUMNAR_OPERATOR = 116,
   /** transaction begin */
diff --git a/storage/innobase/include/row0sel.h b/storage/innobase/include/row0sel.h
index ccf7f4f..26e22f8 100644
--- a/storage/innobase/include/row0sel.h
+++ b/storage/innobase/include/row0sel.h
@@ -190,6 +190,19 @@ The cursor is an iterator over the table/index.
                                          row_prebuilt_t *prebuilt,
                                          ulint match_mode, ulint direction);
 
+/*******************************************************************//**
+Checks if MySQL at the moment is allowed for this table to retrieve a
+consistent read result, or store it to the query cache.
+@return TRUE if storing or retrieving from the query cache is permitted */
+bool
+row_search_check_if_query_cache_permitted(
+/*======================================*/
+	trx_t*		trx,		/*!< in: transaction object */
+	const char*	norm_name);	/*!< in: concatenation of database name,
+					'/' char, table name */
+
+
+
 /** Searches for rows in the database using cursor.
 Function is mainly used for tables that are shared accorss connection and
 so it employs technique that can help re-construct the rows that
diff --git a/storage/innobase/lock/lock0lock.cc b/storage/innobase/lock/lock0lock.cc
index 759c4c9..5957443 100644
--- a/storage/innobase/lock/lock0lock.cc
+++ b/storage/innobase/lock/lock0lock.cc
@@ -4336,6 +4336,20 @@ namespace locksys {
       if (lock_get_type_low(lock) == LOCK_REC) {
         lock_rec_dequeue_from_page(lock);
       } else {
+        dict_table_t*	table;
+        table = lock->tab_lock.table;
+
+        if (lock_get_mode(lock) != LOCK_IS
+			    && trx->undo_no != 0) {
+
+				  /* The trx may have modified the table. We
+				  block the use of the MySQL query cache for
+				  all currently active transactions. */
+
+				  table->query_cache_inv_id = trx_sys_get_next_trx_id_or_no();
+          //table->query_cache_inv_id = trx_sys_get_committed_version();
+			  }
+
         lock_table_dequeue(lock);
       }
     });
diff --git a/storage/innobase/log/log0recv.cc b/storage/innobase/log/log0recv.cc
index bc49001..8863f0e 100644
--- a/storage/innobase/log/log0recv.cc
+++ b/storage/innobase/log/log0recv.cc
@@ -4747,6 +4747,9 @@ const char *get_mlog_string(mlog_id_t type) {
     case MLOG_GAIA_FILE_DELETE_LSN:
       return ("MLOG_GAIA_FILE_DELETE_LSN");
 
+    case MLOG_GAIA_QC_INVIDATE_OP:
+      return ("MLOG_GAIA_QC_INVIDATE_OP");
+
     case MLOG_GAIA_IBUF_MERGE_ENTRY_COUNTER:
       return ("MLOG_GAIA_IBUF_MERGE_ENTRY_COUNTER");
 
diff --git a/storage/innobase/row/row0sel.cc b/storage/innobase/row/row0sel.cc
index 84bc05d..b7fde3c 100644
--- a/storage/innobase/row/row0sel.cc
+++ b/storage/innobase/row/row0sel.cc
@@ -6176,6 +6176,64 @@ loop:
   goto loop;
 }
 
+/*******************************************************************//**
+Checks if MySQL at the moment is allowed for this table to retrieve a
+consistent read result, or store it to the query cache.
+@return TRUE if storing or retrieving from the query cache is permitted */
+bool
+row_search_check_if_query_cache_permitted(
+/*======================================*/
+	trx_t*		trx,		/*!< in: transaction object */
+	const char*	norm_name)	/*!< in: concatenation of database name,
+					'/' char, table name */
+{
+	dict_table_t*	table;
+	bool		ret	= false;
+
+	table = dict_table_open_on_name(
+		norm_name, false, false, DICT_ERR_IGNORE_NONE);
+
+	if (table == NULL) {
+
+		return(false);
+	}
+
+	/* Start the transaction if it is not started yet */
+
+	trx_start_if_not_started(trx, false, UT_LOCATION_HERE);
+
+	/* If there are locks on the table or some trx has invalidated the
+	cache before this transaction started then this transaction cannot
+	read/write from/to the cache.
+
+	If a read view has not been created for the transaction then it doesn't
+	really matter what this transactin sees. If a read view was created
+	then the view low_limit_id is the max trx id that this transaction
+	saw at the time of the read view creation.  */
+
+	if (UT_LIST_GET_LEN(table->locks) == 0
+	    && ((trx->id != 0 && trx->id >= table->query_cache_inv_id)
+		|| !MVCC::is_view_active(trx->read_view)
+		|| trx->read_view->low_limit_id() >= table->query_cache_inv_id)) {
+
+		ret = true;
+
+		/* If the isolation level is high, assign a read view for the
+		transaction if it does not yet have one */
+
+		if (trx->isolation_level >= TRX_ISO_REPEATABLE_READ
+		    && !srv_read_only_mode
+		    && !MVCC::is_view_active(trx->read_view)) {
+
+			trx_sys->mvcc->view_open(trx->read_view, trx);
+		}
+	}
+
+	dict_table_close(table, false, false);
+
+	return(ret);
+}
+
 /** Read the AUTOINC column from the current row. If the value is less than
  0 and the type is not unsigned then we reset the value to 0.
  @return value read from the column */
diff --git a/storage/innobase/trx/trx0trx.cc b/storage/innobase/trx/trx0trx.cc
index a62a625..661a66a 100644
--- a/storage/innobase/trx/trx0trx.cc
+++ b/storage/innobase/trx/trx0trx.cc
@@ -2465,6 +2465,53 @@ que_thr_t *trx_commit_step(que_thr_t *thr) /*!< in: query thread */
   return (thr);
 }
 
+void write_single_invalidate_qc_redo(char* table_name) {
+  mtr_t mtr;
+  byte  *log_ptr = NULL;
+  mtr.start();
+  mlog_add_last_record_check_bits(&mtr);
+
+  int length = strlen(table_name);
+
+  ib::system(ER_IB_MSG_1208) << "write  MLOG_GAIA_QC_INVIDATE_OP type table_name: " << table_name << "  length: " << length;
+  
+  if (!mlog_open(&mtr, 14 + 4 + length, log_ptr)) {  
+    return;
+  }
+  /* MLOG_GAIA_QC_INVIDATE_OP rec header */
+  log_ptr = mlog_write_initial_log_record_low(MLOG_GAIA_QC_INVIDATE_OP, 0, 0, log_ptr, &mtr);
+  
+  //firstly write table_name's length
+  mach_write_to_4(log_ptr, length);
+  log_ptr += 4;
+
+  //write table name info to mtr
+  memcpy(log_ptr, table_name, length);
+  log_ptr += length;
+
+  mlog_close(&mtr, log_ptr);
+
+  mtr.commit();
+}
+
+/** For each table that has been modified by the given transaction.
+ * write redo log with MLOG_GAIA_QC_INVIDATE_OP type except for system table.
+ */
+static void trx_write_invalidate_qc_redo(trx_t *trx) /*!< in: transaction */
+{
+  ut_ad(trx->id != 0);
+
+  trx_mod_tables_t::const_iterator end = trx->mod_tables.end();
+
+  for (trx_mod_tables_t::const_iterator it = trx->mod_tables.begin(); it != end;
+       ++it) {
+    if((*it)->is_system_table) {
+      continue;
+    }
+    write_single_invalidate_qc_redo((*it)->name.m_name);
+  }
+}
+
 /** Does the transaction commit for MySQL.
  @return DB_SUCCESS or error number */
 dberr_t trx_commit_for_mysql(trx_t *trx) /*!< in/out: transaction */
@@ -2505,6 +2552,11 @@ dberr_t trx_commit_for_mysql(trx_t *trx) /*!< in/out: transaction */
       }
 
       if (trx->id != 0) {
+        /* If enable fast query, it will send invalidate redo type. */
+        if (trx->mysql_thd && trx->mysql_thd->variables.fast_query_cache) {
+          trx_write_invalidate_qc_redo(trx);
+        }
+
         trx_update_mod_tables_timestamp(trx);
       }
 
diff --git a/storage/perfschema/ha_perfschema.h b/storage/perfschema/ha_perfschema.h
index c1b6d63..2a4028a 100644
--- a/storage/perfschema/ha_perfschema.h
+++ b/storage/perfschema/ha_perfschema.h
@@ -219,6 +219,13 @@ class ha_perfschema : public handler {
   THR_LOCK_DATA **store_lock(THD *thd_g, THR_LOCK_DATA **to,
                              enum thr_lock_type lock_type) override;
 
+  virtual uint8 table_cache_type(void){ return HA_CACHE_TBL_NOCACHE; }
+
+  virtual bool register_query_cache_table(THD *, char *, size_t , qc_engine_callback *engine_callback[[maybe_unused]], ulonglong_typ *)
+  {
+    return false;
+  }
+
   void print_error(int error, myf errflags) override;
 
  private:
